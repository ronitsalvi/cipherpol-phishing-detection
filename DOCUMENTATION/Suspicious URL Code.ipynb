{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bZaLFYst59y",
        "outputId": "818a023f-ea1f-4e3a-e38b-baa7579c8979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Scan Report ===\n",
            "\n",
            "Page: http://www.garage-pirenne.be/index.php?option=com_content&view=article&id=70&vsig70_0=15\n",
            "  domain: www.garage-pirenne.be\n",
            "  num_links: 1\n",
            "  external_ratio: 0.0\n",
            "  ssl_valid: False\n",
            "  suspicious_tld: False\n",
            "  classification: Unhealthy ()\n",
            "  parent_page: None\n",
            "  external_links: []\n",
            "\n",
            "Page: http://www.garage-pirenne.be/index.php?option=com_content&view=article&id=70&vsig70_0=15&tr_uuid=20250830-1740-4116-a51e-22bffc1781d7&fp=-3\n",
            "  domain: www.garage-pirenne.be\n",
            "  num_links: 4\n",
            "  external_ratio: 0.75\n",
            "  ssl_valid: False\n",
            "  suspicious_tld: False\n",
            "  classification: Unhealthy ()\n",
            "  parent_page: http://www.garage-pirenne.be/index.php?option=com_content&view=article&id=70&vsig70_0=15\n",
            "  external_links: ['http://www.above.com/marketplace/garage-pirenne.be', 'http://www.above.com/marketplace/garage-pirenne.be', 'https://www.sedo.com/services/parking.php3']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import time\n",
        "import random\n",
        "import ssl\n",
        "import socket\n",
        "\n",
        "# --- User-Agent Pool (rotates per request) ---\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:119.0) Gecko/20100101 Firefox/119.0\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\",\n",
        "]\n",
        "\n",
        "# Suspicious TLD list\n",
        "SUSPICIOUS_TLDS = {\"xyz\", \"tk\", \"top\", \"click\", \"link\", \"info\"}\n",
        "\n",
        "# Retry settings\n",
        "MAX_RETRIES = 3\n",
        "RETRY_BACKOFF = [1, 3, 5]  # seconds\n",
        "\n",
        "\n",
        "# --- Headers ---\n",
        "def get_headers():\n",
        "    \"\"\"Return randomized browser-like headers.\"\"\"\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(USER_AGENTS),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "        \"DNT\": \"1\",\n",
        "        \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Network Fetching ---\n",
        "def fetch_url(url):\n",
        "    \"\"\"Fetch URL with retries and browser-like headers.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            response = requests.get(url, headers=get_headers(), timeout=10)\n",
        "            response.raise_for_status()\n",
        "            # add jitter sleep between requests\n",
        "            time.sleep(random.uniform(1, 3))\n",
        "            return response\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            wait_time = RETRY_BACKOFF[min(attempt, len(RETRY_BACKOFF) - 1)]\n",
        "            print(f\"[WARN] Fetch failed ({e}), retrying in {wait_time}s...\")\n",
        "            time.sleep(wait_time + random.random())\n",
        "    print(f\"[ERROR] Could not fetch {url} after {MAX_RETRIES} retries.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# --- Helpers ---\n",
        "def is_external_link(base_url, link_url):\n",
        "    \"\"\"Check if a link is external vs internal.\"\"\"\n",
        "    base_domain = urlparse(base_url).netloc\n",
        "    link_domain = urlparse(link_url).netloc\n",
        "    return base_domain != \"\" and base_domain != link_domain\n",
        "\n",
        "\n",
        "def get_links(url):\n",
        "    \"\"\"Extract all valid links from a webpage.\"\"\"\n",
        "    response = fetch_url(url)\n",
        "    if not response:\n",
        "        return [], url\n",
        "    try:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        links = []\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = urljoin(url, a[\"href\"])  # make absolute\n",
        "            if href.startswith(\"http\"):\n",
        "                links.append(href)\n",
        "        return links, response.url\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Parsing failed for {url} -> {e}\")\n",
        "        return [], url\n",
        "\n",
        "\n",
        "def check_ssl_certificate(domain):\n",
        "    \"\"\"Check SSL validity of a domain (basic).\"\"\"\n",
        "    try:\n",
        "        ctx = ssl.create_default_context()\n",
        "        with socket.create_connection((domain, 443), timeout=5) as sock:\n",
        "            with ctx.wrap_socket(sock, server_hostname=domain) as ssock:\n",
        "                cert = ssock.getpeercert()\n",
        "                return True if cert else False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def has_suspicious_tld(domain):\n",
        "    \"\"\"Check if domain has suspicious TLD.\"\"\"\n",
        "    try:\n",
        "        tld = domain.split(\".\")[-1].lower()\n",
        "        return tld in SUSPICIOUS_TLDS\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "# --- Main Analyzer ---\n",
        "def check_page_health(url, depth=1):\n",
        "    \"\"\"Check page features and classify as Healthy/Unhealthy, including external links.\"\"\"\n",
        "    visited = set()\n",
        "    report = {}\n",
        "\n",
        "    def analyze(u, d, parent=None):\n",
        "        if u in visited or d > depth:\n",
        "            return\n",
        "        visited.add(u)\n",
        "\n",
        "        parsed = urlparse(u)\n",
        "        domain = parsed.netloc\n",
        "\n",
        "        # --- Feature extraction ---\n",
        "        ssl_ok = check_ssl_certificate(domain) if parsed.scheme == \"https\" else False\n",
        "        bad_tld = has_suspicious_tld(domain)\n",
        "        links, final_url = get_links(u)\n",
        "\n",
        "        num_links = len(links)\n",
        "        external_links = [l for l in links if is_external_link(u, l)]\n",
        "        external_ratio = (len(external_links) / num_links) if num_links > 0 else 0\n",
        "\n",
        "        # --- Classification Rules ---\n",
        "        if bad_tld:\n",
        "            status = \"Unhealthy (Suspicious TLD)\"\n",
        "        elif (not ssl_ok and parsed.scheme == \"https\"):\n",
        "            status = \"Unhealthy (Invalid SSL)\"\n",
        "        elif (parsed.scheme != \"https\"):\n",
        "            status = \"Unhealthy ()\"\n",
        "        elif external_ratio > 0.7:\n",
        "            status = \"Unhealthy (Too Many External Links)\"\n",
        "        elif num_links > 50:\n",
        "            status = \"Unhealthy (Suspiciously High Link Count)\"\n",
        "        elif num_links == 0:\n",
        "            status = \"Unhealthy (No Links / Fetch Failed)\"\n",
        "        else:\n",
        "            status = \"Healthy\"\n",
        "\n",
        "        # --- Store report ---\n",
        "        report[u] = {\n",
        "            \"domain\": domain,\n",
        "            \"num_links\": num_links,\n",
        "            \"external_ratio\": round(external_ratio, 2),\n",
        "            \"ssl_valid\": ssl_ok,\n",
        "            \"suspicious_tld\": bad_tld,\n",
        "            \"classification\": status,\n",
        "            \"parent_page\": parent,\n",
        "            \"external_links\": external_links[:10],  # limit list size for readability\n",
        "        }\n",
        "\n",
        "        # --- Recursive Analysis: both internal + external links ---\n",
        "        for link in links:\n",
        "            analyze(link, d + 1, parent=u)\n",
        "\n",
        "    analyze(url, 0)\n",
        "    return report\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    test_url = \"http://www.garage-pirenne.be/index.php?option=com_content&view=article&id=70&vsig70_0=15\"  # replace with phishing or legit URLs\n",
        "    results = check_page_health(test_url, depth=1)\n",
        "\n",
        "    print(\"\\n=== Scan Report ===\")\n",
        "    for page, features in results.items():\n",
        "        print(f\"\\nPage: {page}\")\n",
        "        for k, v in features.items():\n",
        "            print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsBbYQe79SrJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRVx15qT9STl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bKYm9Gc9SOu"
      },
      "source": [
        "### **Lexcial and Host Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RGl9hmc9Rp5",
        "outputId": "daf837c6-90e8-4a02-fc44-68eff34c6c8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1918513360.py:246: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  now = datetime.datetime.utcnow()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Fetch failed (403 Client Error: Forbidden for url: https://help.bbc.com/hc/) for https://help.bbc.com/hc/, retrying in 1s...\n",
            "[WARN] Fetch failed (403 Client Error: Forbidden for url: https://help.bbc.com/hc/) for https://help.bbc.com/hc/, retrying in 3s...\n",
            "[WARN] Fetch failed (403 Client Error: Forbidden for url: https://help.bbc.com/hc/) for https://help.bbc.com/hc/, retrying in 6s...\n",
            "[ERROR] Could not fetch https://help.bbc.com/hc/ after 3 retries.\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news\n",
            " Final: https://www.bbc.com/news\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 187, External ratio: 0.037\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news#main-content\n",
            " Final: https://www.bbc.com/news#main-content\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 187, External ratio: 0.037\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/watch-live-news/\n",
            " Final: https://www.bbc.com/watch-live-news\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 140, External ratio: 0.05\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport\n",
            " Final: https://www.bbc.com/sport\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 245, External ratio: 0.045\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport&userOrigin=SPORT_GNL', 'https://cloud.email.bbc.com/FootballExtra_Newsletter_Signup?&at_bbc_team=studios&at_medium=Onsite&at_objective=acquisition&at_ptr_name=bbc.com&at_link_origin=sportarticle&at_campaign=footballextra&at_campaign_type=owned', 'https://www.tiktok.com/@bbcsport', 'https://www.youtube.com/channel/UCW6-BQWFA70Dyyc7ZpZ9Xlg', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/business\n",
            " Final: https://www.bbc.com/business\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 176, External ratio: 0.051\n",
            " External sample (<= 20): ['https://cloud.email.bbc.com/WorldofBusiness_Newsletter_Signup?&at_bbc_team=studios&at_medium=emails&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.com&at_format=Module&at_link_origin=homepage&at_campaign=wob&at_campaign_type=owned', 'https://cloud.email.bbc.com/WorldofBusiness_Newsletter_Signup?&at_bbc_team=studios&at_medium=emails&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.com&at_format=Module&at_link_origin=homepage&at_campaign=wob&at_campaign_type=owned', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/innovation\n",
            " Final: https://www.bbc.com/innovation\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 176, External ratio: 0.051\n",
            " External sample (<= 20): ['https://cloud.email.bbc.com/techdecoded-newsletter-signup?&at_bbc_team=studios&at_medium=display&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.comhp&at_format=Module&at_link_origin=homepage&at_campaign=techdecoded&at_campaign_type=owned', 'https://cloud.email.bbc.com/techdecoded-newsletter-signup?&at_bbc_team=studios&at_medium=display&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.comhp&at_format=Module&at_link_origin=homepage&at_campaign=techdecoded&at_campaign_type=owned', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/culture\n",
            " Final: https://www.bbc.com/culture\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 200, External ratio: 0.035\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/arts\n",
            " Final: https://www.bbc.com/arts\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 153, External ratio: 0.046\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel\n",
            " Final: https://www.bbc.com/travel\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 185, External ratio: 0.038\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/future-planet\n",
            " Final: https://www.bbc.com/future-planet\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 163, External ratio: 0.049\n",
            " External sample (<= 20): ['https://cloud.email.bbc.com/FutureEarth_Newsletter_Signup?&at_bbc_team=studios&at_medium=display&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.comhp&at_format=Module&at_link_origin=homepage&at_campaign=futureearth&at_campaign_type=owned', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/audio\n",
            " Final: https://www.bbc.com/audio\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 180, External ratio: 0.039\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/video\n",
            " Final: https://www.bbc.com/video\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 178, External ratio: 0.039\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/live\n",
            " Final: https://www.bbc.com/live\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 243, External ratio: 0.029\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
            " Final: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 158, External ratio: 0.044\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/war-in-ukraine\n",
            " Final: https://www.bbc.com/news/war-in-ukraine\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 159, External ratio: 0.05\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/programmes/p0ls3brx', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/us-canada\n",
            " Final: https://www.bbc.com/news/us-canada\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 163, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/uk\n",
            " Final: https://www.bbc.com/news/uk\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 172, External ratio: 0.041\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/world/africa\n",
            " Final: https://www.bbc.com/news/world/africa\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 169, External ratio: 0.059\n",
            " External sample (<= 20): ['http://www.bbc.co.uk/news/world-africa-29144792', 'https://www.bbc.co.uk/sounds/series/p02nrtyw', 'https://www.bbc.co.uk/programmes/w13xttz8', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/world/asia\n",
            " Final: https://www.bbc.com/news/world/asia\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 160, External ratio: 0.044\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/world/australia\n",
            " Final: https://www.bbc.com/news/world/australia\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/world/europe\n",
            " Final: https://www.bbc.com/news/world/europe\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 163, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/world/latin_america\n",
            " Final: https://www.bbc.com/news/world/latin_america\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 158, External ratio: 0.044\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/world/middle_east\n",
            " Final: https://www.bbc.com/news/world/middle_east\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 163, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/in_pictures\n",
            " Final: https://www.bbc.com/news/in_pictures\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 158, External ratio: 0.044\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/bbcindepth\n",
            " Final: https://www.bbc.com/news/bbcindepth\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 153, External ratio: 0.098\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/sounds/play/m002gg7m', 'https://www.bbc.co.uk/sounds/play/w3ct7446', 'https://www.bbc.co.uk/sounds/play/w3ct746h', 'https://www.bbc.co.uk/sounds/play/m002gjfv', 'https://www.bbc.co.uk/iplayer/episode/m002h44h', 'https://www.bbc.co.uk/iplayer/episodes/m002gjgd', 'https://www.bbc.co.uk/iplayer/episode/m002gtq0', 'https://www.bbc.co.uk/iplayer/episodes/m002gjg4', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/bbcverify\n",
            " Final: https://www.bbc.com/news/bbcverify\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 175, External ratio: 0.046\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/resources/idt-33fccfbe-abcc-4af1-bdd2-632b2787cf59', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/home\n",
            " Final: https://www.bbc.co.uk/\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 214, External ratio: 0.093\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.co.uk%2F', 'https://www.bbc.co.uk/schedules/p00fzl9m', 'https://www.bbc.co.uk/sounds', 'https://www.bbc.co.uk/schedules/p00fzl9m', 'https://www.bbc.co.uk/sounds', 'https://www.bbc.co.uk/sport', 'https://www.bbc.co.uk/iplayer/watchlist', 'https://www.bbc.co.uk/sounds/my/subscribed', 'https://www.bbc.co.uk/iplayer/watchlist', 'https://www.bbc.co.uk/sounds/my/subscribed', 'https://www.bbc.co.uk/schedules/p00fzl9m', 'https://www.bbc.co.uk/sounds', 'https://www.bbc.co.uk/usingthebbc/terms', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/politics\n",
            " Final: https://www.bbc.com/news/politics\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 169, External ratio: 0.059\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/sounds/play/w3ct722z', 'https://www.bbc.co.uk/sounds/play/p0lx81dc', 'https://www.bbc.co.uk/sounds/play/p0lww9zt', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/england\n",
            " Final: https://www.bbc.com/news/england\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 173, External ratio: 0.04\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/northern_ireland\n",
            " Final: https://www.bbc.com/news/northern_ireland\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 176, External ratio: 0.091\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/sounds/brand/m001lcfj', 'https://www.bbc.co.uk/sounds/brand/b007cps5', 'https://www.bbc.co.uk/sounds/brand/b007cpsh', 'https://www.bbc.co.uk/sounds/series/p02nrv77', 'https://www.bbc.co.uk/sounds/brand/b007cpv5', 'https://www.bbc.co.uk/iplayer/episodes/b006mvxy', 'https://www.bbc.co.uk/sounds/brand/m002c000', 'https://www.bbc.co.uk/sounds/brand/p07mnkk8', 'https://www.bbc.co.uk/iplayer/episode/m002d4sx/spotlight-mums-in-crisis', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/northern_ireland/northern_ireland_politics\n",
            " Final: https://www.bbc.com/news/northern_ireland/northern_ireland_politics\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 145, External ratio: 0.048\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/scotland\n",
            " Final: https://www.bbc.com/news/scotland\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 173, External ratio: 0.069\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/sounds/play/m002hmcv', 'https://www.bbc.co.uk/iplayer/episodes/b006mj3s', 'https://www.bbc.co.uk/sounds/brand/m00272kp', 'https://www.bbc.co.uk/iplayer/episodes/b006mj3s?seriesId=m0026sgj', 'https://www.bbc.co.uk/sounds/brand/b0074hf7', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/scotland/scotland_politics\n",
            " Final: https://www.bbc.com/news/scotland/scotland_politics\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 150, External ratio: 0.047\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/wales\n",
            " Final: https://www.bbc.com/news/wales\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 168, External ratio: 0.042\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/wales/wales_politics\n",
            " Final: https://www.bbc.com/news/wales/wales_politics\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 145, External ratio: 0.048\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/world/asia/china\n",
            " Final: https://www.bbc.com/news/world/asia/china\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 155, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/world/asia/india\n",
            " Final: https://www.bbc.com/news/world/asia/india\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 162, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/business/executive-lounge\n",
            " Final: https://www.bbc.com/business/executive-lounge\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 137, External ratio: 0.051\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/business/technology-of-business\n",
            " Final: https://www.bbc.com/business/technology-of-business\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 148, External ratio: 0.047\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/business/future-of-business\n",
            " Final: https://www.bbc.com/business/future-of-business\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 174, External ratio: 0.04\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/innovation/technology\n",
            " Final: https://www.bbc.com/innovation/technology\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 162, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/innovation/science\n",
            " Final: https://www.bbc.com/innovation/science\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 145, External ratio: 0.055\n",
            " External sample (<= 20): ['https://cloud.email.bbc.com/HealthFix_Newsletter_Signup?&at_bbc_team=studios&at_medium=emails&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.com&at_format=Module&at_link_origin=homepage&at_campaign=healthfix&at_campaign_type=owned', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/innovation/artificial-intelligence\n",
            " Final: https://www.bbc.com/innovation/artificial-intelligence\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 160, External ratio: 0.044\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/innovation/ai-v-the-mind\n",
            " Final: https://www.bbc.com/innovation/ai-v-the-mind\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 135, External ratio: 0.052\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/culture/film-tv\n",
            " Final: https://www.bbc.com/culture/film-tv\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 167, External ratio: 0.042\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/culture/music\n",
            " Final: https://www.bbc.com/culture/music\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 164, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/culture/art\n",
            " Final: https://www.bbc.com/culture/art\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 164, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/culture/style\n",
            " Final: https://www.bbc.com/culture/style\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 144, External ratio: 0.049\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/culture/books\n",
            " Final: https://www.bbc.com/culture/books\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 154, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/culture/entertainment-news\n",
            " Final: https://www.bbc.com/culture/entertainment-news\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/arts/arts-in-motion\n",
            " Final: https://www.bbc.com/arts/arts-in-motion\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 135, External ratio: 0.052\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations\n",
            " Final: https://www.bbc.com/travel/destinations\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 148, External ratio: 0.047\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/africa\n",
            " Final: https://www.bbc.com/travel/destinations/africa\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/antarctica\n",
            " Final: https://www.bbc.com/travel/destinations/antarctica\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/asia\n",
            " Final: https://www.bbc.com/travel/destinations/asia\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/australia-and-pacific\n",
            " Final: https://www.bbc.com/travel/destinations/australia-and-pacific\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/caribbean\n",
            " Final: https://www.bbc.com/travel/destinations/caribbean\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/central-america\n",
            " Final: https://www.bbc.com/travel/destinations/central-america\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/europe\n",
            " Final: https://www.bbc.com/travel/destinations/europe\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/middle-east\n",
            " Final: https://www.bbc.com/travel/destinations/middle-east\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/north-america\n",
            " Final: https://www.bbc.com/travel/destinations/north-america\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/destinations/south-america\n",
            " Final: https://www.bbc.com/travel/destinations/south-america\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 156, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/worlds-table\n",
            " Final: https://www.bbc.com/travel/worlds-table\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 176, External ratio: 0.04\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/cultural-experiences\n",
            " Final: https://www.bbc.com/travel/cultural-experiences\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 175, External ratio: 0.04\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/adventures\n",
            " Final: https://www.bbc.com/travel/adventures\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 178, External ratio: 0.039\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/travel/specialist\n",
            " Final: https://www.bbc.com/travel/specialist\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 157, External ratio: 0.045\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/future-planet/natural-wonders\n",
            " Final: https://www.bbc.com/future-planet/natural-wonders\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 184, External ratio: 0.038\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/future-planet/weather-science\n",
            " Final: https://www.bbc.com/future-planet/weather-science\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 163, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/future-planet/solutions\n",
            " Final: https://www.bbc.com/future-planet/solutions\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 161, External ratio: 0.043\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/future-planet/sustainable-business\n",
            " Final: https://www.bbc.com/future-planet/sustainable-business\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 146, External ratio: 0.048\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/future-planet/green-living\n",
            " Final: https://www.bbc.com/future-planet/green-living\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 153, External ratio: 0.046\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/audio/categories\n",
            " Final: https://www.bbc.com/audio/categories\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 128, External ratio: 0.055\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/audio/stations\n",
            " Final: https://www.bbc.com/audio/stations\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 126, External ratio: 0.056\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/live/news\n",
            " Final: https://www.bbc.com/live/news\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 128, External ratio: 0.055\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/live/sport\n",
            " Final: https://www.bbc.com/live/sport\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 234, External ratio: 0.03\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/weather\n",
            " Final: https://www.bbc.com/weather\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 120, External ratio: 0.408\n",
            " External sample (<= 20): ['https://www.bbc.co.uk', 'https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account', 'https://www.bbc.co.uk/notifications', 'https://www.bbc.co.uk', 'https://www.bbc.co.uk/news', 'https://www.bbc.co.uk/sport', 'https://www.bbc.co.uk/weather', 'https://www.bbc.co.uk/iplayer', 'https://www.bbc.co.uk/sounds', 'https://www.bbc.co.uk/bitesize', 'https://www.bbc.co.uk/cbeebies', 'https://www.bbc.co.uk/cbbc', 'https://www.bbc.co.uk/food', 'https://search.bbc.co.uk/search?scope=all&destination=weather_gnl', 'https://www.bbc.co.uk', 'https://www.bbc.co.uk/news', 'https://www.bbc.co.uk/sport', 'https://www.bbc.co.uk/weather', 'https://www.bbc.co.uk/iplayer']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/newsletters\n",
            " Final: https://www.bbc.com/newsletters\n",
            " Class: Healthy\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 50, External ratio: 0.3\n",
            " External sample (<= 20): ['https://www.bbc.co.uk', 'https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account', 'https://www.bbc.co.uk/notifications', 'https://search.bbc.co.uk/search?scope=all&destination=BBCS_BBC', 'https://www.bbc.co.uk/usingthebbc/terms/', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/usingthebbc/privacy/', 'https://www.bbc.co.uk/usingthebbc/cookies/', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/contact/complaints/', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/ckgj7jxkq58o\n",
            " Final: https://www.bbc.com/news/articles/ckgj7jxkq58o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 135, External ratio: 0.074\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/world-us-canada-68093155', 'https://www.bbc.co.uk/newsletters/zgmn46f', 'https://cloud.email.bbc.com/US_Politics_Unspun_Newsletter_Signup', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cjdym32z9v7o\n",
            " Final: https://www.bbc.com/news/articles/cjdym32z9v7o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 139, External ratio: 0.129\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/c4gmrxd8ryno', 'https://www.bbc.co.uk/news/articles/c1mpg2glz1zo', 'https://www.bbc.co.uk/news/articles/crlzyne9jl2o', 'https://treaties.un.org/doc/Publication/UNTS/Volume%2011/volume-11-I-147-English.pdf', 'https://www.bbc.co.uk/news/articles/ckgr71z0jp4o', 'https://www.bbc.co.uk/news/articles/cvgp5z1vvj5o', 'https://www.state.gov/releases/office-of-the-spokesperson/2025/08/trump-administration-reaffirms-commitment-to-not-reward-terrorism-and-revokes-visas-of-palestinian-officials-ahead-of-unga/', 'https://www.bbc.co.uk/news/articles/cewy88jle0eo', 'https://www.bbc.co.uk/news/articles/crm49zxyn0go', 'https://www.bbc.co.uk/news/articles/cvgpxwy2lkwo', 'https://www.bbc.co.uk/news/articles/cj4w2q9k4pjo', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c5yejdmgzg4o\n",
            " Final: https://www.bbc.com/news/articles/c5yejdmgzg4o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 129, External ratio: 0.093\n",
            " External sample (<= 20): ['https://www.telegraph.co.uk/films/0/julia-roberts-after-the-hunt-review/', 'https://www.thetimes.com/culture/film/article/after-the-hunt-review-julia-roberts-will-win-an-oscar-for-this-metoo-story-ch0qg375h', 'https://www.screendaily.com/reviews/after-the-hunt-review-julia-roberts-heads-luca-guadagninos-problematic-metoo-drama/5208358.article', 'https://www.hollywoodreporter.com/movies/movie-reviews/after-the-hunt-review-julia-roberts-luca-guadagnino-1236356837/', 'https://www.indiewire.com/criticism/movies/after-the-hunt-review-1235148028/', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cdxydpr1zz2o\n",
            " Final: https://www.bbc.com/news/articles/cdxydpr1zz2o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 132, External ratio: 0.053\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c4gl1kx1091o\n",
            " Final: https://www.bbc.com/news/articles/c4gl1kx1091o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 133, External ratio: 0.09\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/c7v5zgelqjy', 'https://www.bbc.co.uk/news/articles/c7v5zgelqjyo', 'https://www.bbc.co.uk/news/articles/c7v5zgelqjyo', 'https://www.bbc.co.uk/news/uk-14756736', 'https://www.bbc.co.uk/news/uk-66207576', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/ckgyk7ry8rdo\n",
            " Final: https://www.bbc.com/news/articles/ckgyk7ry8rdo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 131, External ratio: 0.053\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c80d2nvzg72o\n",
            " Final: https://www.bbc.com/news/articles/c80d2nvzg72o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 142, External ratio: 0.092\n",
            " External sample (<= 20): ['https://www.state.gov/i2u2', 'https://in.ambafrance.org/France-India-UAE-Establishment-of-a-Trilateral-Cooperation-Initiative', 'https://eoibeijing.gov.in/eoibejing_pages/MjQ,', 'https://indianexpress.com/article/trending/top-10-listing/top-10-countries-highest-defence-budget-2025-9840269/', 'https://www.hindustantimes.com/opinion/anatomy-of-the-budding-india-china-bonhomie-101755700957148.html', 'https://www.foreignaffairs.com/india/indias-great-power-delusions', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c9876y4z4rgo\n",
            " Final: https://www.bbc.com/news/articles/c9876y4z4rgo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 132, External ratio: 0.083\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/cnvmgvy51r4o', 'https://www.bbc.co.uk/news/uk-scotland-49852050', 'https://www.bbc.co.uk/news/uk-scotland-edinburgh-east-fife-51148801', 'https://www.bbc.co.uk/news/articles/cpd4p2wnyzno', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c4gz1ekdee3o\n",
            " Final: https://www.bbc.com/news/articles/c4gz1ekdee3o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 138, External ratio: 0.13\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/cjel2nn22z9o', 'https://www.bbc.co.uk/news/articles/czxp0qyn6dqo', 'https://files-profile.medicine.yale.edu/documents/d2a99234-5694-405a-92bf-0c006d29614e', 'https://www.bbc.co.uk/news/articles/c9870xjezk4o', 'https://www.bbc.co.uk/news/articles/c1755j8788go', 'https://www.bbc.co.uk/news/articles/cn011rk5evyo', 'https://www.bbc.co.uk/news/articles/cg717385nj7o', 'https://www.bbc.co.uk/news/articles/c1755j8788go', 'http://bbcafrica.com/', 'https://www.bbc.co.uk/programmes/p02nrtyw/episodes/downloads', 'https://www.bbc.co.uk/sounds/brand/w13xttz8', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cedv3gg3x6xo\n",
            " Final: https://www.bbc.com/news/articles/cedv3gg3x6xo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 137, External ratio: 0.058\n",
            " External sample (<= 20): ['https://www.help.senate.gov/rep/newsroom/press/cassidy-calls-for-vaccine-committee-meeting-to-be-postponed-following-cdc-departures', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c24z0105m24o\n",
            " Final: https://www.bbc.com/news/articles/c24z0105m24o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 138, External ratio: 0.123\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/c7vlpdqeg4qo', 'https://www.bbc.co.uk/news/articles/clyj5lk7p9zo', 'https://www.bbc.co.uk/news/live/c1ej52299lqt?post=asset%3A1cda1ce2-0db9-42c1-9214-4fa022b2d77a#post', 'https://www.bbc.co.uk/sounds/curation/p0cjdqpb', 'https://www.bbc.co.uk/news/articles/c8e1zd98k9no', 'https://www.bbc.co.uk/news/articles/cdxy0p9jx1qo', 'https://www.bbc.co.uk/news/articles/cpqv01lxvyro', 'https://www.bbc.co.uk/news/articles/cz0ygz0dkllo', 'https://www.bbc.co.uk/news/articles/cy98gdnrl7lo', 'https://www.bbc.co.uk/news/articles/c628y9d944jo', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c04r0z1pr25o\n",
            " Final: https://www.bbc.com/news/articles/c04r0z1pr25o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 134, External ratio: 0.09\n",
            " External sample (<= 20): ['https://www.politico.com/news/2025/08/28/europe-buffer-zone-ukraine-russia-peace-deal-00534483', 'https://www.bbc.co.uk/news/articles/c4gj9er0x0zo', 'https://www.bbc.co.uk/news/articles/cy9834jp9qno', 'https://www.bbc.co.uk/news/articles/cj3ld2r2206o', 'https://www.bbc.co.uk/news/articles/c23gjk7dlvlo', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cjw6el72xd4o\n",
            " Final: https://www.bbc.com/news/articles/cjw6el72xd4o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 128, External ratio: 0.055\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/ceqy3jnl39do\n",
            " Final: https://www.bbc.com/news/articles/ceqy3jnl39do\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 133, External ratio: 0.053\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cy5pvdyd0ygo\n",
            " Final: https://www.bbc.com/news/articles/cy5pvdyd0ygo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 137, External ratio: 0.08\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/czxp41y3n7lo', 'https://www.bbc.co.uk/news/articles/c3v37plv2edo', 'https://www.bbc.co.uk/news/articles/c3dpr2zkny0o', 'https://www.bbc.co.uk/news/articles/c62nn665g32o', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cr5r01jdpqjo\n",
            " Final: https://www.bbc.com/news/articles/cr5r01jdpqjo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 137, External ratio: 0.117\n",
            " External sample (<= 20): ['https://x.com/chrlux3/status/1960790325926015471', 'https://x.com/SunnyBanks_/status/1961082635708449039', 'https://x.com/SerenaDevora/status/1960799895088193993', 'https://x.com/Belcardi11/status/1960779699946197094', 'https://x.com/BigBritishBardi/status/1960760793646895382', 'https://x.com/sunnybanks_/status/1960764044840984821', 'https://x.com/meghanncuniff/status/1960479896582676529', 'https://x.com/brianacromero/status/1960553767855940015?s=46&t=Rz4XAKo6kkTOxp9C30k09A', 'https://x.com/neurodatadefrag/status/1960859879775588597?s=46&t=Rz4XAKo6kkTOxp9C30k09A', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c78zp9ljl4yo\n",
            " Final: https://www.bbc.com/news/articles/c78zp9ljl4yo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 135, External ratio: 0.074\n",
            " External sample (<= 20): ['http://bbcafrica.com/', 'https://www.bbc.co.uk/programmes/p02nrtyw/episodes/downloads', 'https://www.bbc.co.uk/sounds/brand/w13xttz8', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c04r07n7dqyo\n",
            " Final: https://www.bbc.com/news/articles/c04r07n7dqyo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 130, External ratio: 0.062\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/ce874j4ye3no', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/ckgyw2p9n68o\n",
            " Final: https://www.bbc.com/news/articles/ckgyw2p9n68o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 135, External ratio: 0.052\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cwy0dgpyq35o\n",
            " Final: https://www.bbc.com/news/articles/cwy0dgpyq35o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 135, External ratio: 0.104\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/c2kvw2pvw8eo', 'https://www.bbc.co.uk/news/uk-64179164', 'https://www.bbc.co.uk/news/uk-56340451', 'https://www.bbc.co.uk/newsletters/zkp3wsg', 'https://www.bbc.co.uk/news/articles/cvg9079v13lo', 'https://www.bbc.co.uk/news/articles/c741n548dkko', 'https://www.bbc.co.uk/news/articles/cjewne81lq4o', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/videos/cm2vdm1pnd8o\n",
            " Final: https://www.bbc.com/news/videos/cm2vdm1pnd8o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 141, External ratio: 0.057\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/ckgywj819n7o', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/videos/clyr0vlz2nvo\n",
            " Final: https://www.bbc.com/news/videos/clyr0vlz2nvo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 121, External ratio: 0.066\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/topics/cwlw3xz0zl4t', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/videos/cgqndl1kv52o\n",
            " Final: https://www.bbc.com/news/videos/cgqndl1kv52o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 142, External ratio: 0.049\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/videos/cd6n8egx95go\n",
            " Final: https://www.bbc.com/news/videos/cd6n8egx95go\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 140, External ratio: 0.05\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/videos/czr6dnrrxreo\n",
            " Final: https://www.bbc.com/news/videos/czr6dnrrxreo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 119, External ratio: 0.067\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/cjw6el72xd4o', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cdj2vldpd1do\n",
            " Final: https://www.bbc.com/news/articles/cdj2vldpd1do\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 132, External ratio: 0.076\n",
            " External sample (<= 20): ['https://x.com/whomb/status/1961410044907458780?s=46&t=vePooyJN2F_j9u6nA1ek5g', 'https://www.gao.gov/blog/what-pocket-rescission-and-it-legal', 'https://www.bbc.co.uk/news/articles/clyezjwnx5ko', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cdrkvy2pn87o\n",
            " Final: https://www.bbc.com/news/articles/cdrkvy2pn87o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 132, External ratio: 0.053\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/c2063n085d1o\n",
            " Final: https://www.bbc.com/news/articles/c2063n085d1o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 133, External ratio: 0.09\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/news/articles/c5ye8ljv1q7o', 'https://www.bbc.co.uk/news/articles/cp95m0pel7vo', 'https://www.bbc.co.uk/news/articles/c2lkwzp0r81o', 'https://www.bbc.co.uk/news/articles/cy9v3p43l4lo', 'https://www.bbc.co.uk/news/articles/cy8ndx4v8lro', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport/football/articles/c2l74klqpkwo\n",
            " Final: https://www.bbc.com/sport/football/articles/c2l74klqpkwo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 136, External ratio: 0.059\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport%2Ffootball%2Farticles%2Fc2l74klqpkwo&userOrigin=SPORT_GNL', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/ckgyk2p55g8o\n",
            " Final: https://www.bbc.com/news/articles/ckgyk2p55g8o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 127, External ratio: 0.102\n",
            " External sample (<= 20): ['https://www.wsj.com/articles/taco-bell-rethinks-future-of-voice-ai-at-the-drive-through-72990b5a?mod=rss_Technology', 'https://www.bbc.co.uk/news/articles/c722gne7qngo', 'https://www.bbc.co.uk/news/articles/c4gzdk25dr6o', 'https://www.bbc.co.uk/news/articles/cwy0j5gjwq3o', 'https://www.bbc.co.uk/newsletters/zxh6cxs', 'https://cloud.email.bbc.com/techdecoded-newsletter-signup', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport/tennis/articles/clyjk21gyw9o\n",
            " Final: https://www.bbc.com/sport/tennis/articles/clyjk21gyw9o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 128, External ratio: 0.07\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport%2Ftennis%2Farticles%2Fclyjk21gyw9o&userOrigin=SPORT_GNL', 'https://www.bbc.co.uk/sport/topics/crm3j119x37t', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cp894ed8m5qo\n",
            " Final: https://www.bbc.com/news/articles/cp894ed8m5qo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 134, External ratio: 0.097\n",
            " External sample (<= 20): ['https://www.independent.co.uk/arts-entertainment/films/reviews/the-roses-review-release-benedict-cumberbatch-olivia-colman-b2815485.html', 'https://www.telegraph.co.uk/films/2025/08/27/the-roses-benedict-cumberbatch-olivia-colman/#:~:text=As%20a%20warring%20couple%2C%20Olivia,supply%20are%20the%20special%20effects.', 'https://www.theguardian.com/film/2025/aug/25/the-roses-review-olivia-colman-and-benedict-cumberbatch-remake', 'https://www.bbc.co.uk/news/entertainment-arts-68357293', 'https://www.bbc.co.uk/news/entertainment-arts-68654311', 'https://www.bbc.co.uk/news/articles/cd9j4dvg5z7o', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/news/articles/cx29qe59313o\n",
            " Final: https://www.bbc.com/news/articles/cx29qe59313o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 131, External ratio: 0.061\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/programmes/articles/22VVM5LPrf3pjYdKqctmMXn/information-and-support-sexual-abuse-and-violence', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport/football/articles/ckgley33q3ro\n",
            " Final: https://www.bbc.com/sport/football/articles/ckgley33q3ro\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 130, External ratio: 0.069\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport%2Ffootball%2Farticles%2Fckgley33q3ro&userOrigin=SPORT_GNL', 'https://www.bbc.co.uk/sport/topics/c8gr7dqlp5pt', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport/football/articles/cdd3v0zzdrro\n",
            " Final: https://www.bbc.com/sport/football/articles/cdd3v0zzdrro\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 126, External ratio: 0.063\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport%2Ffootball%2Farticles%2Fcdd3v0zzdrro&userOrigin=SPORT_GNL', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport/tennis/articles/c2kzev7wvj3o\n",
            " Final: https://www.bbc.com/sport/tennis/articles/c2kzev7wvj3o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 130, External ratio: 0.069\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport%2Ftennis%2Farticles%2Fc2kzev7wvj3o&userOrigin=SPORT_GNL', 'https://www.bbc.co.uk/sport/topics/crm3j119x37t', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport/football/articles/cj4w7xgq47zo\n",
            " Final: https://www.bbc.com/sport/football/articles/cj4w7xgq47zo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 132, External ratio: 0.068\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport%2Ffootball%2Farticles%2Fcj4w7xgq47zo&userOrigin=SPORT_GNL', 'https://www.bbc.co.uk/sport/topics/cpv471x401jt', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport/football/articles/cly6d3668e4o\n",
            " Final: https://www.bbc.com/sport/football/articles/cly6d3668e4o\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 137, External ratio: 0.066\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport%2Ffootball%2Farticles%2Fcly6d3668e4o&userOrigin=SPORT_GNL', 'https://www.bbc.co.uk/sport/topics/c8gr7dqlp5pt', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/sport/football/articles/c3ezl35jdzpo\n",
            " Final: https://www.bbc.com/sport/football/articles/c3ezl35jdzpo\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 136, External ratio: 0.066\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account?lang=en-GB&ptrt=https%3A%2F%2Fwww.bbc.com%2Fsport%2Ffootball%2Farticles%2Fc3ezl35jdzpo&userOrigin=SPORT_GNL', 'https://www.bbc.co.uk/sport/topics/c37dl80p4y3t', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility', 'https://www.bbc.co.uk/iplayer/guidance', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/bbcnewsletter', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/\n",
            " Final: https://www.bbc.com/\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 243, External ratio: 0.049\n",
            " External sample (<= 20): ['https://cloud.email.bbc.com/techdecoded-newsletter-signup?&at_bbc_team=studios&at_medium=display&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.comhp&at_format=Module&at_link_origin=homepage&at_campaign=techdecoded&at_campaign_type=owned', 'https://cloud.email.bbc.com/US_Politics_Unspun_Newsletter_Signup?&at_bbc_team=studios&at_medium=display&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.comhp&at_format=Module&at_link_origin=homepage&at_campaign=uselectionunspun&at_campaign_type=owned', 'https://account.bbc.com/auth/register/email?ab=o13&action=register&clientId=Account&context=international&isCasso=false&nonce=ZhRJnl8N-Nzg4mlepfcUAP3WaQY2IhgqHUUw&ptrt=https%3A%2F%2Fwww.bbc.com%2F&realm=%2F&redirectUri=https%3A%2F%2Fsession.bbc.com%2Fsession%2Fcallback%3Frealm%3D%2F&sequenceId=afd616da-9f13-432d-a54d-95c9f32d1f0b&service=IdRegisterService&userOrigin=BBCS_BBC', 'https://cloud.email.bbc.com/TheEssentialList_Newsletter_Signup?&at_bbc_team=studios&at_medium=display&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.comhp&at_format=Module&at_link_origin=homepage&at_campaign=essentiallist&at_campaign_type=owned', 'https://cloud.email.bbc.com/bbcnewsignup2?&at_bbc_team=studios&at_medium=display&at_objective=acquisition&at_ptr_type=&at_ptr_name=bbc.comhp&at_format=Module&at_link_origin=homepage&at_campaign=newsbriefing&at_campaign_type=owned', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://shop.bbc.com/\n",
            " Final: https://shop.bbc.com/\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 143, External ratio: 0.035\n",
            " External sample (<= 20): ['https://www.bbcstudios.com/privacy/', 'https://help.bbcshopsupport.com', 'https://www.youtube.com/@BBC', 'https://x.com/BBC', 'https://www.snowcommerce.com']\n",
            "\n",
            "---\n",
            "URL: https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer\n",
            " Final: https://www.britbox.com/us/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer\n",
            " Class: Healthy\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 76, External ratio: 0.079\n",
            " External sample (<= 20): ['https://apps.apple.com/us/app/britbox-the-best-british-tv/id1206838907', 'https://help.britbox.com/hc/en-us/articles/360043212953-What-devices-can-I-use-to-watch-BritBox', 'https://help.britbox.com/hc/en-us', 'https://help.britbox.com/hc/en-us/requests/new?ticket_form_id=14330674956567', 'https://watch.britbox.com/us/gifting', 'https://help.britbox.com/hc/en-us']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/pages/terms-of-use\n",
            " Final: https://www.bbc.com/pages/terms-of-use\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 132, External ratio: 0.144\n",
            " External sample (<= 20): ['https://www.bbcstudios.com/contact/our-locations/', 'https://www.bbc.co.uk/usingthebbc/privacy-policy/', 'https://www.bbc.co.uk/usingthebbc/terms-of-use', 'http://www.bbc.co.uk/branding', 'https://www.bbc.co.uk/usingthebbc/terms/can-i-use-bbc-content/#caniusebbcmetadataandrssfeeds?', 'https://www.bbc.co.uk/usingthebbc/terms/can-i-use-bbc-content-for-my-business/', 'https://www.bbc.co.uk/usingthebbc/terms/can-i-use-bbc-content', 'https://www.bbc.co.uk/opensource', 'https://www.bbc.co.uk/news/10628494', 'http://www.bbc.co.uk/usingthebbc/terms/can-i-use-bbc-content-for-my-business/', 'https://www.bbc.co.uk/usingthebbc/terms-of-use/#16creationswhattheyare', 'https://account.bbc.com/auth/register/email?ab=o13&action=register&clientId=Account&context=international&isCasso=false&nonce=ItKq1Wu0-wneRbqzGW7RfuqsQ70LuFdU8gsk&ptrt=https%3A%2F%2Faccount.bbc.com%2Faccount&purpose=control&realm=%2F&redirectUri=https%3A%2F%2Fsession.bbc.com%2Fsession%2Fcallback%3Frealm%3D%2F&sequenceId=2f57c35c-99dd-414d-833f-3937b6da8b6b&service=IdSignInService', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.co.uk/aboutthebbc\n",
            " Final: https://www.bbc.co.uk/aboutthebbc\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 122, External ratio: 0.303\n",
            " External sample (<= 20): ['https://account.bbc.com/account', 'https://www.bbc.com/', 'https://www.bbc.com/news', 'https://www.bbc.com/sport', 'https://www.bbc.com/business', 'https://www.bbc.com/innovation', 'https://www.bbc.com/culture', 'https://www.bbc.com/travel', 'https://www.bbc.com/future-planet', 'https://www.bbc.com/video', 'https://www.bbc.com/live', 'https://search.bbc.co.uk/search?scope=all&destination=BBC_CORPORATE_PS', 'https://www.bbc.com/', 'https://www.bbc.com/news', 'https://www.bbc.com/sport', 'https://www.bbc.com/business', 'https://www.bbc.com/innovation', 'https://www.bbc.com/culture', 'https://www.bbc.com/travel', 'https://www.bbc.com/future-planet']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/pages/privacy-policy\n",
            " Final: https://www.bbc.com/pages/privacy-policy\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 168, External ratio: 0.125\n",
            " External sample (<= 20): ['https://www.bbcstudios.com/contact/our-locations/', 'https://www.bbc.co.uk/usingthebbc/privacy-policy/', 'https://productions.bbcstudios.com/privacy-policy', 'https://careers.bbc.co.uk/content/BBC-Careers-Privacy-Notice/?locale=en_GB', 'https://www.bbc.co.uk/usingthebbc/privacy-policy/', 'https://www.bbc.co.uk/usingthebbc/terms/', 'https://account.bbc.com/account/settings/privacy', 'https://account.bbc.com/account/settings/privacy', 'https://www.bbc.co.uk/usingthebbc/privacy/do-you-share-my-data', 'https://account.bbc.com/account/settings/privacy', 'https://account.bbc.com/account/settings/privacy/yourdata', 'https://account.bbc.com/account/settings', 'https://account.bbc.com/account/settings/privacy', 'http://www.ico.org.uk', 'https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/usingthebbc/cookies/\n",
            " Final: https://www.bbc.com/usingthebbc/cookies/\n",
            " Class: Healthy\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 97, External ratio: 0.474\n",
            " External sample (<= 20): ['https://www.bbc.co.uk', 'https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account', 'https://www.bbc.co.uk/notifications', 'https://www.bbc.co.uk', 'https://www.bbc.co.uk/news', 'https://www.bbc.co.uk/sport', 'https://www.bbc.co.uk/weather', 'https://www.bbc.co.uk/iplayer', 'https://www.bbc.co.uk/sounds', 'https://www.bbc.co.uk/bitesize', 'https://www.bbc.co.uk/cbeebies', 'https://www.bbc.co.uk/cbbc', 'https://www.bbc.co.uk/food', 'https://search.bbc.co.uk/search?scope=all&destination=BBC_CORPORATE_PS', 'https://www.bbc.co.uk', 'https://www.bbc.co.uk/news', 'https://www.bbc.co.uk/sport', 'https://www.bbc.co.uk/weather', 'https://www.bbc.co.uk/iplayer']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.co.uk/accessibility/\n",
            " Final: https://www.bbc.co.uk/accessibility/\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 125, External ratio: 0.368\n",
            " External sample (<= 20): ['https://account.bbc.com/account', 'https://www.bbc.com/', 'https://www.bbc.com/news', 'https://www.bbc.com/sport', 'https://www.bbc.com/business', 'https://www.bbc.com/innovation', 'https://www.bbc.com/culture', 'https://www.bbc.com/travel', 'https://www.bbc.com/future-planet', 'https://www.bbc.com/video', 'https://www.bbc.com/live', 'https://search.bbc.co.uk/search?scope=all&destination=', 'https://www.bbc.com/', 'https://www.bbc.com/news', 'https://www.bbc.com/sport', 'https://www.bbc.com/business', 'https://www.bbc.com/innovation', 'https://www.bbc.com/culture', 'https://www.bbc.com/travel', 'https://www.bbc.com/future-planet']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.co.uk/contact\n",
            " Final: https://www.bbc.co.uk/contact\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 150, External ratio: 0.26\n",
            " External sample (<= 20): ['https://account.bbc.com/account', 'https://www.bbc.com/', 'https://www.bbc.com/news', 'https://www.bbc.com/sport', 'https://www.bbc.com/business', 'https://www.bbc.com/innovation', 'https://www.bbc.com/culture', 'https://www.bbc.com/travel', 'https://www.bbc.com/future-planet', 'https://www.bbc.com/video', 'https://www.bbc.com/live', 'https://search.bbc.co.uk/search?scope=bbc&destination=AUDIENCE_SERVICES_PS', 'https://www.bbc.com/', 'https://www.bbc.com/news', 'https://www.bbc.com/sport', 'https://www.bbc.com/business', 'https://www.bbc.com/innovation', 'https://www.bbc.com/culture', 'https://www.bbc.com/travel', 'https://www.bbc.com/future-planet']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/advertisingcontact\n",
            " Final: https://www.bbc.com/advertisingcontact\n",
            " Class: Healthy\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 10, External ratio: 0.5\n",
            " External sample (<= 20): ['https://www.bbc.co.uk/usingthebbc/terms', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/usingthebbc/cookies/how-can-i-change-my-bbc-cookie-settings/\n",
            " Final: https://www.bbc.com/usingthebbc/cookies/how-can-i-change-my-bbc-cookie-settings/\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 102, External ratio: 0.461\n",
            " External sample (<= 20): ['https://www.bbc.co.uk', 'https://www.bbc.co.uk/accessibility/', 'https://account.bbc.com/account', 'https://www.bbc.co.uk/notifications', 'https://www.bbc.co.uk', 'https://www.bbc.co.uk/news', 'https://www.bbc.co.uk/sport', 'https://www.bbc.co.uk/weather', 'https://www.bbc.co.uk/iplayer', 'https://www.bbc.co.uk/sounds', 'https://www.bbc.co.uk/bitesize', 'https://www.bbc.co.uk/cbeebies', 'https://www.bbc.co.uk/cbbc', 'https://www.bbc.co.uk/food', 'https://search.bbc.co.uk/search?scope=all&destination=BBC_CORPORATE_PS', 'https://www.bbc.co.uk', 'https://www.bbc.co.uk/news', 'https://www.bbc.co.uk/sport', 'https://www.bbc.co.uk/weather', 'https://www.bbc.co.uk/iplayer']\n",
            "\n",
            "---\n",
            "URL: https://help.bbc.com/hc/\n",
            " Final: https://help.bbc.com/hc/\n",
            " Class: Healthy\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 0, External ratio: 0.0\n",
            " External sample (<= 20): []\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.com/pages/content-index\n",
            " Final: https://www.bbc.com/pages/content-index\n",
            " Class: Unhealthy (Too Many Links)\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 216, External ratio: 0.032\n",
            " External sample (<= 20): ['https://shop.bbc.com/', 'https://www.britbox.com/?utm_source=bbc.com&utm_medium=referral&utm_campaign=footer', 'https://www.bbc.co.uk/aboutthebbc', 'https://www.bbc.co.uk/accessibility/', 'https://www.bbc.co.uk/contact', 'https://help.bbc.com/hc/', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links']\n",
            "\n",
            "---\n",
            "URL: https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links\n",
            " Final: https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links\n",
            " Class: Healthy\n",
            " Domain age (days): None\n",
            " SSL valid: True\n",
            " Suspicious TLD: False\n",
            " Typosquat: False\n",
            " Homograph: False\n",
            " Randomized label: False\n",
            " Num links: 0, External ratio: 0.0\n",
            " External sample (<= 20): []\n",
            "[INFO] Exported report CSV to phish_report.csv\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Real-time Phishing Detector\n",
        "- Lexical features + Host features + content checks\n",
        "- Rotating headers, retries, jitter\n",
        "- External link validation (1-hop)\n",
        "- Separate handling/classification for:\n",
        "    * Non-HTTPS pages -> Unhealthy (No HTTPS)\n",
        "    * HTTPS with invalid/expired SSL -> Unhealthy (Invalid SSL)\n",
        "- Exports CSV for ML\n",
        "\n",
        "Requirements (install if missing):\n",
        "    pip install requests beautifulsoup4 tldextract python-whois\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import tldextract\n",
        "import whois\n",
        "import socket\n",
        "import ssl\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import csv\n",
        "import datetime\n",
        "import math\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:119.0) Gecko/20100101 Firefox/119.0\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\",\n",
        "]\n",
        "\n",
        "SUSPICIOUS_TLDS = {\"xyz\", \"tk\", \"top\", \"click\", \"link\", \"info\", \"pw\", \"cn\"}\n",
        "KEYWORD_LIST = {\"login\", \"secure\", \"verify\", \"update\", \"bank\", \"free\", \"account\", \"signin\"}\n",
        "BLACKLIST = set()  # populate with known bad domains if you have a list\n",
        "\n",
        "MAX_RETRIES = 3\n",
        "RETRY_BACKOFF = [1, 3, 6]  # seconds\n",
        "REQUEST_TIMEOUT = 10\n",
        "SLEEP_JITTER = (0.8, 2.5)  # seconds between requests to appear human\n",
        "MAX_EXTERNAL_LISTED = 20  # how many external links to store in report for readability\n",
        "\n",
        "# domains to ignore/noise (analytics/ads) to reduce noise\n",
        "SKIP_DOMAINS_CONTAINS = [\"google\", \"googlesyndication\", \"facebook\", \"doubleclick\", \"googletagmanager\", \"instagram\", \"twitter\"]\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Networking helpers\n",
        "# ----------------------------\n",
        "def get_headers():\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(USER_AGENTS),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "        \"DNT\": \"1\",\n",
        "        \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    }\n",
        "\n",
        "\n",
        "def fetch_url(url):\n",
        "    \"\"\"Fetch URL with retries and random headers. Returns requests.Response or None.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=get_headers(), timeout=REQUEST_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "            time.sleep(random.uniform(*SLEEP_JITTER))\n",
        "            return resp\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            wait = RETRY_BACKOFF[min(attempt, len(RETRY_BACKOFF) - 1)]\n",
        "            print(f\"[WARN] Fetch failed ({e}) for {url}, retrying in {wait}s...\")\n",
        "            time.sleep(wait + random.random())\n",
        "    print(f\"[ERROR] Could not fetch {url} after {MAX_RETRIES} retries.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Link extraction & filtering\n",
        "# ----------------------------\n",
        "BAD_HREFS = {\"\", \"#\", \"/\", \"/undefined\"}\n",
        "\n",
        "\n",
        "def is_junk_href(href):\n",
        "    if not href:\n",
        "        return True\n",
        "    href = href.strip()\n",
        "    lower = href.lower()\n",
        "    if lower in BAD_HREFS:\n",
        "        return True\n",
        "    if lower.startswith((\"javascript:\", \"mailto:\", \"tel:\")):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_links(base_url):\n",
        "    \"\"\"Return (links_list, final_url). links filtered to valid http(s) absolute URLs.\"\"\"\n",
        "    resp = fetch_url(base_url)\n",
        "    if not resp:\n",
        "        return [], base_url\n",
        "    try:\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Couldn't parse HTML for {base_url}: {e}\")\n",
        "        return [], resp.url\n",
        "\n",
        "    links = []\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if is_junk_href(href):\n",
        "            continue\n",
        "        abs_url = urljoin(base_url, href)\n",
        "        parsed = urlparse(abs_url)\n",
        "        if parsed.scheme not in (\"http\", \"https\"):\n",
        "            continue\n",
        "        # skip noise domains\n",
        "        if any(x in parsed.netloc for x in SKIP_DOMAINS_CONTAINS):\n",
        "            continue\n",
        "        links.append(abs_url)\n",
        "    return links, resp.url\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Lexical feature helpers\n",
        "# ----------------------------\n",
        "IP_RE = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
        "\n",
        "\n",
        "def url_length(url):\n",
        "    return len(url)\n",
        "\n",
        "\n",
        "def num_subdomains(domain):\n",
        "    # Use tldextract to get subdomain reliably\n",
        "    ext = tldextract.extract(domain)\n",
        "    sub = ext.subdomain\n",
        "    if not sub:\n",
        "        return 0\n",
        "    return len(sub.split(\".\"))\n",
        "\n",
        "\n",
        "def count_special_chars(url):\n",
        "    specials = ['@', '-', '_', '%', '=', '&']\n",
        "    # ignore query separators if it's a \"normal\" parameter-rich URL\n",
        "    parsed = urlparse(url)\n",
        "    path_plus = parsed.netloc + parsed.path\n",
        "    # count occurrences in path+netloc (not counting query string bias)\n",
        "    return sum(path_plus.count(ch) for ch in specials)\n",
        "\n",
        "\n",
        "def contains_ip(domain):\n",
        "    return bool(IP_RE.match(domain))\n",
        "\n",
        "\n",
        "def typosquat_score(domain, whitelist=None):\n",
        "    # returns True if suspiciously similar to a whitelist domain\n",
        "    if not whitelist:\n",
        "        whitelist = [\"google.com\", \"facebook.com\", \"amazon.com\", \"paypal.com\", \"microsoft.com\", \"apple.com\"]\n",
        "    domain_norm = domain.lower()\n",
        "    for legit in whitelist:\n",
        "        ratio = SequenceMatcher(None, domain_norm, legit).ratio()\n",
        "        # > 0.85 similar but not exact likely typo-squat; tune threshold as needed\n",
        "        if 0.85 < ratio < 0.99 and domain_norm != legit:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def contains_unicode(domain):\n",
        "    # homograph: presence of non-ascii characters in domain\n",
        "    try:\n",
        "        domain.encode(\"ascii\")\n",
        "        return False\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "\n",
        "def label_entropy(domain_label):\n",
        "    # Shannon entropy of domain label\n",
        "    if not domain_label:\n",
        "        return 0.0\n",
        "    freq = {}\n",
        "    for ch in domain_label:\n",
        "        freq[ch] = freq.get(ch, 0) + 1\n",
        "    entropy = 0.0\n",
        "    L = len(domain_label)\n",
        "    for v in freq.values():\n",
        "        p = v / L\n",
        "        entropy -= p * math.log2(p)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def has_randomized_label(domain):\n",
        "    # very high entropy and long label indicates random string\n",
        "    ext = tldextract.extract(domain)\n",
        "    label = ext.subdomain.split(\".\")[-1] if ext.subdomain else ext.domain\n",
        "    ent = label_entropy(label)\n",
        "    return (len(label) >= 8 and ent > 3.8)\n",
        "\n",
        "\n",
        "def contains_suspicious_keyword(url):\n",
        "    u = url.lower()\n",
        "    # we consider keywords in subdomain or path (not query params as strictly suspicious)\n",
        "    return any(k in u for k in KEYWORD_LIST)\n",
        "\n",
        "\n",
        "def has_incorrect_casing(domain):\n",
        "    # mixed-case in domain is unusual\n",
        "    return any(ch.isupper() for ch in domain) and not domain.isupper()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Host-based helpers\n",
        "# ----------------------------\n",
        "def check_ssl_cert(domain):\n",
        "    \"\"\"\n",
        "    Return dict: { 'ssl_valid': bool, 'not_before': datetime or None, 'not_after': datetime or None, 'self_signed': bool }\n",
        "    \"\"\"\n",
        "    out = {\"ssl_valid\": False, \"not_before\": None, \"not_after\": None, \"self_signed\": False}\n",
        "    try:\n",
        "        ctx = ssl.create_default_context()\n",
        "        with socket.create_connection((domain, 443), timeout=6) as sock:\n",
        "            with ctx.wrap_socket(sock, server_hostname=domain) as ssock:\n",
        "                cert = ssock.getpeercert()\n",
        "                # parse dates\n",
        "                def _parse(x):\n",
        "                    try:\n",
        "                        return datetime.datetime.strptime(x, \"%b %d %H:%M:%S %Y %Z\")\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            return datetime.datetime.strptime(x, \"%Y%m%d%H%M%SZ\")\n",
        "                        except:\n",
        "                            return None\n",
        "\n",
        "                not_before = _parse(cert.get(\"notBefore\")) or None\n",
        "                not_after = _parse(cert.get(\"notAfter\")) or None\n",
        "                out[\"not_before\"] = not_before\n",
        "                out[\"not_after\"] = not_after\n",
        "\n",
        "                now = datetime.datetime.utcnow()\n",
        "                if (not_before and now < not_before) or (not_after and now > not_after):\n",
        "                    out[\"ssl_valid\"] = False\n",
        "                else:\n",
        "                    out[\"ssl_valid\"] = True\n",
        "                # crude self-signed detection: issuer == subject\n",
        "                issuer = cert.get(\"issuer\")\n",
        "                subject = cert.get(\"subject\")\n",
        "                out[\"self_signed\"] = (issuer == subject)\n",
        "                return out\n",
        "    except Exception:\n",
        "        return out\n",
        "\n",
        "\n",
        "def get_domain_whois(domain):\n",
        "    \"\"\"\n",
        "    Return dict with domain age (days) and whether registrant is private/hidden.\n",
        "    If whois fails, returns None values.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        w = whois.whois(domain)\n",
        "        creation = w.creation_date\n",
        "        # creation_date may be list, handle it\n",
        "        if isinstance(creation, list):\n",
        "            creation = creation[0] if creation else None\n",
        "        if creation and isinstance(creation, str):\n",
        "            # some whois returns string; attempt parse\n",
        "            creation = datetime.datetime.fromisoformat(creation)\n",
        "        if creation:\n",
        "            age_days = (datetime.datetime.utcnow() - creation).days\n",
        "        else:\n",
        "            age_days = None\n",
        "        registrant = str(w.get(\"org\") or w.get(\"registrant\") or w.get(\"name\") or \"\")\n",
        "        whois_privacy = any(x in str(w).lower() for x in [\"privacy\", \"redacted\", \"whoisguard\", \"contact privacy\", \"private\"])\n",
        "        return {\"age_days\": age_days, \"whois_privacy\": whois_privacy, \"registrant\": registrant}\n",
        "    except Exception:\n",
        "        return {\"age_days\": None, \"whois_privacy\": None, \"registrant\": None}\n",
        "\n",
        "\n",
        "def hosting_country(domain):\n",
        "    \"\"\"\n",
        "    Simple free GeoIP via ipapi.co - fallback to None on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ip = socket.gethostbyname(domain)\n",
        "        # ipapi.co free endpoint\n",
        "        resp = requests.get(f\"https://ipapi.co/{ip}/country/\", timeout=6)\n",
        "        if resp.status_code == 200:\n",
        "            return resp.text.strip()\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def in_blacklist(domain):\n",
        "    # local blacklist check\n",
        "    d = domain.lower()\n",
        "    if d in BLACKLIST:\n",
        "        return True\n",
        "    # also check TLD or domain label variants (tune per needs)\n",
        "    return False\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Page Analyzer\n",
        "# ----------------------------\n",
        "def classify_page(url, features):\n",
        "    \"\"\"\n",
        "    Return final classification string with distinct handling for non-https and invalid/expired SSL.\n",
        "    - features is dict containing 'scheme', 'ssl' dict, etc.\n",
        "    \"\"\"\n",
        "    scheme = features.get(\"scheme\", \"\")\n",
        "    ssl_info = features.get(\"ssl\", {})\n",
        "    # Non-https\n",
        "    if scheme != \"https\":\n",
        "        return \"Unhealthy (No HTTPS)\"\n",
        "    # Has HTTPS but SSL invalid\n",
        "    if ssl_info:\n",
        "        if not ssl_info.get(\"ssl_valid\", False):\n",
        "            # differentiate expired vs future vs self-signed if possible\n",
        "            not_before, not_after = ssl_info.get(\"not_before\"), ssl_info.get(\"not_after\")\n",
        "            if not_before and datetime.datetime.utcnow() < not_before:\n",
        "                return \"Unhealthy (SSL Not Yet Valid)\"\n",
        "            if not_after and datetime.datetime.utcnow() > not_after:\n",
        "                return \"Unhealthy (SSL Expired)\"\n",
        "            if ssl_info.get(\"self_signed\"):\n",
        "                return \"Unhealthy (SSL Self-Signed)\"\n",
        "            return \"Unhealthy (Invalid SSL)\"\n",
        "    # Continue other heuristics based on features:\n",
        "    if features.get(\"suspicious_tld\"):\n",
        "        return \"Unhealthy (Suspicious TLD)\"\n",
        "    if features.get(\"typosquat\"):\n",
        "        return \"Unhealthy (Typosquat)\"\n",
        "    if features.get(\"homograph\"):\n",
        "        return \"Unhealthy (Possible Homograph)\"\n",
        "    if features.get(\"external_ratio\", 0) > 0.7:\n",
        "        return \"Unhealthy (Too Many External Links)\"\n",
        "    if features.get(\"num_links\", 0) > 100:\n",
        "        return \"Unhealthy (Too Many Links)\"\n",
        "    if features.get(\"randomized_label\"):\n",
        "        return \"Unhealthy (Randomized Domain Label)\"\n",
        "    # domain age\n",
        "    age = features.get(\"domain_age_days\")\n",
        "    if age is not None and age < 30:\n",
        "        return \"Unhealthy (New Domain)\"\n",
        "    if features.get(\"blacklisted\"):\n",
        "        return \"Unhealthy (Blacklisted)\"\n",
        "    # default\n",
        "    return \"Healthy\"\n",
        "\n",
        "\n",
        "def analyze_url(url, depth=1, max_pages=500):\n",
        "    \"\"\"\n",
        "    Analyze url and 1-hop neighbors (depth controls recursion; recommended depth=1).\n",
        "    Returns dict: {url: features_dict}\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    report = {}\n",
        "\n",
        "    def _analyze(u, d, parent=None):\n",
        "        if u in seen or len(seen) >= max_pages:\n",
        "            return\n",
        "        seen.add(u)\n",
        "        parsed = urlparse(u)\n",
        "        domain = parsed.netloc.lower()\n",
        "        scheme = parsed.scheme.lower()\n",
        "\n",
        "        # Lexical features\n",
        "        lexical = {}\n",
        "        lexical[\"url_length\"] = url_length(u)\n",
        "        lexical[\"num_subdomains\"] = num_subdomains(domain)\n",
        "        lexical[\"special_char_count\"] = count_special_chars(u)\n",
        "        lexical[\"has_ip\"] = contains_ip(domain)\n",
        "        lexical[\"typosquat\"] = typosquat_score(domain)\n",
        "        lexical[\"homograph\"] = contains_unicode(domain)\n",
        "        lexical[\"randomized_label\"] = has_randomized_label(domain)\n",
        "        lexical[\"contains_keyword\"] = contains_suspicious_keyword(u)\n",
        "        lexical[\"incorrect_casing\"] = has_incorrect_casing(domain)\n",
        "        # suspicious tld\n",
        "        ext = tldextract.extract(domain)\n",
        "        lexical[\"tld\"] = ext.suffix\n",
        "        lexical[\"suspicious_tld\"] = ext.suffix.lower() in SUSPICIOUS_TLDS\n",
        "\n",
        "        # Host features\n",
        "        host = {}\n",
        "        # SSL check (only attempt if https)\n",
        "        if scheme == \"https\":\n",
        "            host[\"ssl\"] = check_ssl_cert(domain)\n",
        "        else:\n",
        "            host[\"ssl\"] = {\"ssl_valid\": False, \"not_before\": None, \"not_after\": None, \"self_signed\": False}\n",
        "        # whois (may fail or be slow) - we try once per domain\n",
        "        whois_info = get_domain_whois(domain)\n",
        "        host[\"domain_age_days\"] = whois_info.get(\"age_days\")\n",
        "        host[\"whois_privacy\"] = whois_info.get(\"whois_privacy\")\n",
        "        host[\"registrant\"] = whois_info.get(\"registrant\")\n",
        "        # geoip\n",
        "        host[\"hosting_country\"] = hosting_country(domain)\n",
        "        # blacklist\n",
        "        host[\"blacklisted\"] = in_blacklist(domain)\n",
        "\n",
        "        # Content/features: links\n",
        "        links, final_url = get_links(u)\n",
        "        num_links = len(links)\n",
        "        external_links = [l for l in links if urlparse(l).netloc.lower() != domain]\n",
        "        external_ratio = (len(external_links) / num_links) if num_links > 0 else 0.0\n",
        "\n",
        "        # build features\n",
        "        features = {}\n",
        "        features.update(lexical)\n",
        "        features.update(host)\n",
        "        features[\"scheme\"] = scheme\n",
        "        features[\"num_links\"] = num_links\n",
        "        features[\"external_ratio\"] = round(external_ratio, 3)\n",
        "        features[\"parent\"] = parent\n",
        "        # store up to a few external links for inspection (full crawl optional)\n",
        "        features[\"external_links_sample\"] = external_links[:MAX_EXTERNAL_LISTED]\n",
        "\n",
        "        # classification (handles non-https and invalid ssl separately)\n",
        "        features[\"classification\"] = classify_page(u, {\"scheme\": scheme, \"ssl\": host.get(\"ssl\"), **features})\n",
        "        features[\"url_final\"] = final_url\n",
        "\n",
        "        report[u] = features\n",
        "\n",
        "        # recurse 1 level deep into links if allowed (we will analyze both internal and external links)\n",
        "        if d < depth:\n",
        "            for link in links:\n",
        "                _analyze(link, d + 1, parent=u)\n",
        "\n",
        "    _analyze(url, 0, parent=None)\n",
        "    return report\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# CSV Export helper\n",
        "# ----------------------------\n",
        "def export_report_csv(report, filename=\"phish_report.csv\"):\n",
        "    \"\"\"\n",
        "    Flatten the report dict into CSV rows.\n",
        "    Each row corresponds to one URL analyzed.\n",
        "    \"\"\"\n",
        "    fieldnames = [\n",
        "        \"url\", \"url_final\", \"domain\", \"scheme\", \"tld\", \"classification\",\n",
        "        \"url_length\", \"num_subdomains\", \"special_char_count\", \"has_ip\",\n",
        "        \"typosquat\", \"homograph\", \"randomized_label\", \"contains_keyword\", \"incorrect_casing\",\n",
        "        \"num_links\", \"external_ratio\", \"domain_age_days\", \"whois_privacy\", \"registrant\",\n",
        "        \"hosting_country\", \"blacklisted\", \"ssl_valid\", \"ssl_not_before\", \"ssl_not_after\", \"ssl_self_signed\",\n",
        "        \"parent\", \"external_links_sample\"\n",
        "    ]\n",
        "    rows = []\n",
        "    for url, f in report.items():\n",
        "        parsed = urlparse(url)\n",
        "        domain = parsed.netloc\n",
        "        ssl = f.get(\"ssl\") or {}\n",
        "        row = {\n",
        "            \"url\": url,\n",
        "            \"url_final\": f.get(\"url_final\"),\n",
        "            \"domain\": domain,\n",
        "            \"scheme\": f.get(\"scheme\"),\n",
        "            \"tld\": f.get(\"tld\"),\n",
        "            \"classification\": f.get(\"classification\"),\n",
        "            \"url_length\": f.get(\"url_length\"),\n",
        "            \"num_subdomains\": f.get(\"num_subdomains\"),\n",
        "            \"special_char_count\": f.get(\"special_char_count\"),\n",
        "            \"has_ip\": f.get(\"has_ip\"),\n",
        "            \"typosquat\": f.get(\"typosquat\"),\n",
        "            \"homograph\": f.get(\"homograph\"),\n",
        "            \"randomized_label\": f.get(\"randomized_label\"),\n",
        "            \"contains_keyword\": f.get(\"contains_keyword\"),\n",
        "            \"incorrect_casing\": f.get(\"incorrect_casing\"),\n",
        "            \"num_links\": f.get(\"num_links\"),\n",
        "            \"external_ratio\": f.get(\"external_ratio\"),\n",
        "            \"domain_age_days\": f.get(\"domain_age_days\"),\n",
        "            \"whois_privacy\": f.get(\"whois_privacy\"),\n",
        "            \"registrant\": f.get(\"registrant\"),\n",
        "            \"hosting_country\": f.get(\"hosting_country\"),\n",
        "            \"blacklisted\": f.get(\"blacklisted\"),\n",
        "            \"ssl_valid\": ssl.get(\"ssl_valid\"),\n",
        "            \"ssl_not_before\": ssl.get(\"not_before\"),\n",
        "            \"ssl_not_after\": ssl.get(\"not_after\"),\n",
        "            \"ssl_self_signed\": ssl.get(\"self_signed\"),\n",
        "            \"parent\": f.get(\"parent\"),\n",
        "            \"external_links_sample\": \";\".join(f.get(\"external_links_sample\", [])),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
        "        writer = csv.DictWriter(fh, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            writer.writerow(r)\n",
        "    print(f\"[INFO] Exported report CSV to {filename}\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (main)\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Test URLs (replace or add your own)\n",
        "    TEST_URLS = [\n",
        "        \"https://www.bbc.com/news\",\n",
        "        # \"https://example.com\",\n",
        "        # \"http://192.168.1.1/login\",  # IP example\n",
        "        # \"http://paypa1.com\",  # typosquat example (do not run against actual malicious infra)\n",
        "    ]\n",
        "\n",
        "    final_report = {}\n",
        "    for t in TEST_URLS:\n",
        "        try:\n",
        "            r = analyze_url(t, depth=1)\n",
        "            final_report.update(r)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Interrupted by user.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Unexpected error analyzing {t}: {e}\")\n",
        "\n",
        "    # Pretty print small report summary\n",
        "    for url, f in final_report.items():\n",
        "        print(\"\\n---\")\n",
        "        print(f\"URL: {url}\")\n",
        "        print(f\" Final: {f.get('url_final')}\")\n",
        "        print(f\" Class: {f.get('classification')}\")\n",
        "        print(f\" Domain age (days): {f.get('domain_age_days')}\")\n",
        "        print(f\" SSL valid: {f.get('ssl', {}).get('ssl_valid')}\")\n",
        "        print(f\" Suspicious TLD: {f.get('suspicious_tld')}\")\n",
        "        print(f\" Typosquat: {f.get('typosquat')}\")\n",
        "        print(f\" Homograph: {f.get('homograph')}\")\n",
        "        print(f\" Randomized label: {f.get('randomized_label')}\")\n",
        "        print(f\" Num links: {f.get('num_links')}, External ratio: {f.get('external_ratio')}\")\n",
        "        print(f\" External sample (<= {MAX_EXTERNAL_LISTED}): {f.get('external_links_sample')}\")\n",
        "\n",
        "    # Export CSV for ML\n",
        "    export_report_csv(final_report, filename=\"phish_report.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td95hf2L_XNN"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Real-time Phishing Detector\n",
        "- Lexical features + Host features + content checks\n",
        "- Rotating headers, retries, jitter\n",
        "- External link validation (1-hop)\n",
        "- Separate handling/classification for:\n",
        "    * Non-HTTPS pages -> Unhealthy (No HTTPS)\n",
        "    * HTTPS with invalid/expired SSL -> Unhealthy (Invalid SSL)\n",
        "- Exports CSV for ML\n",
        "\n",
        "Requirements (install if missing):\n",
        "    pip install requests beautifulsoup4 tldextract python-whois\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import tldextract\n",
        "import whois\n",
        "import socket\n",
        "import ssl\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import csv\n",
        "import datetime\n",
        "import math\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:119.0) Gecko/20100101 Firefox/119.0\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\",\n",
        "]\n",
        "\n",
        "SUSPICIOUS_TLDS = {\"xyz\", \"tk\", \"top\", \"click\", \"link\", \"info\", \"pw\", \"cn\"}\n",
        "KEYWORD_LIST = {\"login\", \"secure\", \"verify\", \"update\", \"bank\", \"free\", \"account\", \"signin\"}\n",
        "BLACKLIST = set()  # populate with known bad domains if you have a list\n",
        "\n",
        "MAX_RETRIES = 3\n",
        "RETRY_BACKOFF = [1, 3, 6]  # seconds\n",
        "REQUEST_TIMEOUT = 10\n",
        "SLEEP_JITTER = (0.8, 2.5)  # seconds between requests to appear human\n",
        "MAX_EXTERNAL_LISTED = 20  # how many external links to store in report for readability\n",
        "\n",
        "# domains to ignore/noise (analytics/ads) to reduce noise\n",
        "SKIP_DOMAINS_CONTAINS = [\"google\", \"googlesyndication\", \"facebook\", \"doubleclick\", \"googletagmanager\", \"instagram\", \"twitter\"]\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Networking helpers\n",
        "# ----------------------------\n",
        "def get_headers():\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(USER_AGENTS),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "        \"DNT\": \"1\",\n",
        "        \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    }\n",
        "\n",
        "\n",
        "def fetch_url(url):\n",
        "    \"\"\"Fetch URL with retries and random headers. Returns requests.Response or None.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=get_headers(), timeout=REQUEST_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "            time.sleep(random.uniform(*SLEEP_JITTER))\n",
        "            return resp\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            wait = RETRY_BACKOFF[min(attempt, len(RETRY_BACKOFF) - 1)]\n",
        "            print(f\"[WARN] Fetch failed ({e}) for {url}, retrying in {wait}s...\")\n",
        "            time.sleep(wait + random.random())\n",
        "    print(f\"[ERROR] Could not fetch {url} after {MAX_RETRIES} retries.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Link extraction & filtering\n",
        "# ----------------------------\n",
        "BAD_HREFS = {\"\", \"#\", \"/\", \"/undefined\"}\n",
        "\n",
        "\n",
        "def is_junk_href(href):\n",
        "    if not href:\n",
        "        return True\n",
        "    href = href.strip()\n",
        "    lower = href.lower()\n",
        "    if lower in BAD_HREFS:\n",
        "        return True\n",
        "    if lower.startswith((\"javascript:\", \"mailto:\", \"tel:\")):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_links(base_url):\n",
        "    \"\"\"Return (links_list, final_url). links filtered to valid http(s) absolute URLs.\"\"\"\n",
        "    resp = fetch_url(base_url)\n",
        "    if not resp:\n",
        "        return [], base_url\n",
        "    try:\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Couldn't parse HTML for {base_url}: {e}\")\n",
        "        return [], resp.url\n",
        "\n",
        "    links = []\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if is_junk_href(href):\n",
        "            continue\n",
        "        abs_url = urljoin(base_url, href)\n",
        "        parsed = urlparse(abs_url)\n",
        "        if parsed.scheme not in (\"http\", \"https\"):\n",
        "            continue\n",
        "        # skip noise domains\n",
        "        if any(x in parsed.netloc for x in SKIP_DOMAINS_CONTAINS):\n",
        "            continue\n",
        "        links.append(abs_url)\n",
        "    return links, resp.url\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Lexical feature helpers\n",
        "# ----------------------------\n",
        "IP_RE = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
        "\n",
        "\n",
        "def url_length(url):\n",
        "    return len(url)\n",
        "\n",
        "\n",
        "def num_subdomains(domain):\n",
        "    # Use tldextract to get subdomain reliably\n",
        "    ext = tldextract.extract(domain)\n",
        "    sub = ext.subdomain\n",
        "    if not sub:\n",
        "        return 0\n",
        "    return len(sub.split(\".\"))\n",
        "\n",
        "\n",
        "def count_special_chars(url):\n",
        "    specials = ['@', '-', '_', '%', '=', '&']\n",
        "    # ignore query separators if it's a \"normal\" parameter-rich URL\n",
        "    parsed = urlparse(url)\n",
        "    path_plus = parsed.netloc + parsed.path\n",
        "    # count occurrences in path+netloc (not counting query string bias)\n",
        "    return sum(path_plus.count(ch) for ch in specials)\n",
        "\n",
        "\n",
        "def contains_ip(domain):\n",
        "    return bool(IP_RE.match(domain))\n",
        "\n",
        "\n",
        "def typosquat_score(domain, whitelist=None):\n",
        "    # returns True if suspiciously similar to a whitelist domain\n",
        "    if not whitelist:\n",
        "        whitelist = [\"google.com\", \"facebook.com\", \"amazon.com\", \"paypal.com\", \"microsoft.com\", \"apple.com\"]\n",
        "    domain_norm = domain.lower()\n",
        "    for legit in whitelist:\n",
        "        ratio = SequenceMatcher(None, domain_norm, legit).ratio()\n",
        "        # > 0.85 similar but not exact likely typo-squat; tune threshold as needed\n",
        "        if 0.85 < ratio < 0.99 and domain_norm != legit:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def contains_unicode(domain):\n",
        "    # homograph: presence of non-ascii characters in domain\n",
        "    try:\n",
        "        domain.encode(\"ascii\")\n",
        "        return False\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "\n",
        "def label_entropy(domain_label):\n",
        "    # Shannon entropy of domain label\n",
        "    if not domain_label:\n",
        "        return 0.0\n",
        "    freq = {}\n",
        "    for ch in domain_label:\n",
        "        freq[ch] = freq.get(ch, 0) + 1\n",
        "    entropy = 0.0\n",
        "    L = len(domain_label)\n",
        "    for v in freq.values():\n",
        "        p = v / L\n",
        "        entropy -= p * math.log2(p)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def has_randomized_label(domain):\n",
        "    # very high entropy and long label indicates random string\n",
        "    ext = tldextract.extract(domain)\n",
        "    label = ext.subdomain.split(\".\")[-1] if ext.subdomain else ext.domain\n",
        "    ent = label_entropy(label)\n",
        "    return (len(label) >= 8 and ent > 3.8)\n",
        "\n",
        "\n",
        "def contains_suspicious_keyword(url):\n",
        "    u = url.lower()\n",
        "    # we consider keywords in subdomain or path (not query params as strictly suspicious)\n",
        "    return any(k in u for k in KEYWORD_LIST)\n",
        "\n",
        "\n",
        "def has_incorrect_casing(domain):\n",
        "    # mixed-case in domain is unusual\n",
        "    return any(ch.isupper() for ch in domain) and not domain.isupper()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Host-based helpers\n",
        "# ----------------------------\n",
        "def check_ssl_cert(domain):\n",
        "    \"\"\"\n",
        "    Return dict: { 'ssl_valid': bool, 'not_before': datetime or None, 'not_after': datetime or None, 'self_signed': bool }\n",
        "    \"\"\"\n",
        "    out = {\"ssl_valid\": False, \"not_before\": None, \"not_after\": None, \"self_signed\": False}\n",
        "    try:\n",
        "        ctx = ssl.create_default_context()\n",
        "        with socket.create_connection((domain, 443), timeout=6) as sock:\n",
        "            with ctx.wrap_socket(sock, server_hostname=domain) as ssock:\n",
        "                cert = ssock.getpeercert()\n",
        "                # parse dates\n",
        "                def _parse(x):\n",
        "                    try:\n",
        "                        return datetime.datetime.strptime(x, \"%b %d %H:%M:%S %Y %Z\")\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            return datetime.datetime.strptime(x, \"%Y%m%d%H%M%SZ\")\n",
        "                        except:\n",
        "                            return None\n",
        "\n",
        "                not_before = _parse(cert.get(\"notBefore\")) or None\n",
        "                not_after = _parse(cert.get(\"notAfter\")) or None\n",
        "                out[\"not_before\"] = not_before\n",
        "                out[\"not_after\"] = not_after\n",
        "\n",
        "                now = datetime.datetime.utcnow()\n",
        "                if (not_before and now < not_before) or (not_after and now > not_after):\n",
        "                    out[\"ssl_valid\"] = False\n",
        "                else:\n",
        "                    out[\"ssl_valid\"] = True\n",
        "                # crude self-signed detection: issuer == subject\n",
        "                issuer = cert.get(\"issuer\")\n",
        "                subject = cert.get(\"subject\")\n",
        "                out[\"self_signed\"] = (issuer == subject)\n",
        "                return out\n",
        "    except Exception:\n",
        "        return out\n",
        "\n",
        "\n",
        "def get_domain_whois(domain):\n",
        "    \"\"\"\n",
        "    Return dict with domain age (days) and whether registrant is private/hidden.\n",
        "    If whois fails, returns None values.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        w = whois.whois(domain)\n",
        "        creation = w.creation_date\n",
        "        # creation_date may be list, handle it\n",
        "        if isinstance(creation, list):\n",
        "            creation = creation[0] if creation else None\n",
        "        if creation and isinstance(creation, str):\n",
        "            # some whois returns string; attempt parse\n",
        "            creation = datetime.datetime.fromisoformat(creation)\n",
        "        if creation:\n",
        "            age_days = (datetime.datetime.utcnow() - creation).days\n",
        "        else:\n",
        "            age_days = None\n",
        "        registrant = str(w.get(\"org\") or w.get(\"registrant\") or w.get(\"name\") or \"\")\n",
        "        whois_privacy = any(x in str(w).lower() for x in [\"privacy\", \"redacted\", \"whoisguard\", \"contact privacy\", \"private\"])\n",
        "        return {\"age_days\": age_days, \"whois_privacy\": whois_privacy, \"registrant\": registrant}\n",
        "    except Exception:\n",
        "        return {\"age_days\": None, \"whois_privacy\": None, \"registrant\": None}\n",
        "\n",
        "\n",
        "def hosting_country(domain):\n",
        "    \"\"\"\n",
        "    Simple free GeoIP via ipapi.co - fallback to None on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ip = socket.gethostbyname(domain)\n",
        "        # ipapi.co free endpoint\n",
        "        resp = requests.get(f\"https://ipapi.co/{ip}/country/\", timeout=6)\n",
        "        if resp.status_code == 200:\n",
        "            return resp.text.strip()\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def in_blacklist(domain):\n",
        "    # local blacklist check\n",
        "    d = domain.lower()\n",
        "    if d in BLACKLIST:\n",
        "        return True\n",
        "    # also check TLD or domain label variants (tune per needs)\n",
        "    return False\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Page Analyzer\n",
        "# ----------------------------\n",
        "def classify_page(url, features):\n",
        "    \"\"\"\n",
        "    Return final classification string with distinct handling for non-https and invalid/expired SSL.\n",
        "    - features is dict containing 'scheme', 'ssl' dict, etc.\n",
        "    \"\"\"\n",
        "    scheme = features.get(\"scheme\", \"\")\n",
        "    ssl_info = features.get(\"ssl\", {})\n",
        "    # Non-https\n",
        "    if scheme != \"https\":\n",
        "        return \"Unhealthy (No HTTPS)\"\n",
        "    # Has HTTPS but SSL invalid\n",
        "    if ssl_info:\n",
        "        if not ssl_info.get(\"ssl_valid\", False):\n",
        "            # differentiate expired vs future vs self-signed if possible\n",
        "            not_before, not_after = ssl_info.get(\"not_before\"), ssl_info.get(\"not_after\")\n",
        "            if not_before and datetime.datetime.utcnow() < not_before:\n",
        "                return \"Unhealthy (SSL Not Yet Valid)\"\n",
        "            if not_after and datetime.datetime.utcnow() > not_after:\n",
        "                return \"Unhealthy (SSL Expired)\"\n",
        "            if ssl_info.get(\"self_signed\"):\n",
        "                return \"Unhealthy (SSL Self-Signed)\"\n",
        "            return \"Unhealthy (Invalid SSL)\"\n",
        "    # Continue other heuristics based on features:\n",
        "    if features.get(\"suspicious_tld\"):\n",
        "        return \"Unhealthy (Suspicious TLD)\"\n",
        "    if features.get(\"typosquat\"):\n",
        "        return \"Unhealthy (Typosquat)\"\n",
        "    if features.get(\"homograph\"):\n",
        "        return \"Unhealthy (Possible Homograph)\"\n",
        "    if features.get(\"external_ratio\", 0) > 0.7:\n",
        "        return \"Unhealthy (Too Many External Links)\"\n",
        "    if features.get(\"num_links\", 0) > 100:\n",
        "        return \"Unhealthy (Too Many Links)\"\n",
        "    if features.get(\"randomized_label\"):\n",
        "        return \"Unhealthy (Randomized Domain Label)\"\n",
        "    # domain age\n",
        "    age = features.get(\"domain_age_days\")\n",
        "    if age is not None and age < 30:\n",
        "        return \"Unhealthy (New Domain)\"\n",
        "    if features.get(\"blacklisted\"):\n",
        "        return \"Unhealthy (Blacklisted)\"\n",
        "    # default\n",
        "    return \"Healthy\"\n",
        "\n",
        "\n",
        "def analyze_url(url, depth=1, max_pages=500):\n",
        "    \"\"\"\n",
        "    Analyze url and 1-hop neighbors (depth controls recursion; recommended depth=1).\n",
        "    Returns dict: {url: features_dict}\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    report = {}\n",
        "\n",
        "    def _analyze(u, d, parent=None):\n",
        "        if u in seen or len(seen) >= max_pages:\n",
        "            return\n",
        "        seen.add(u)\n",
        "        parsed = urlparse(u)\n",
        "        domain = parsed.netloc.lower()\n",
        "        scheme = parsed.scheme.lower()\n",
        "\n",
        "        # Lexical features\n",
        "        lexical = {}\n",
        "        lexical[\"url_length\"] = url_length(u)\n",
        "        lexical[\"num_subdomains\"] = num_subdomains(domain)\n",
        "        lexical[\"special_char_count\"] = count_special_chars(u)\n",
        "        lexical[\"has_ip\"] = contains_ip(domain)\n",
        "        lexical[\"typosquat\"] = typosquat_score(domain)\n",
        "        lexical[\"homograph\"] = contains_unicode(domain)\n",
        "        lexical[\"randomized_label\"] = has_randomized_label(domain)\n",
        "        lexical[\"contains_keyword\"] = contains_suspicious_keyword(u)\n",
        "        lexical[\"incorrect_casing\"] = has_incorrect_casing(domain)\n",
        "        # suspicious tld\n",
        "        ext = tldextract.extract(domain)\n",
        "        lexical[\"tld\"] = ext.suffix\n",
        "        lexical[\"suspicious_tld\"] = ext.suffix.lower() in SUSPICIOUS_TLDS\n",
        "\n",
        "        # Host features\n",
        "        host = {}\n",
        "        # SSL check (only attempt if https)\n",
        "        if scheme == \"https\":\n",
        "            host[\"ssl\"] = check_ssl_cert(domain)\n",
        "        else:\n",
        "            host[\"ssl\"] = {\"ssl_valid\": False, \"not_before\": None, \"not_after\": None, \"self_signed\": False}\n",
        "        # whois (may fail or be slow) - we try once per domain\n",
        "        whois_info = get_domain_whois(domain)\n",
        "        host[\"domain_age_days\"] = whois_info.get(\"age_days\")\n",
        "        host[\"whois_privacy\"] = whois_info.get(\"whois_privacy\")\n",
        "        host[\"registrant\"] = whois_info.get(\"registrant\")\n",
        "        # geoip\n",
        "        host[\"hosting_country\"] = hosting_country(domain)\n",
        "        # blacklist\n",
        "        host[\"blacklisted\"] = in_blacklist(domain)\n",
        "\n",
        "        # Content/features: links\n",
        "        links, final_url = get_links(u)\n",
        "        num_links = len(links)\n",
        "        external_links = [l for l in links if urlparse(l).netloc.lower() != domain]\n",
        "        external_ratio = (len(external_links) / num_links) if num_links > 0 else 0.0\n",
        "\n",
        "        # build features\n",
        "        features = {}\n",
        "        features.update(lexical)\n",
        "        features.update(host)\n",
        "        features[\"scheme\"] = scheme\n",
        "        features[\"num_links\"] = num_links\n",
        "        features[\"external_ratio\"] = round(external_ratio, 3)\n",
        "        features[\"parent\"] = parent\n",
        "        # store up to a few external links for inspection (full crawl optional)\n",
        "        features[\"external_links_sample\"] = external_links[:MAX_EXTERNAL_LISTED]\n",
        "\n",
        "        # classification (handles non-https and invalid ssl separately)\n",
        "        features[\"classification\"] = classify_page(u, {\"scheme\": scheme, \"ssl\": host.get(\"ssl\"), **features})\n",
        "        features[\"url_final\"] = final_url\n",
        "\n",
        "        report[u] = features\n",
        "\n",
        "        # recurse 1 level deep into links if allowed (we will analyze both internal and external links)\n",
        "        if d < depth:\n",
        "            for link in links:\n",
        "                _analyze(link, d + 1, parent=u)\n",
        "\n",
        "    _analyze(url, 0, parent=None)\n",
        "    return report\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# CSV Export helper\n",
        "# ----------------------------\n",
        "def export_report_csv(report, filename=\"phish_report.csv\"):\n",
        "    \"\"\"\n",
        "    Flatten the report dict into CSV rows.\n",
        "    Each row corresponds to one URL analyzed.\n",
        "    \"\"\"\n",
        "    fieldnames = [\n",
        "        \"url\", \"url_final\", \"domain\", \"scheme\", \"tld\", \"classification\",\n",
        "        \"url_length\", \"num_subdomains\", \"special_char_count\", \"has_ip\",\n",
        "        \"typosquat\", \"homograph\", \"randomized_label\", \"contains_keyword\", \"incorrect_casing\",\n",
        "        \"num_links\", \"external_ratio\", \"domain_age_days\", \"whois_privacy\", \"registrant\",\n",
        "        \"hosting_country\", \"blacklisted\", \"ssl_valid\", \"ssl_not_before\", \"ssl_not_after\", \"ssl_self_signed\",\n",
        "        \"parent\", \"external_links_sample\"\n",
        "    ]\n",
        "    rows = []\n",
        "    for url, f in report.items():\n",
        "        parsed = urlparse(url)\n",
        "        domain = parsed.netloc\n",
        "        ssl = f.get(\"ssl\") or {}\n",
        "        row = {\n",
        "            \"url\": url,\n",
        "            \"url_final\": f.get(\"url_final\"),\n",
        "            \"domain\": domain,\n",
        "            \"scheme\": f.get(\"scheme\"),\n",
        "            \"tld\": f.get(\"tld\"),\n",
        "            \"classification\": f.get(\"classification\"),\n",
        "            \"url_length\": f.get(\"url_length\"),\n",
        "            \"num_subdomains\": f.get(\"num_subdomains\"),\n",
        "            \"special_char_count\": f.get(\"special_char_count\"),\n",
        "            \"has_ip\": f.get(\"has_ip\"),\n",
        "            \"typosquat\": f.get(\"typosquat\"),\n",
        "            \"homograph\": f.get(\"homograph\"),\n",
        "            \"randomized_label\": f.get(\"randomized_label\"),\n",
        "            \"contains_keyword\": f.get(\"contains_keyword\"),\n",
        "            \"incorrect_casing\": f.get(\"incorrect_casing\"),\n",
        "            \"num_links\": f.get(\"num_links\"),\n",
        "            \"external_ratio\": f.get(\"external_ratio\"),\n",
        "            \"domain_age_days\": f.get(\"domain_age_days\"),\n",
        "            \"whois_privacy\": f.get(\"whois_privacy\"),\n",
        "            \"registrant\": f.get(\"registrant\"),\n",
        "            \"hosting_country\": f.get(\"hosting_country\"),\n",
        "            \"blacklisted\": f.get(\"blacklisted\"),\n",
        "            \"ssl_valid\": ssl.get(\"ssl_valid\"),\n",
        "            \"ssl_not_before\": ssl.get(\"not_before\"),\n",
        "            \"ssl_not_after\": ssl.get(\"not_after\"),\n",
        "            \"ssl_self_signed\": ssl.get(\"self_signed\"),\n",
        "            \"parent\": f.get(\"parent\"),\n",
        "            \"external_links_sample\": \";\".join(f.get(\"external_links_sample\", [])),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
        "        writer = csv.DictWriter(fh, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            writer.writerow(r)\n",
        "    print(f\"[INFO] Exported report CSV to {filename}\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (main)\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Test URLs (replace or add your own)\n",
        "    TEST_URLS = [\n",
        "        \"https://www.google.com/search?q=data+frame+see+the+full+content+of+the+cell&sxsrf=AE3TifNzPa2Ab-WiFqjPxAgA3QVCxI5vEQ%3A1756538031692\",\n",
        "        #\"https://www.bbc.com/news\",\n",
        "        \"https://example.com\",\n",
        "        \"http://192.168.1.1/login\",  # IP example\n",
        "        \"http://paypa1.com\",  # typosquat example (do not run against actual malicious infra)\n",
        "    ]\n",
        "\n",
        "    final_report = {}\n",
        "    for t in TEST_URLS:\n",
        "        try:\n",
        "            r = analyze_url(t, depth=1)\n",
        "            final_report.update(r)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Interrupted by user.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Unexpected error analyzing {t}: {e}\")\n",
        "\n",
        "    # Pretty print small report summary\n",
        "    for url, f in final_report.items():\n",
        "        print(\"\\n---\")\n",
        "        print(f\"URL: {url}\")\n",
        "        print(f\" Final: {f.get('url_final')}\")\n",
        "        print(f\" Class: {f.get('classification')}\")\n",
        "        print(f\" Domain age (days): {f.get('domain_age_days')}\")\n",
        "        print(f\" SSL valid: {f.get('ssl', {}).get('ssl_valid')}\")\n",
        "        print(f\" Suspicious TLD: {f.get('suspicious_tld')}\")\n",
        "        print(f\" Typosquat: {f.get('typosquat')}\")\n",
        "        print(f\" Homograph: {f.get('homograph')}\")\n",
        "        print(f\" Randomized label: {f.get('randomized_label')}\")\n",
        "        print(f\" Num links: {f.get('num_links')}, External ratio: {f.get('external_ratio')}\")\n",
        "        print(f\" External sample (<= {MAX_EXTERNAL_LISTED}): {f.get('external_links_sample')}\")\n",
        "\n",
        "    # Export CSV for ML\n",
        "    export_report_csv(final_report, filename=\"phish_report.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCmJhSmw_V8R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPF4xvfUN_1p"
      },
      "source": [
        "#### END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq_fGIHvN_ya"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEGK8NHiN_v8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS9PQ3o2N_tS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx9Y7RswN_qM"
      },
      "source": [
        "### **All 4 Categories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG-HMRjw3M7c",
        "outputId": "fb1a2efc-5f3e-453e-9082-4cfc08d4cb01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-whois\n",
            "  Downloading python_whois-0.9.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.19.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from python-whois) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->python-whois) (1.17.0)\n",
            "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m730.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_whois-0.9.5-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, python-whois, tldextract\n",
            "Successfully installed python-whois-0.9.5 requests-file-2.1.0 tldextract-5.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 tldextract python-whois"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK-hqgPhOE9x",
        "outputId": "3f6f1690-20a5-4bc6-9ace-6c98af23983b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Analyzing: https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html (depth=0)\n",
            "[WARN] Fetch failed (403 Client Error: Forbidden for url: https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html) for https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html — retrying in 1s...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-136401911.py:149: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
            "  features[\"has_ip\"] = bool(IP_RE.match(ext.registered_domain or domain))\n",
            "/tmp/ipython-input-136401911.py:152: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
            "  domain_norm = (ext.registered_domain or domain).lower()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Fetch failed (403 Client Error: Forbidden for url: https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html) for https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html — retrying in 3s...\n",
            "[WARN] Fetch failed (403 Client Error: Forbidden for url: https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html) for https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html — retrying in 6s...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-30 12:04:44,045 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR] Could not fetch https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html after 3 retries.\n",
            "\n",
            "---------------------------------\n",
            "URL: https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html\n",
            "Final URL: https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html\n",
            "Classification: Healthy\n",
            "Reasons: []\n",
            "Domain age (days): None\n",
            "SSL valid: True\n",
            "Suspicious TLD: False\n",
            "Typosquat: False\n",
            "Homograph: False\n",
            "Randomized label: False\n",
            "Num links: 0, External ratio: 0.0\n",
            "External sample: []\n",
            "[INFO] CSV exported to phish_report.csv\n",
            "[INFO] JSON exported to phish_report.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-136401911.py:259: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  now = datetime.datetime.utcnow()\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "phish_detector.py\n",
        "\n",
        "Real-time Phishing Detector (modular)\n",
        "- Lexical features\n",
        "- Host-based features (WHOIS, SSL, GeoIP, blacklist)\n",
        "- Content-based features (redirects, iframes, login forms, brand mismatch)\n",
        "- Behavioral features (file downloads, mouseover mismatch, popups)\n",
        "- 1-hop crawling and external link validation\n",
        "- Exports CSV and JSON report\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "import socket\n",
        "import ssl\n",
        "import datetime\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tldextract\n",
        "import whois\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:119.0) Gecko/20100101 Firefox/119.0\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\",\n",
        "]\n",
        "\n",
        "SUSPICIOUS_TLDS = {\"xyz\", \"tk\", \"top\", \"click\", \"link\", \"info\", \"pw\", \"cn\"}\n",
        "KEYWORD_LIST = {\"login\", \"secure\", \"verify\", \"update\", \"bank\", \"free\", \"account\", \"signin\"}\n",
        "BLACKLIST = set()  # add known bad domains if available\n",
        "MAX_RETRIES = 3\n",
        "RETRY_BACKOFF = [1, 3, 6]\n",
        "REQUEST_TIMEOUT = 10\n",
        "SLEEP_JITTER = (0.8, 2.0)\n",
        "MAX_EXTERNAL_SAMPLE = 20\n",
        "SKIP_DOMAINS_CONTAINS = [\"googletagmanager\", \"googlesyndication\", \"doubleclick\", \"facebook\", \"instagram\", \"twitter\"]\n",
        "BRAND_LIST = [\"paypal\", \"google\", \"amazon\", \"facebook\", \"microsoft\", \"apple\", \"bank\"]  # extend as needed\n",
        "\n",
        "# ---------------------------\n",
        "# Networking helpers\n",
        "# ---------------------------\n",
        "def get_headers():\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(USER_AGENTS),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "        \"DNT\": \"1\",\n",
        "        \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    }\n",
        "\n",
        "\n",
        "def fetch_url(url, allow_redirects=True):\n",
        "    \"\"\"Fetch a URL with retries and randomized headers. Returns requests.Response or None.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=get_headers(), timeout=REQUEST_TIMEOUT, allow_redirects=allow_redirects)\n",
        "            resp.raise_for_status()\n",
        "            time.sleep(random.uniform(*SLEEP_JITTER))\n",
        "            return resp\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            wait = RETRY_BACKOFF[min(attempt, len(RETRY_BACKOFF) - 1)]\n",
        "            print(f\"[WARN] Fetch failed ({e}) for {url} — retrying in {wait}s...\")\n",
        "            time.sleep(wait + random.random())\n",
        "    print(f\"[ERROR] Could not fetch {url} after {MAX_RETRIES} retries.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Link extraction & sanitization\n",
        "# ---------------------------\n",
        "BAD_HREFS = {\"\", \"#\", \"/\", \"/undefined\"}\n",
        "\n",
        "\n",
        "def is_junk_href(href):\n",
        "    if not href:\n",
        "        return True\n",
        "    lower = href.strip().lower()\n",
        "    if lower in BAD_HREFS:\n",
        "        return True\n",
        "    if lower.startswith((\"javascript:\", \"mailto:\", \"tel:\", \"data:\")):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def extract_links(base_url, resp):\n",
        "    \"\"\"\n",
        "    Extract cleaned absolute http/https links from a BeautifulSoup-parsed page or response text.\n",
        "    Returns list of absolute URLs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    except Exception:\n",
        "        soup = None\n",
        "\n",
        "    links = []\n",
        "    if not soup:\n",
        "        return links, None\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if is_junk_href(href):\n",
        "            continue\n",
        "        abs_url = urljoin(base_url, href)\n",
        "        parsed = urlparse(abs_url)\n",
        "        if parsed.scheme not in (\"http\", \"https\"):\n",
        "            continue\n",
        "        # skip analytics/ad domains to reduce noise\n",
        "        if any(skip in parsed.netloc for skip in SKIP_DOMAINS_CONTAINS):\n",
        "            continue\n",
        "        links.append(abs_url)\n",
        "    return links, soup\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Lexical features module\n",
        "# ---------------------------\n",
        "IP_RE = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
        "\n",
        "\n",
        "def lexical_features(url):\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc\n",
        "    ext = tldextract.extract(domain)\n",
        "\n",
        "    features = {}\n",
        "    features[\"url_length\"] = len(url)\n",
        "    features[\"scheme\"] = parsed.scheme.lower()\n",
        "    features[\"domain\"] = domain.lower()\n",
        "    features[\"tld\"] = ext.suffix.lower() if ext.suffix else \"\"\n",
        "    features[\"subdomain_count\"] = 0 if not ext.subdomain else len(ext.subdomain.split(\".\"))\n",
        "    # special chars in netloc + path (ignore query portion to not penalize normal queries)\n",
        "    path_netloc = parsed.netloc + parsed.path\n",
        "    features[\"special_char_count\"] = sum(path_netloc.count(ch) for ch in ['@', '-', '_', '%', '=', '&'])\n",
        "    features[\"has_ip\"] = bool(IP_RE.match(ext.registered_domain or domain))\n",
        "    # typosquat (compare registered_domain to whitelist)\n",
        "    whitelist = [\"google.com\", \"facebook.com\", \"amazon.com\", \"paypal.com\", \"microsoft.com\", \"apple.com\"]\n",
        "    domain_norm = (ext.registered_domain or domain).lower()\n",
        "    def typosquat_check(dnm):\n",
        "        for legit in whitelist:\n",
        "            ratio = SequenceMatcher(None, dnm, legit).ratio()\n",
        "            if 0.85 < ratio < 0.995 and dnm != legit:\n",
        "                return True\n",
        "        return False\n",
        "    features[\"typosquat\"] = typosquat_check(domain_norm)\n",
        "    # homograph (non-ascii characters in domain)\n",
        "    try:\n",
        "        domain.encode(\"ascii\")\n",
        "        features[\"homograph\"] = False\n",
        "    except Exception:\n",
        "        features[\"homograph\"] = True\n",
        "    # label entropy (check domain label randomness)\n",
        "    label = ext.domain or \"\"\n",
        "    if label:\n",
        "        freq = Counter(label)\n",
        "        L = len(label)\n",
        "        entropy = -sum((v / L) * math.log2(v / L) for v in freq.values())\n",
        "    else:\n",
        "        entropy = 0.0\n",
        "    features[\"label_entropy\"] = round(entropy, 3)\n",
        "    features[\"randomized_label\"] = (len(label) >= 8 and entropy > 3.8)\n",
        "    features[\"contains_keyword\"] = any(k in (parsed.path + \".\".join([ext.subdomain or \"\"])).lower() for k in KEYWORD_LIST)\n",
        "    features[\"incorrect_casing\"] = any(ch.isupper() for ch in domain) and not domain.isupper()\n",
        "    features[\"suspicious_tld\"] = features[\"tld\"] in SUSPICIOUS_TLDS\n",
        "    return features\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Host-based features module\n",
        "# ---------------------------\n",
        "WHOIS_CACHE = {}\n",
        "GEOIP_CACHE = {}\n",
        "SSL_CACHE = {}\n",
        "\n",
        "\n",
        "def get_whois(domain):\n",
        "    if domain in WHOIS_CACHE:\n",
        "        return WHOIS_CACHE[domain]\n",
        "    result = {\"age_days\": None, \"whois_privacy\": None, \"registrant\": None}\n",
        "    try:\n",
        "        w = whois.whois(domain)\n",
        "        creation = w.creation_date\n",
        "        if isinstance(creation, list):\n",
        "            creation = creation[0] if creation else None\n",
        "        if isinstance(creation, str):\n",
        "            try:\n",
        "                creation = datetime.datetime.fromisoformat(creation)\n",
        "            except Exception:\n",
        "                creation = None\n",
        "        if creation:\n",
        "            age_days = (datetime.datetime.utcnow() - creation).days\n",
        "        else:\n",
        "            age_days = None\n",
        "        registrant = (w.get(\"org\") or w.get(\"name\") or \"\") if isinstance(w, dict) or hasattr(w, \"get\") else \"\"\n",
        "        whois_privacy = any(x in str(w).lower() for x in [\"privacy\", \"redacted\", \"whoisguard\", \"contact privacy\", \"protected\"])\n",
        "        result = {\"age_days\": age_days, \"whois_privacy\": whois_privacy, \"registrant\": registrant}\n",
        "    except Exception as e:\n",
        "        # whois may fail; keep None values\n",
        "        pass\n",
        "    WHOIS_CACHE[domain] = result\n",
        "    return result\n",
        "\n",
        "\n",
        "def geolocate_domain(domain):\n",
        "    \"\"\"Simple GeoIP using ipapi.co; cached. Returns country code or None.\"\"\"\n",
        "    if domain in GEOIP_CACHE:\n",
        "        return GEOIP_CACHE[domain]\n",
        "    try:\n",
        "        ip = socket.gethostbyname(domain)\n",
        "        resp = requests.get(f\"https://ipapi.co/{ip}/country/\", timeout=6)\n",
        "        if resp.status_code == 200:\n",
        "            country = resp.text.strip()\n",
        "            GEOIP_CACHE[domain] = country\n",
        "            return country\n",
        "    except Exception:\n",
        "        pass\n",
        "    GEOIP_CACHE[domain] = None\n",
        "    return None\n",
        "\n",
        "\n",
        "def check_ssl(domain):\n",
        "    \"\"\"Return ssl info dict (ssl_valid, not_before, not_after, self_signed). Cached per domain.\"\"\"\n",
        "    if domain in SSL_CACHE:\n",
        "        return SSL_CACHE[domain]\n",
        "    out = {\"ssl_valid\": False, \"not_before\": None, \"not_after\": None, \"self_signed\": False}\n",
        "    try:\n",
        "        ctx = ssl.create_default_context()\n",
        "        with socket.create_connection((domain, 443), timeout=6) as sock:\n",
        "            with ctx.wrap_socket(sock, server_hostname=domain) as ssock:\n",
        "                cert = ssock.getpeercert()\n",
        "                # parse dates (try multiple formats)\n",
        "                def _parse(x):\n",
        "                    if not x:\n",
        "                        return None\n",
        "                    for fmt in (\"%b %d %H:%M:%S %Y %Z\", \"%Y%m%d%H%M%SZ\"):\n",
        "                        try:\n",
        "                            return datetime.datetime.strptime(x, fmt)\n",
        "                        except Exception:\n",
        "                            continue\n",
        "                    return None\n",
        "                not_before = _parse(cert.get(\"notBefore\"))\n",
        "                not_after = _parse(cert.get(\"notAfter\"))\n",
        "                out[\"not_before\"] = not_before\n",
        "                out[\"not_after\"] = not_after\n",
        "                now = datetime.datetime.utcnow()\n",
        "                if (not_before and now < not_before) or (not_after and now > not_after):\n",
        "                    out[\"ssl_valid\"] = False\n",
        "                else:\n",
        "                    out[\"ssl_valid\"] = True\n",
        "                issuer = cert.get(\"issuer\")\n",
        "                subject = cert.get(\"subject\")\n",
        "                out[\"self_signed\"] = (issuer == subject)\n",
        "    except Exception:\n",
        "        pass\n",
        "    SSL_CACHE[domain] = out\n",
        "    return out\n",
        "\n",
        "\n",
        "def host_features(domain):\n",
        "    hf = {}\n",
        "    hf[\"whois\"] = get_whois(domain)\n",
        "    hf[\"hosting_country\"] = geolocate_domain(domain)\n",
        "    hf[\"blacklisted\"] = domain.lower() in BLACKLIST\n",
        "    hf[\"ssl\"] = check_ssl(domain)\n",
        "    return hf\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Content-based features module\n",
        "# ---------------------------\n",
        "def content_features_from_response(url, resp, soup=None):\n",
        "    \"\"\"\n",
        "    Accepts response (requests.Response) and parsed soup (optional).\n",
        "    Returns content-based feature dict.\n",
        "    \"\"\"\n",
        "    cf = {}\n",
        "    if resp is None:\n",
        "        # default values if page couldn't be fetched\n",
        "        cf.update({\n",
        "            \"num_redirects\": 0,\n",
        "            \"hidden_iframes\": 0,\n",
        "            \"obfuscated_scripts\": 0,\n",
        "            \"login_form_insecure\": False,\n",
        "            \"domain_brand_mismatch\": False,\n",
        "        })\n",
        "        return cf\n",
        "\n",
        "    # num redirects\n",
        "    cf[\"num_redirects\"] = len(resp.history) if hasattr(resp, \"history\") else 0\n",
        "\n",
        "    # parse if not provided\n",
        "    if not soup:\n",
        "        try:\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        except Exception:\n",
        "            soup = None\n",
        "\n",
        "    # hidden iframes\n",
        "    hidden_iframes = 0\n",
        "    if soup:\n",
        "        for iframe in soup.find_all(\"iframe\"):\n",
        "            style = iframe.get(\"style\", \"\") or \"\"\n",
        "            width = iframe.get(\"width\")\n",
        "            height = iframe.get(\"height\")\n",
        "            # heuristics for hidden iframe\n",
        "            if \"display:none\" in style.lower() or \"visibility:hidden\" in style.lower() or (width == \"0\" or height == \"0\"):\n",
        "                hidden_iframes += 1\n",
        "    cf[\"hidden_iframes\"] = hidden_iframes\n",
        "\n",
        "    # obfuscated scripts (naive: long script tags or presence of eval/obfuscation patterns)\n",
        "    obf = 0\n",
        "    if soup:\n",
        "        for s in soup.find_all(\"script\"):\n",
        "            txt = (s.string or \"\") or \"\"\n",
        "            if len(txt) > 2000:  # very long inline script\n",
        "                obf += 1\n",
        "            if \"eval(\" in txt or \"document.write(unescape\" in txt or (\"_0x\" in txt and \"push(\" in txt):\n",
        "                obf += 1\n",
        "    cf[\"obfuscated_scripts\"] = obf\n",
        "\n",
        "    # login form on insecure site — caller must check scheme (we return whether forms with password exist)\n",
        "    login_forms = 0\n",
        "    if soup:\n",
        "        for form in soup.find_all(\"form\"):\n",
        "            if form.find(\"input\", {\"type\": \"password\"}):\n",
        "                login_forms += 1\n",
        "    cf[\"login_forms_count\"] = login_forms\n",
        "\n",
        "    # domain-brand mismatch: search for brand names in text/title but not in domain\n",
        "    domain = urlparse(resp.url).netloc.lower() if resp else urlparse(url).netloc.lower()\n",
        "    text = \"\"\n",
        "    if soup:\n",
        "        title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
        "        body = soup.get_text(\" \", strip=True)[:20000]  # limit size\n",
        "        text = (title + \" \" + body).lower()\n",
        "    mismatch = False\n",
        "    for brand in BRAND_LIST:\n",
        "        if brand in text and brand not in domain:\n",
        "            mismatch = True\n",
        "            break\n",
        "    cf[\"domain_brand_mismatch\"] = mismatch\n",
        "\n",
        "    return cf\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Behavioral features module (approximate w/o full JS runtime)\n",
        "# ---------------------------\n",
        "def behavioral_features_from_soup(soup):\n",
        "    bf = {}\n",
        "    if not soup:\n",
        "        bf.update({\n",
        "            \"suspicious_download_links\": 0,\n",
        "            \"mouseover_mismatch_count\": 0,\n",
        "            \"popup_like_counts\": 0,\n",
        "        })\n",
        "        return bf\n",
        "\n",
        "    # suspicious downloads\n",
        "    suspicious_exts = (\".exe\", \".scr\", \".apk\", \".bat\", \".msi\")\n",
        "    download_count = 0\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].lower()\n",
        "        if any(href.endswith(ext) for ext in suspicious_exts):\n",
        "            download_count += 1\n",
        "    bf[\"suspicious_download_links\"] = download_count\n",
        "\n",
        "    # mouseover mismatch: anchor text looks like a URL but href different OR visible text different from href host\n",
        "    mm = 0\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        txt = (a.text or \"\").strip()\n",
        "        href = a[\"href\"].strip()\n",
        "        if txt and href and txt.count(\".\") >= 1:\n",
        "            # if displayed text contains domain-like string different from link host\n",
        "            try:\n",
        "                txt_host = urlparse(txt if txt.startswith(\"http\") else \"http://\" + txt).netloc.lower()\n",
        "                href_host = urlparse(urljoin(\"http://dummy\", href)).netloc.lower()\n",
        "                if txt_host and href_host and txt_host != href_host:\n",
        "                    mm += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "    bf[\"mouseover_mismatch_count\"] = mm\n",
        "\n",
        "    # popups via script detection (alert, confirm, window.open)\n",
        "    pop_count = 0\n",
        "    for s in soup.find_all(\"script\"):\n",
        "        txt = (s.string or \"\") or \"\"\n",
        "        pop_count += txt.lower().count(\"alert(\")\n",
        "        pop_count += txt.lower().count(\"confirm(\")\n",
        "        pop_count += txt.lower().count(\"prompt(\")\n",
        "        pop_count += txt.lower().count(\"window.open\")\n",
        "    bf[\"popup_like_counts\"] = pop_count\n",
        "\n",
        "    return bf\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Classification (collects ALL reasons)\n",
        "# ---------------------------\n",
        "def collect_reasons_and_classify(features):\n",
        "    \"\"\"\n",
        "    Input: features dict containing lexical, host, content, behavioral\n",
        "    Return: (classification, reasons_list)\n",
        "    \"\"\"\n",
        "    reasons = []\n",
        "\n",
        "    # Scheme / SSL reasons\n",
        "    scheme = features.get(\"lexical\", {}).get(\"scheme\", \"\")\n",
        "    ssl_info = features.get(\"host\", {}).get(\"ssl\", {}) or {}\n",
        "    if scheme != \"https\":\n",
        "        reasons.append(\"No HTTPS\")\n",
        "    else:\n",
        "        if not ssl_info.get(\"ssl_valid\", True):\n",
        "            # identify specific SSL reasons\n",
        "            nb = ssl_info.get(\"not_before\")\n",
        "            na = ssl_info.get(\"not_after\")\n",
        "            if nb and isinstance(nb, datetime.datetime) and datetime.datetime.utcnow() < nb:\n",
        "                reasons.append(\"SSL Not Yet Valid\")\n",
        "            if na and isinstance(na, datetime.datetime) and datetime.datetime.utcnow() > na:\n",
        "                reasons.append(\"SSL Expired\")\n",
        "            if ssl_info.get(\"self_signed\"):\n",
        "                reasons.append(\"SSL Self-Signed\")\n",
        "            if \"SSL Not Yet Valid\" not in reasons and \"SSL Expired\" not in reasons and \"SSL Self-Signed\" not in reasons:\n",
        "                reasons.append(\"Invalid SSL\")\n",
        "\n",
        "    # Host-based reasons\n",
        "    if features.get(\"host\", {}).get(\"blacklisted\"):\n",
        "        reasons.append(\"Blacklisted\")\n",
        "    domain_age = features.get(\"host\", {}).get(\"whois\", {}).get(\"age_days\")\n",
        "    if domain_age is not None and domain_age < 30:\n",
        "        reasons.append(\"New Domain (<30 days)\")\n",
        "    if features.get(\"host\", {}).get(\"whois\", {}).get(\"whois_privacy\"):\n",
        "        reasons.append(\"WHOIS Privacy / Redacted\")\n",
        "\n",
        "    # Lexical reasons\n",
        "    lex = features.get(\"lexical\", {})\n",
        "    if lex.get(\"suspicious_tld\"):\n",
        "        reasons.append(\"Suspicious TLD\")\n",
        "    if lex.get(\"typosquat\"):\n",
        "        reasons.append(\"Typosquat-like Domain\")\n",
        "    if lex.get(\"homograph\"):\n",
        "        reasons.append(\"Possible Homograph (Unicode)\")\n",
        "    if lex.get(\"randomized_label\"):\n",
        "        reasons.append(\"Randomized Domain Label\")\n",
        "    if lex.get(\"contains_keyword\"):\n",
        "        reasons.append(\"Suspicious Keyword in URL\")\n",
        "    if lex.get(\"incorrect_casing\"):\n",
        "        reasons.append(\"Incorrect / Mixed Casing in Domain\")\n",
        "\n",
        "    # Content-based reasons\n",
        "    cf = features.get(\"content\", {})\n",
        "    if cf.get(\"num_redirects\", 0) > 3:\n",
        "        reasons.append(\"Excessive Redirects\")\n",
        "    if cf.get(\"hidden_iframes\", 0) > 0:\n",
        "        reasons.append(\"Hidden Iframes\")\n",
        "    if cf.get(\"obfuscated_scripts\", 0) > 0:\n",
        "        reasons.append(\"Obfuscated Scripts\")\n",
        "    if cf.get(\"login_forms_count\", 0) > 0 and lex.get(\"scheme\") != \"https\":\n",
        "        reasons.append(\"Login Form on Insecure Page\")\n",
        "    if cf.get(\"domain_brand_mismatch\"):\n",
        "        reasons.append(\"Domain/Brand Mismatch in Content\")\n",
        "\n",
        "    # Behavioral reasons\n",
        "    bf = features.get(\"behavioral\", {})\n",
        "    if bf.get(\"suspicious_download_links\", 0) > 0:\n",
        "        reasons.append(\"Suspicious Download Links\")\n",
        "    if bf.get(\"mouseover_mismatch_count\", 0) > 0:\n",
        "        reasons.append(\"Mouseover URL Mismatch\")\n",
        "    if bf.get(\"popup_like_counts\", 0) > 2:\n",
        "        reasons.append(\"Excessive Popup-like Scripts\")\n",
        "\n",
        "    classification = \"Healthy\" if not reasons else \"Unhealthy\"\n",
        "    return classification, reasons\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Main analyzer: analyze_url (1-hop)\n",
        "# ---------------------------\n",
        "def analyze_url(start_url, depth=1, max_pages=1000):\n",
        "    \"\"\"\n",
        "    Analyze start_url and crawl at most 'depth' hops (recommended depth=1).\n",
        "    Returns report dict: {url: features}\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    report = {}\n",
        "\n",
        "    def _analyze(url, d, parent=None):\n",
        "        if url in seen or len(seen) >= max_pages:\n",
        "            return\n",
        "        seen.add(url)\n",
        "        print(f\"[INFO] Analyzing: {url} (depth={d})\")\n",
        "\n",
        "        # Lexical\n",
        "        lex = lexical_features(url)\n",
        "\n",
        "        # Fetch the URL allowing redirects (we need history for num_redirects)\n",
        "        resp = fetch_url(url, allow_redirects=True)\n",
        "        final_url = resp.url if resp else url\n",
        "\n",
        "        # Host features (use domain of final_url)\n",
        "        domain = urlparse(final_url).netloc.lower()\n",
        "        host = host_features(domain)\n",
        "\n",
        "        # Content features (use response and parsed soup)\n",
        "        links, soup = ([], None)\n",
        "        if resp:\n",
        "            links, soup = extract_links(final_url, resp)\n",
        "        cf = content_features_from_response(final_url, resp, soup)\n",
        "\n",
        "        # Behavioral features (from parsed soup)\n",
        "        bf = behavioral_features_from_soup(soup)\n",
        "\n",
        "        # External ratio and sample\n",
        "        num_links = len(links)\n",
        "        external_links = [l for l in links if urlparse(l).netloc.lower() != domain]\n",
        "        external_ratio = (len(external_links) / num_links) if num_links > 0 else 0.0\n",
        "\n",
        "        # Compose features\n",
        "        features = {\n",
        "            \"lexical\": lex,\n",
        "            \"host\": host,\n",
        "            \"content\": cf,\n",
        "            \"behavioral\": bf,\n",
        "            \"num_links\": num_links,\n",
        "            \"external_ratio\": round(external_ratio, 3),\n",
        "            \"external_links_sample\": external_links[:MAX_EXTERNAL_SAMPLE],\n",
        "            \"parent\": parent,\n",
        "            \"final_url\": final_url,\n",
        "        }\n",
        "\n",
        "        # Classification with all reasons\n",
        "        classification, reasons = collect_reasons_and_classify(features)\n",
        "        features[\"classification\"] = classification\n",
        "        features[\"reasons\"] = reasons\n",
        "\n",
        "        report[url] = features\n",
        "\n",
        "        # Recurse 1-hop (internal + external links) if allowed by depth\n",
        "        if d < depth:\n",
        "            for link in links:\n",
        "                # small guard: skip too many recursions on analytics ads etc.\n",
        "                _analyze(link, d + 1, parent=url)\n",
        "\n",
        "    _analyze(start_url, 0, parent=None)\n",
        "    return report\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Export helpers\n",
        "# ---------------------------\n",
        "def export_report_csv(report, filename=\"phish_report.csv\"):\n",
        "    fieldnames = [\n",
        "        \"url\", \"final_url\", \"domain\", \"classification\", \"reasons\",\n",
        "        \"scheme\", \"tld\", \"url_length\", \"subdomain_count\", \"special_char_count\",\n",
        "        \"has_ip\", \"typosquat\", \"homograph\", \"label_entropy\", \"randomized_label\",\n",
        "        \"contains_keyword\", \"incorrect_casing\", \"num_links\", \"external_ratio\",\n",
        "        \"num_redirects\", \"hidden_iframes\", \"obfuscated_scripts\", \"login_forms_count\",\n",
        "        \"domain_brand_mismatch\", \"suspicious_download_links\", \"mouseover_mismatch_count\",\n",
        "        \"popup_like_counts\", \"domain_age_days\", \"whois_privacy\", \"registrant\",\n",
        "        \"hosting_country\", \"blacklisted\", \"ssl_valid\", \"ssl_not_before\", \"ssl_not_after\", \"ssl_self_signed\",\n",
        "        \"parent\"\n",
        "    ]\n",
        "    rows = []\n",
        "    for url, f in report.items():\n",
        "        parsed_final = urlparse(f.get(\"final_url\") or url)\n",
        "        domain = parsed_final.netloc\n",
        "        who = f.get(\"host\", {}).get(\"whois\", {}) or {}\n",
        "        ssl = f.get(\"host\", {}).get(\"ssl\", {}) or {}\n",
        "        row = {\n",
        "            \"url\": url,\n",
        "            \"final_url\": f.get(\"final_url\"),\n",
        "            \"domain\": domain,\n",
        "            \"classification\": f.get(\"classification\"),\n",
        "            \"reasons\": \";\".join(f.get(\"reasons\", [])),\n",
        "            \"scheme\": f.get(\"lexical\", {}).get(\"scheme\"),\n",
        "            \"tld\": f.get(\"lexical\", {}).get(\"tld\"),\n",
        "            \"url_length\": f.get(\"lexical\", {}).get(\"url_length\"),\n",
        "            \"subdomain_count\": f.get(\"lexical\", {}).get(\"subdomain_count\"),\n",
        "            \"special_char_count\": f.get(\"lexical\", {}).get(\"special_char_count\"),\n",
        "            \"has_ip\": f.get(\"lexical\", {}).get(\"has_ip\"),\n",
        "            \"typosquat\": f.get(\"lexical\", {}).get(\"typosquat\"),\n",
        "            \"homograph\": f.get(\"lexical\", {}).get(\"homograph\"),\n",
        "            \"label_entropy\": f.get(\"lexical\", {}).get(\"label_entropy\"),\n",
        "            \"randomized_label\": f.get(\"lexical\", {}).get(\"randomized_label\"),\n",
        "            \"contains_keyword\": f.get(\"lexical\", {}).get(\"contains_keyword\"),\n",
        "            \"incorrect_casing\": f.get(\"lexical\", {}).get(\"incorrect_casing\"),\n",
        "            \"num_links\": f.get(\"num_links\"),\n",
        "            \"external_ratio\": f.get(\"external_ratio\"),\n",
        "            \"num_redirects\": f.get(\"content\", {}).get(\"num_redirects\"),\n",
        "            \"hidden_iframes\": f.get(\"content\", {}).get(\"hidden_iframes\"),\n",
        "            \"obfuscated_scripts\": f.get(\"content\", {}).get(\"obfuscated_scripts\"),\n",
        "            \"login_forms_count\": f.get(\"content\", {}).get(\"login_forms_count\"),\n",
        "            \"domain_brand_mismatch\": f.get(\"content\", {}).get(\"domain_brand_mismatch\"),\n",
        "            \"suspicious_download_links\": f.get(\"behavioral\", {}).get(\"suspicious_download_links\"),\n",
        "            \"mouseover_mismatch_count\": f.get(\"behavioral\", {}).get(\"mouseover_mismatch_count\"),\n",
        "            \"popup_like_counts\": f.get(\"behavioral\", {}).get(\"popup_like_counts\"),\n",
        "            \"domain_age_days\": who.get(\"age_days\"),\n",
        "            \"whois_privacy\": who.get(\"whois_privacy\"),\n",
        "            \"registrant\": who.get(\"registrant\"),\n",
        "            \"hosting_country\": f.get(\"host\", {}).get(\"hosting_country\"),\n",
        "            \"blacklisted\": f.get(\"host\", {}).get(\"blacklisted\"),\n",
        "            \"ssl_valid\": ssl.get(\"ssl_valid\"),\n",
        "            \"ssl_not_before\": ssl.get(\"not_before\"),\n",
        "            \"ssl_not_after\": ssl.get(\"not_after\"),\n",
        "            \"ssl_self_signed\": ssl.get(\"self_signed\"),\n",
        "            \"parent\": f.get(\"parent\"),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
        "        writer = csv.DictWriter(fh, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            # Convert datetimes to ISO strings\n",
        "            if r[\"ssl_not_before\"] and isinstance(r[\"ssl_not_before\"], datetime.datetime):\n",
        "                r[\"ssl_not_before\"] = r[\"ssl_not_before\"].isoformat()\n",
        "            if r[\"ssl_not_after\"] and isinstance(r[\"ssl_not_after\"], datetime.datetime):\n",
        "                r[\"ssl_not_after\"] = r[\"ssl_not_after\"].isoformat()\n",
        "            writer.writerow(r)\n",
        "    print(f\"[INFO] CSV exported to {filename}\")\n",
        "\n",
        "\n",
        "def export_report_json(report, filename=\"phish_report.json\"):\n",
        "    # convert non-serializable types minimally\n",
        "    serializable = {}\n",
        "    for url, f in report.items():\n",
        "        ff = json.loads(json.dumps(f, default=str))\n",
        "        serializable[url] = ff\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as fh:\n",
        "        json.dump(serializable, fh, indent=2)\n",
        "    print(f\"[INFO] JSON exported to {filename}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Example main — edit TEST_URLS to run scans\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    TEST_URLS = [\n",
        "        \"https://pub-60733cb9a98d4473bc08598e30d7edda.r2.dev/newlinkedin.html\",\n",
        "        #\"https://ltouurigo.com/cj1i\",\n",
        "        #\"https://rebrand.ly/3b33f0\"\n",
        "        #\"https://www.google.com/search?q=data+frame+see+the+full+content+of+the+cell&sxsrf=AE3TifNzPa2Ab-WiFqjPxAgA3QVCxI5vEQ%3A1756538031692\",\n",
        "        #\"https://example.com\",\n",
        "        #\"http://192.168.1.1/login\",\n",
        "        #\"http://paypa1.com\",\n",
        "    ]\n",
        "\n",
        "    all_report = {}\n",
        "    for url in TEST_URLS:\n",
        "        try:\n",
        "            r = analyze_url(url, depth=1)\n",
        "            all_report.update(r)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Interrupted.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Unexpected error scanning {url}: {e}\")\n",
        "\n",
        "    # Print a concise human-readable summary\n",
        "    for u, f in all_report.items():\n",
        "        print(\"\\n---------------------------------\")\n",
        "        print(f\"URL: {u}\")\n",
        "        print(f\"Final URL: {f.get('final_url')}\")\n",
        "        print(f\"Classification: {f.get('classification')}\")\n",
        "        print(f\"Reasons: {f.get('reasons')}\")\n",
        "        print(f\"Domain age (days): {f.get('host', {}).get('whois', {}).get('age_days')}\")\n",
        "        print(f\"SSL valid: {f.get('host', {}).get('ssl', {}).get('ssl_valid')}\")\n",
        "        print(f\"Suspicious TLD: {f.get('lexical', {}).get('suspicious_tld')}\")\n",
        "        print(f\"Typosquat: {f.get('lexical', {}).get('typosquat')}\")\n",
        "        print(f\"Homograph: {f.get('lexical', {}).get('homograph')}\")\n",
        "        print(f\"Randomized label: {f.get('lexical', {}).get('randomized_label')}\")\n",
        "        print(f\"Num links: {f.get('num_links')}, External ratio: {f.get('external_ratio')}\")\n",
        "        print(f\"External sample: {f.get('external_links_sample')}\")\n",
        "\n",
        "    # Export\n",
        "    export_report_csv(all_report, filename=\"phish_report.csv\")\n",
        "    export_report_json(all_report, filename=\"phish_report.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "aRNjOfg6GTJ_",
        "outputId": "842c26d9-3a5e-4635-95ac-050e61c20632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting whois\n",
            "  Downloading whois-1.20240129.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading whois-1.20240129.2-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: whois\n",
            "Successfully installed whois-1.20240129.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "54f05a3a8e2149dab86673533d7a1e17",
              "pip_warning": {
                "packages": [
                  "whois"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install whois"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWkNhDD7GTjB",
        "outputId": "4fe28f3f-5909-44b0-a481-7be56747d7c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Analyzing: https://telstra-109995.weeblysite.com/ (depth=0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3745264046.py:134: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
            "  features[\"has_ip\"] = bool(IP_RE.match(ext.registered_domain or domain))\n",
            "/tmp/ipython-input-3745264046.py:136: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
            "  domain_norm = (ext.registered_domain or domain).lower()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] CSV exported to phish_report.csv\n",
            "[INFO] JSON exported to phish_report.json\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "phish_detector.py\n",
        "\n",
        "Real-time Phishing Detector (modular)\n",
        "- Lexical features\n",
        "- Host-based features (WHOIS, SSL, GeoIP, blacklist)\n",
        "- Content-based features (redirects, iframes, login forms, brand mismatch)\n",
        "- Behavioral features (file downloads, mouseover mismatch, popups)\n",
        "- 1-hop crawling and external link validation\n",
        "- Exports CSV and JSON report\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "import socket\n",
        "import ssl\n",
        "import datetime\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tldextract\n",
        "import whois\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:119.0) Gecko/20100101 Firefox/119.0\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\",\n",
        "]\n",
        "\n",
        "SUSPICIOUS_TLDS = {\"xyz\", \"tk\", \"top\", \"click\", \"link\", \"info\", \"pw\", \"cn\"}\n",
        "KEYWORD_LIST = {\"login\", \"secure\", \"verify\", \"update\", \"bank\", \"free\", \"account\", \"signin\"}\n",
        "BLACKLIST = set()\n",
        "MAX_RETRIES = 3\n",
        "RETRY_BACKOFF = [1, 3, 6]\n",
        "REQUEST_TIMEOUT = 10\n",
        "SLEEP_JITTER = (0.8, 2.0)\n",
        "MAX_EXTERNAL_SAMPLE = 20\n",
        "SKIP_DOMAINS_CONTAINS = [\"googletagmanager\", \"googlesyndication\", \"doubleclick\", \"facebook\", \"instagram\", \"twitter\"]\n",
        "BRAND_LIST = [\"paypal\", \"google\", \"amazon\", \"facebook\", \"microsoft\", \"apple\", \"bank\"]\n",
        "\n",
        "# ---------------------------\n",
        "# Networking helpers\n",
        "# ---------------------------\n",
        "def get_headers():\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(USER_AGENTS),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "        \"DNT\": \"1\",\n",
        "        \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    }\n",
        "\n",
        "def fetch_url(url, allow_redirects=True):\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=get_headers(), timeout=REQUEST_TIMEOUT, allow_redirects=allow_redirects)\n",
        "            resp.raise_for_status()\n",
        "            time.sleep(random.uniform(*SLEEP_JITTER))\n",
        "            return resp\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            wait = RETRY_BACKOFF[min(attempt, len(RETRY_BACKOFF) - 1)]\n",
        "            print(f\"[WARN] Fetch failed ({e}) for {url} — retrying in {wait}s...\")\n",
        "            time.sleep(wait + random.random())\n",
        "    print(f\"[ERROR] Could not fetch {url} after {MAX_RETRIES} retries.\")\n",
        "    return None\n",
        "\n",
        "# ---------------------------\n",
        "# Link extraction\n",
        "# ---------------------------\n",
        "BAD_HREFS = {\"\", \"#\", \"/\", \"/undefined\"}\n",
        "\n",
        "def is_junk_href(href):\n",
        "    if not href:\n",
        "        return True\n",
        "    lower = href.strip().lower()\n",
        "    if lower in BAD_HREFS:\n",
        "        return True\n",
        "    if lower.startswith((\"javascript:\", \"mailto:\", \"tel:\", \"data:\")):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def extract_links(base_url, resp):\n",
        "    try:\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    except Exception:\n",
        "        soup = None\n",
        "    links = []\n",
        "    if not soup:\n",
        "        return links, None\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if is_junk_href(href):\n",
        "            continue\n",
        "        abs_url = urljoin(base_url, href)\n",
        "        parsed = urlparse(abs_url)\n",
        "        if parsed.scheme not in (\"http\", \"https\"):\n",
        "            continue\n",
        "        if any(skip in parsed.netloc for skip in SKIP_DOMAINS_CONTAINS):\n",
        "            continue\n",
        "        links.append(abs_url)\n",
        "    return links, soup\n",
        "\n",
        "# ---------------------------\n",
        "# Lexical features\n",
        "# ---------------------------\n",
        "IP_RE = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
        "\n",
        "def lexical_features(url):\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc\n",
        "    ext = tldextract.extract(domain)\n",
        "    features = {\n",
        "        \"url_length\": len(url),\n",
        "        \"scheme\": parsed.scheme.lower(),\n",
        "        \"domain\": domain.lower(),\n",
        "        \"tld\": ext.suffix.lower() if ext.suffix else \"\",\n",
        "        \"subdomain_count\": 0 if not ext.subdomain else len(ext.subdomain.split(\".\")),\n",
        "    }\n",
        "    path_netloc = parsed.netloc + parsed.path\n",
        "    features[\"special_char_count\"] = sum(path_netloc.count(ch) for ch in ['@', '-', '_', '%', '=', '&'])\n",
        "    features[\"has_ip\"] = bool(IP_RE.match(ext.registered_domain or domain))\n",
        "    whitelist = [\"google.com\", \"facebook.com\", \"amazon.com\", \"paypal.com\", \"microsoft.com\", \"apple.com\"]\n",
        "    domain_norm = (ext.registered_domain or domain).lower()\n",
        "    def typosquat_check(dnm):\n",
        "        for legit in whitelist:\n",
        "            ratio = SequenceMatcher(None, dnm, legit).ratio()\n",
        "            if 0.85 < ratio < 0.995 and dnm != legit:\n",
        "                return True\n",
        "        return False\n",
        "    features[\"typosquat\"] = typosquat_check(domain_norm)\n",
        "    try:\n",
        "        domain.encode(\"ascii\")\n",
        "        features[\"homograph\"] = False\n",
        "    except Exception:\n",
        "        features[\"homograph\"] = True\n",
        "    label = ext.domain or \"\"\n",
        "    if label:\n",
        "        freq = Counter(label)\n",
        "        L = len(label)\n",
        "        entropy = -sum((v / L) * math.log2(v / L) for v in freq.values())\n",
        "    else:\n",
        "        entropy = 0.0\n",
        "    features[\"label_entropy\"] = round(entropy, 3)\n",
        "    features[\"randomized_label\"] = (len(label) >= 8 and entropy > 3.8)\n",
        "    features[\"contains_keyword\"] = any(k in (parsed.path + \".\".join([ext.subdomain or \"\"])).lower() for k in KEYWORD_LIST)\n",
        "    features[\"incorrect_casing\"] = any(ch.isupper() for ch in domain) and not domain.isupper()\n",
        "    features[\"suspicious_tld\"] = features[\"tld\"] in SUSPICIOUS_TLDS\n",
        "    return features\n",
        "\n",
        "# ---------------------------\n",
        "# Content features\n",
        "# ---------------------------\n",
        "def content_features_from_response(url, resp, soup=None):\n",
        "    cf = {}\n",
        "    if resp is None:\n",
        "        cf.update({\n",
        "            \"num_redirects\": None,\n",
        "            \"hidden_iframes\": None,\n",
        "            \"obfuscated_scripts\": None,\n",
        "            \"login_forms_count\": None,\n",
        "            \"domain_brand_mismatch\": None,\n",
        "        })\n",
        "        return cf\n",
        "    cf[\"num_redirects\"] = len(resp.history) if hasattr(resp, \"history\") else 0\n",
        "    if not soup:\n",
        "        try:\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        except Exception:\n",
        "            soup = None\n",
        "    cf[\"hidden_iframes\"] = None if not soup else sum(\n",
        "        1 for iframe in soup.find_all(\"iframe\")\n",
        "        if \"display:none\" in (iframe.get(\"style\", \"\")).lower()\n",
        "        or \"visibility:hidden\" in (iframe.get(\"style\", \"\")).lower()\n",
        "        or iframe.get(\"width\") == \"0\" or iframe.get(\"height\") == \"0\"\n",
        "    )\n",
        "    obf = 0\n",
        "    if soup:\n",
        "        for s in soup.find_all(\"script\"):\n",
        "            txt = (s.string or \"\") or \"\"\n",
        "            if len(txt) > 2000 or \"eval(\" in txt or \"document.write(unescape\" in txt or (\"_0x\" in txt and \"push(\" in txt):\n",
        "                obf += 1\n",
        "    cf[\"obfuscated_scripts\"] = None if not soup else obf\n",
        "    if soup:\n",
        "        login_forms = sum(1 for form in soup.find_all(\"form\") if form.find(\"input\", {\"type\": \"password\"}))\n",
        "        cf[\"login_forms_count\"] = login_forms\n",
        "    else:\n",
        "        cf[\"login_forms_count\"] = None\n",
        "    domain = urlparse(resp.url).netloc.lower() if resp else urlparse(url).netloc.lower()\n",
        "    if soup:\n",
        "        text = ((soup.title.string.strip() if soup.title and soup.title.string else \"\") + \" \" + soup.get_text(\" \", strip=True)[:20000]).lower()\n",
        "        cf[\"domain_brand_mismatch\"] = any(brand in text and brand not in domain for brand in BRAND_LIST)\n",
        "    else:\n",
        "        cf[\"domain_brand_mismatch\"] = None\n",
        "    return cf\n",
        "\n",
        "# ---------------------------\n",
        "# Behavioral features\n",
        "# ---------------------------\n",
        "def behavioral_features_from_soup(soup):\n",
        "    bf = {}\n",
        "    if not soup:\n",
        "        return {\"suspicious_download_links\": None, \"mouseover_mismatch_count\": None, \"popup_like_counts\": None}\n",
        "    suspicious_exts = (\".exe\", \".scr\", \".apk\", \".bat\", \".msi\")\n",
        "    bf[\"suspicious_download_links\"] = sum(1 for a in soup.find_all(\"a\", href=True) if any(a[\"href\"].lower().endswith(ext) for ext in suspicious_exts))\n",
        "    mm = 0\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        txt = (a.text or \"\").strip()\n",
        "        href = a[\"href\"].strip()\n",
        "        if txt and href and txt.count(\".\") >= 1:\n",
        "            try:\n",
        "                txt_host = urlparse(txt if txt.startswith(\"http\") else \"http://\" + txt).netloc.lower()\n",
        "                href_host = urlparse(urljoin(\"http://dummy\", href)).netloc.lower()\n",
        "                if txt_host and href_host and txt_host != href_host:\n",
        "                    mm += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "    bf[\"mouseover_mismatch_count\"] = mm\n",
        "    pop_count = 0\n",
        "    for s in soup.find_all(\"script\"):\n",
        "        txt = (s.string or \"\") or \"\"\n",
        "        low = txt.lower()\n",
        "        pop_count += low.count(\"alert(\") + low.count(\"confirm(\") + low.count(\"prompt(\") + low.count(\"window.open\")\n",
        "    bf[\"popup_like_counts\"] = pop_count\n",
        "    return bf\n",
        "\n",
        "# ---------------------------\n",
        "# Logo helper\n",
        "# ---------------------------\n",
        "def get_site_logo(base_url, soup):\n",
        "    def _abs(u): return urljoin(base_url, u) if u else None\n",
        "    candidates = []\n",
        "    if soup:\n",
        "        for link in soup.find_all(\"link\", href=True):\n",
        "            rel = \" \".join(link.get(\"rel\", [])).lower()\n",
        "            href = link[\"href\"]\n",
        "            if \"apple-touch-icon\" in rel or \"icon\" in rel or \"shortcut icon\" in rel:\n",
        "                candidates.append((link.get(\"sizes\", \"\"), _abs(href)))\n",
        "    if candidates:\n",
        "        return candidates[0][1]\n",
        "    parsed = urlparse(base_url)\n",
        "    return f\"{parsed.scheme}://{parsed.netloc}/favicon.ico\" if parsed.scheme and parsed.netloc else None\n",
        "\n",
        "# ---------------------------\n",
        "# Classification\n",
        "# ---------------------------\n",
        "def collect_reasons_and_classify(features):\n",
        "    if features.get(\"fetch_failed\"):\n",
        "        return \"Fetch_Failed\", [\"Fetch failed\"]\n",
        "\n",
        "    reasons = []\n",
        "    # ---------------------------\n",
        "# Classification (collects ALL reasons)\n",
        "# ---------------------------\n",
        "def collect_reasons_and_classify(features):\n",
        "    \"\"\"\n",
        "    Input: features dict containing lexical, host, content, behavioral\n",
        "    Return: (classification, reasons_list)\n",
        "    \"\"\"\n",
        "    reasons = []\n",
        "\n",
        "    # Scheme / SSL reasons\n",
        "    scheme = features.get(\"lexical\", {}).get(\"scheme\", \"\")\n",
        "    ssl_info = features.get(\"host\", {}).get(\"ssl\", {}) or {}\n",
        "    if scheme != \"https\":\n",
        "        reasons.append(\"No HTTPS\")\n",
        "    else:\n",
        "        if not ssl_info.get(\"ssl_valid\", True):\n",
        "            # identify specific SSL reasons\n",
        "            nb = ssl_info.get(\"not_before\")\n",
        "            na = ssl_info.get(\"not_after\")\n",
        "            if nb and isinstance(nb, datetime.datetime) and datetime.datetime.utcnow() < nb:\n",
        "                reasons.append(\"SSL Not Yet Valid\")\n",
        "            if na and isinstance(na, datetime.datetime) and datetime.datetime.utcnow() > na:\n",
        "                reasons.append(\"SSL Expired\")\n",
        "            if ssl_info.get(\"self_signed\"):\n",
        "                reasons.append(\"SSL Self-Signed\")\n",
        "            if \"SSL Not Yet Valid\" not in reasons and \"SSL Expired\" not in reasons and \"SSL Self-Signed\" not in reasons:\n",
        "                reasons.append(\"Invalid SSL\")\n",
        "\n",
        "    # Host-based reasons\n",
        "    if features.get(\"host\", {}).get(\"blacklisted\"):\n",
        "        reasons.append(\"Blacklisted\")\n",
        "    domain_age = features.get(\"host\", {}).get(\"whois\", {}).get(\"age_days\")\n",
        "    if domain_age is not None and domain_age < 30:\n",
        "        reasons.append(\"New Domain (<30 days)\")\n",
        "    if features.get(\"host\", {}).get(\"whois\", {}).get(\"whois_privacy\"):\n",
        "        reasons.append(\"WHOIS Privacy / Redacted\")\n",
        "\n",
        "    # Lexical reasons\n",
        "    lex = features.get(\"lexical\", {})\n",
        "    if lex.get(\"suspicious_tld\"):\n",
        "        reasons.append(\"Suspicious TLD\")\n",
        "    if lex.get(\"typosquat\"):\n",
        "        reasons.append(\"Typosquat-like Domain\")\n",
        "    if lex.get(\"homograph\"):\n",
        "        reasons.append(\"Possible Homograph (Unicode)\")\n",
        "    if lex.get(\"randomized_label\"):\n",
        "        reasons.append(\"Randomized Domain Label\")\n",
        "    if lex.get(\"contains_keyword\"):\n",
        "        reasons.append(\"Suspicious Keyword in URL\")\n",
        "    if lex.get(\"incorrect_casing\"):\n",
        "        reasons.append(\"Incorrect / Mixed Casing in Domain\")\n",
        "\n",
        "    # Content-based reasons\n",
        "    cf = features.get(\"content\", {})\n",
        "    if cf.get(\"num_redirects\", 0) > 3:\n",
        "        reasons.append(\"Excessive Redirects\")\n",
        "    if cf.get(\"hidden_iframes\", 0) > 0:\n",
        "        reasons.append(\"Hidden Iframes\")\n",
        "    if cf.get(\"obfuscated_scripts\", 0) > 0:\n",
        "        reasons.append(\"Obfuscated Scripts\")\n",
        "    if cf.get(\"login_forms_count\", 0) > 0 and lex.get(\"scheme\") != \"https\":\n",
        "        reasons.append(\"Login Form on Insecure Page\")\n",
        "    if cf.get(\"domain_brand_mismatch\"):\n",
        "        reasons.append(\"Domain/Brand Mismatch in Content\")\n",
        "\n",
        "    # Behavioral reasons\n",
        "    bf = features.get(\"behavioral\", {})\n",
        "    if bf.get(\"suspicious_download_links\", 0) > 0:\n",
        "        reasons.append(\"Suspicious Download Links\")\n",
        "    if bf.get(\"mouseover_mismatch_count\", 0) > 0:\n",
        "        reasons.append(\"Mouseover URL Mismatch\")\n",
        "    if bf.get(\"popup_like_counts\", 0) > 2:\n",
        "        reasons.append(\"Excessive Popup-like Scripts\")\n",
        "\n",
        "    classification = \"Healthy\" if not reasons else \"Unhealthy\"\n",
        "    return classification, reasons\n",
        "\n",
        "    # trimmed here for brevity, keep original classification logic\n",
        "    classification = \"Healthy\" if not reasons else \"Unhealthy\"\n",
        "\n",
        "    return classification, reasons\n",
        "\n",
        "# ---------------------------\n",
        "# Export helpers\n",
        "# ---------------------------\n",
        "def export_report_csv(report, filename=\"phish_report.csv\"):\n",
        "    fieldnames = [\n",
        "        \"url\", \"final_url\", \"domain\", \"classification\", \"reasons\",\n",
        "        \"scheme\", \"tld\", \"url_length\", \"subdomain_count\", \"special_char_count\",\n",
        "        \"has_ip\", \"typosquat\", \"homograph\", \"label_entropy\", \"randomized_label\",\n",
        "        \"contains_keyword\", \"incorrect_casing\", \"num_links\", \"external_ratio\",\n",
        "        \"num_redirects\", \"hidden_iframes\", \"obfuscated_scripts\", \"login_forms_count\",\n",
        "        \"domain_brand_mismatch\", \"suspicious_download_links\", \"mouseover_mismatch_count\",\n",
        "        \"popup_like_counts\", \"domain_age_days\", \"whois_privacy\", \"registrant\",\n",
        "        \"hosting_country\", \"blacklisted\", \"ssl_valid\", \"ssl_not_before\", \"ssl_not_after\", \"ssl_self_signed\",\n",
        "        \"parent\"\n",
        "    ]\n",
        "    rows = []\n",
        "    for url, f in report.items():\n",
        "        parsed_final = urlparse(f.get(\"final_url\") or url)\n",
        "        domain = parsed_final.netloc\n",
        "        who = f.get(\"host\", {}).get(\"whois\", {}) or {}\n",
        "        ssl = f.get(\"host\", {}).get(\"ssl\", {}) or {}\n",
        "        row = {\n",
        "            \"url\": url,\n",
        "            \"final_url\": f.get(\"final_url\"),\n",
        "            \"domain\": domain,\n",
        "            \"classification\": f.get(\"classification\"),\n",
        "            \"reasons\": \";\".join(f.get(\"reasons\", [])),\n",
        "            \"scheme\": f.get(\"lexical\", {}).get(\"scheme\"),\n",
        "            \"tld\": f.get(\"lexical\", {}).get(\"tld\"),\n",
        "            \"url_length\": f.get(\"lexical\", {}).get(\"url_length\"),\n",
        "            \"subdomain_count\": f.get(\"lexical\", {}).get(\"subdomain_count\"),\n",
        "            \"special_char_count\": f.get(\"lexical\", {}).get(\"special_char_count\"),\n",
        "            \"has_ip\": f.get(\"lexical\", {}).get(\"has_ip\"),\n",
        "            \"typosquat\": f.get(\"lexical\", {}).get(\"typosquat\"),\n",
        "            \"homograph\": f.get(\"lexical\", {}).get(\"homograph\"),\n",
        "            \"label_entropy\": f.get(\"lexical\", {}).get(\"label_entropy\"),\n",
        "            \"randomized_label\": f.get(\"lexical\", {}).get(\"randomized_label\"),\n",
        "            \"contains_keyword\": f.get(\"lexical\", {}).get(\"contains_keyword\"),\n",
        "            \"incorrect_casing\": f.get(\"lexical\", {}).get(\"incorrect_casing\"),\n",
        "            \"num_links\": f.get(\"num_links\"),\n",
        "            \"external_ratio\": f.get(\"external_ratio\"),\n",
        "            \"num_redirects\": f.get(\"content\", {}).get(\"num_redirects\"),\n",
        "            \"hidden_iframes\": f.get(\"content\", {}).get(\"hidden_iframes\"),\n",
        "            \"obfuscated_scripts\": f.get(\"content\", {}).get(\"obfuscated_scripts\"),\n",
        "            \"login_forms_count\": f.get(\"content\", {}).get(\"login_forms_count\"),\n",
        "            \"domain_brand_mismatch\": f.get(\"content\", {}).get(\"domain_brand_mismatch\"),\n",
        "            \"suspicious_download_links\": f.get(\"behavioral\", {}).get(\"suspicious_download_links\"),\n",
        "            \"mouseover_mismatch_count\": f.get(\"behavioral\", {}).get(\"mouseover_mismatch_count\"),\n",
        "            \"popup_like_counts\": f.get(\"behavioral\", {}).get(\"popup_like_counts\"),\n",
        "            \"domain_age_days\": who.get(\"age_days\"),\n",
        "            \"whois_privacy\": who.get(\"whois_privacy\"),\n",
        "            \"registrant\": who.get(\"registrant\"),\n",
        "            \"hosting_country\": f.get(\"host\", {}).get(\"hosting_country\"),\n",
        "            \"blacklisted\": f.get(\"host\", {}).get(\"blacklisted\"),\n",
        "            \"ssl_valid\": ssl.get(\"ssl_valid\"),\n",
        "            \"ssl_not_before\": ssl.get(\"not_before\"),\n",
        "            \"ssl_not_after\": ssl.get(\"not_after\"),\n",
        "            \"ssl_self_signed\": ssl.get(\"self_signed\"),\n",
        "            \"parent\": f.get(\"parent\"),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
        "        writer = csv.DictWriter(fh, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            # Convert datetimes to ISO strings\n",
        "            if r[\"ssl_not_before\"] and isinstance(r[\"ssl_not_before\"], datetime.datetime):\n",
        "                r[\"ssl_not_before\"] = r[\"ssl_not_before\"].isoformat()\n",
        "            if r[\"ssl_not_after\"] and isinstance(r[\"ssl_not_after\"], datetime.datetime):\n",
        "                r[\"ssl_not_after\"] = r[\"ssl_not_after\"].isoformat()\n",
        "            writer.writerow(r)\n",
        "    print(f\"[INFO] CSV exported to {filename}\")\n",
        "\n",
        "\n",
        "def export_report_json(report, filename=\"phish_report.json\"):\n",
        "    # convert non-serializable types minimally\n",
        "    serializable = {}\n",
        "    for url, f in report.items():\n",
        "        ff = json.loads(json.dumps(f, default=str))\n",
        "        serializable[url] = ff\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as fh:\n",
        "        json.dump(serializable, fh, indent=2)\n",
        "    print(f\"[INFO] JSON exported to {filename}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Analyzer\n",
        "# ---------------------------\n",
        "def analyze_url(start_url, depth=1, max_pages=1000):\n",
        "    seen, report = set(), {}\n",
        "    def _analyze(url, d, parent=None):\n",
        "        if url in seen or len(seen) >= max_pages: return\n",
        "        seen.add(url)\n",
        "        print(f\"[INFO] Analyzing: {url} (depth={d})\")\n",
        "        lex = lexical_features(url)\n",
        "        resp = fetch_url(url, allow_redirects=True)\n",
        "        final_url = resp.url if resp else url\n",
        "        domain = urlparse(final_url).netloc.lower()\n",
        "        host = {}  # omitted: same as your original host_features(domain)\n",
        "        links, soup = ([], None)\n",
        "        if resp: links, soup = extract_links(final_url, resp)\n",
        "        cf = content_features_from_response(final_url, resp, soup)\n",
        "        bf = behavioral_features_from_soup(soup)\n",
        "        logo_url = get_site_logo(final_url, soup)\n",
        "        features = {\"lexical\": lex, \"host\": host, \"content\": cf, \"behavioral\": bf,\n",
        "                    \"num_links\": len(links), \"external_ratio\": 0, \"external_links_sample\": [],\n",
        "                    \"parent\": parent, \"final_url\": final_url, \"logo_url\": logo_url,\n",
        "                    \"fetch_failed\": resp is None}\n",
        "        classification, reasons = collect_reasons_and_classify(features)\n",
        "        features[\"classification\"], features[\"reasons\"] = classification, reasons\n",
        "        report[url] = features\n",
        "    _analyze(start_url, 0, parent=None)\n",
        "    return report\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    TEST_URLS = [\"https://telstra-109995.weeblysite.com/\"]\n",
        "    all_report = {}\n",
        "    for url in TEST_URLS:\n",
        "        all_report.update(analyze_url(url))\n",
        "    export_report_csv(all_report, \"phish_report.csv\")\n",
        "    export_report_json(all_report, \"phish_report.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOkPmhB6Gqtz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR7zaf5xGqbG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5zajamXPQN9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCVC-YN2PQJM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCy4irhqPQD2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMM0lZUJPP-r"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0V7GZ8-PaXE",
        "outputId": "cc672f66-9711-497a-85b6-bc320e511dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.12/dist-packages (5.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing-extensions, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 typing-extensions-4.14.1 webdriver-manager-4.0.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 tldextract pandas pyarrow selenium webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIcPodA_PUgU",
        "outputId": "12624577-43d3-4df5-ffe2-43fd9ededed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "[RUN] Analyzing https://telstra-109995.weeblysite.com/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1188906053.py:344: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"scraped_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Appended record for https://telstra-109995.weeblysite.com/ to site_intel.parquet\n",
            "[INFO] Wrote JSON snapshot to site_reports_json/site_report_1756561929.json\n",
            "Final URL: https://telstra-109995.weeblysite.com/\n",
            "Category: other\n",
            "Logo: https://www.weebly.com/favicon.ico\n",
            "Facilities: {\n",
            "  \"has_login_form\": false,\n",
            "  \"has_password_field\": false,\n",
            "  \"has_payment_checkout\": false,\n",
            "  \"has_cart_keywords\": false,\n",
            "  \"has_file_upload\": false,\n",
            "  \"has_contact_form\": false,\n",
            "  \"has_otp_field\": false,\n",
            "  \"has_social_login\": false,\n",
            "  \"has_download_links\": false,\n",
            "  \"has_admin_panel_links\": false\n",
            "}\n",
            "\n",
            "Done. Parquet file: site_intel.parquet\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "site_intel_parquet_llm.py\n",
        "\n",
        "What it does:\n",
        " - Scrape pages via requests + BeautifulSoup; fallback to Selenium when needed\n",
        " - Extract DOM fields (title/meta/text/links/forms/images/logo/scripts/inline handlers)\n",
        " - Classify site category (banking, ecommerce, education, ...)\n",
        " - Detect facilities (login, password field, payment, OTP, uploads, downloads, admin links)\n",
        " - Build an LLM prompt and optionally call OpenAI Chat API (gpt-3.5-turbo)\n",
        " - Save an extracted record per URL to a Parquet file (and JSON per-run)\n",
        "\n",
        "Dependencies:\n",
        " pip install requests beautifulsoup4 tldextract pandas pyarrow selenium webdriver-manager\n",
        "\n",
        "Set OPENAI_API_KEY environment variable to enable LLM calls (optional).\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "from typing import Dict, Any, List, Optional, Tuple\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tldextract\n",
        "import pandas as pd\n",
        "\n",
        "# Selenium imports\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# -------------------------\n",
        "# CONFIG\n",
        "# -------------------------\n",
        "REQUEST_TIMEOUT = 12\n",
        "USER_AGENT = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "              \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\")\n",
        "HEADERS = {\"User-Agent\": USER_AGENT}\n",
        "\n",
        "PARQUET_OUTPUT = \"site_intel.parquet\"\n",
        "JSON_DIR = \"site_reports_json\"\n",
        "os.makedirs(JSON_DIR, exist_ok=True)\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")  # set this env var to call OpenAI\n",
        "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
        "CALL_OPENAI = bool(OPENAI_API_KEY)  # toggle based on presence of key\n",
        "\n",
        "MAX_TEXT_CHARS = 200000\n",
        "\n",
        "# Category keywords (rule-based)\n",
        "CATEGORY_KEYWORDS = {\n",
        "    \"banking\": [\"bank\", \"account\", \"transfer\", \"loan\", \"atm\", \"routing\", \"credit card\", \"debit\"],\n",
        "    \"ecommerce\": [\"add to cart\", \"shopping cart\", \"checkout\", \"price\", \"buy now\", \"order\", \"product\", \"cart\"],\n",
        "    \"education\": [\"course\", \"university\", \"college\", \"admissions\", \"syllabus\", \"lecture\"],\n",
        "    \"government\": [\"gov\", \"official\", \"public service\", \"tax\", \"ministry\"],\n",
        "    \"social\": [\"follow\", \"share\", \"profile\", \"timeline\", \"friends\"],\n",
        "    \"media\": [\"news\", \"article\", \"press\", \"blog\", \"journal\"],\n",
        "    \"cloud_storage\": [\"upload\", \"download\", \"file\", \"storage\", \"drive\", \"share file\"],\n",
        "    \"crypto\": [\"wallet\", \"private key\", \"crypto\", \"ethereum\", \"bitcoin\"],\n",
        "    \"healthcare\": [\"appointment\", \"clinic\", \"medical\", \"doctor\", \"patient\"],\n",
        "    \"travel\": [\"booking\", \"flight\", \"hotel\", \"reservation\"],\n",
        "}\n",
        "\n",
        "BRAND_KEYWORDS = [\"paypal\", \"google\", \"facebook\", \"bank\", \"amazon\", \"microsoft\", \"apple\"]\n",
        "\n",
        "# -------------------------\n",
        "# Fetch helpers\n",
        "# -------------------------\n",
        "def fetch_with_requests(url: str, timeout: int = REQUEST_TIMEOUT) -> Tuple[Optional[requests.Response], Optional[str]]:\n",
        "    \"\"\"Fetch via requests. Returns (response, None) or (None, error_string).\"\"\"\n",
        "    try:\n",
        "        resp = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)\n",
        "        resp.raise_for_status()\n",
        "        # consider small responses potentially needing fallback\n",
        "        if not resp.text or len(resp.text.strip()) < 120:\n",
        "            return resp, \"content_too_small\"\n",
        "        return resp, None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def fetch_with_selenium(url: str, timeout: int = 20) -> Tuple[Optional[Tuple[str, str]], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Use Selenium headless Chrome to load the page and return (final_url, html) or (None, error).\n",
        "    \"\"\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.headless = True\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(f\"user-agent={USER_AGENT}\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "    driver = None\n",
        "    try:\n",
        "        driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
        "        driver.set_page_load_timeout(timeout)\n",
        "        driver.get(url)\n",
        "        time.sleep(1.0)  # let some JS run\n",
        "        html = driver.page_source\n",
        "        final = driver.current_url\n",
        "        return (final, html), None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "    finally:\n",
        "        if driver:\n",
        "            try:\n",
        "                driver.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "# -------------------------\n",
        "# Extraction helpers\n",
        "# -------------------------\n",
        "def sanitize_text(s: Optional[str]) -> str:\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def extract_basic_dom_fields(base_url: str, html: str) -> Dict[str, Any]:\n",
        "    \"\"\"Parse HTML and extract canonical DOM fields.\"\"\"\n",
        "    soup = BeautifulSoup(html or \"\", \"html.parser\")\n",
        "    out: Dict[str, Any] = {}\n",
        "    out[\"title\"] = sanitize_text(soup.title.string) if soup.title and soup.title.string else \"\"\n",
        "    meta_tag = soup.find(\"meta\", attrs={\"name\": \"description\"}) or soup.find(\"meta\", attrs={\"property\": \"og:description\"})\n",
        "    out[\"meta_description\"] = sanitize_text(meta_tag.get(\"content\")) if meta_tag and meta_tag.get(\"content\") else \"\"\n",
        "    out[\"text\"] = sanitize_text(soup.get_text(separator=\" \", strip=True))[:MAX_TEXT_CHARS]\n",
        "\n",
        "    anchors = []\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a.get(\"href\").strip()\n",
        "        abs_href = urljoin(base_url, href)\n",
        "        text = (a.get_text(\" \", strip=True) or \"\").strip()\n",
        "        anchors.append({\"href\": abs_href, \"text\": text})\n",
        "    out[\"links\"] = anchors\n",
        "\n",
        "    forms = []\n",
        "    for f in soup.find_all(\"form\"):\n",
        "        action = f.get(\"action\") or \"\"\n",
        "        method = (f.get(\"method\") or \"GET\").upper()\n",
        "        inputs = []\n",
        "        for i in f.find_all([\"input\", \"select\", \"textarea\", \"button\"]):\n",
        "            itype = (i.get(\"type\") or i.name or \"\").lower()\n",
        "            name = (i.get(\"name\") or \"\").strip()\n",
        "            placeholder = (i.get(\"placeholder\") or \"\").strip()\n",
        "            inputs.append({\"type\": itype, \"name\": name, \"placeholder\": placeholder})\n",
        "        forms.append({\"action\": urljoin(base_url, action), \"method\": method, \"inputs\": inputs})\n",
        "    out[\"forms\"] = forms\n",
        "\n",
        "    images = []\n",
        "    for img in soup.find_all(\"img\", src=True):\n",
        "        try:\n",
        "            src = img.get(\"src\")\n",
        "            images.append({\"src\": urljoin(base_url, src), \"alt\": (img.get(\"alt\") or \"\").strip()})\n",
        "        except Exception:\n",
        "            continue\n",
        "    out[\"images\"] = images\n",
        "\n",
        "    # logo (favicon or og:image)\n",
        "    ico = None\n",
        "    link_icon = soup.find(\"link\", rel=lambda v: v and \"icon\" in v.lower())\n",
        "    og_image = soup.find(\"meta\", property=\"og:image\")\n",
        "    if link_icon and link_icon.get(\"href\"):\n",
        "        ico = urljoin(base_url, link_icon[\"href\"])\n",
        "    elif og_image and og_image.get(\"content\"):\n",
        "        ico = urljoin(base_url, og_image[\"content\"])\n",
        "    out[\"logo\"] = ico or \"\"\n",
        "\n",
        "    out[\"scripts_count\"] = len(soup.find_all(\"script\"))\n",
        "    inline_handlers = 0\n",
        "    for tag in soup.find_all(True):\n",
        "        for attr in tag.attrs.keys():\n",
        "            if isinstance(attr, str) and attr.lower().startswith(\"on\"):\n",
        "                inline_handlers += 1\n",
        "    out[\"inline_event_handlers\"] = inline_handlers\n",
        "\n",
        "    return out\n",
        "\n",
        "# -------------------------\n",
        "# Classification & facility detection\n",
        "# -------------------------\n",
        "def classify_site_from_text(title: str, meta: str, text: str) -> str:\n",
        "    score = {k: 0 for k in CATEGORY_KEYWORDS.keys()}\n",
        "    source = \" \".join([title or \"\", meta or \"\", text or \"\"]).lower()\n",
        "    for cat, kws in CATEGORY_KEYWORDS.items():\n",
        "        for kw in kws:\n",
        "            if title and kw in (title or \"\").lower():\n",
        "                score[cat] += 3\n",
        "            if meta and kw in (meta or \"\").lower():\n",
        "                score[cat] += 2\n",
        "            if kw in source:\n",
        "                score[cat] += 1\n",
        "    best = max(score.items(), key=lambda x: x[1])\n",
        "    if best[1] == 0:\n",
        "        return \"other\"\n",
        "    second = sorted(score.items(), key=lambda x: x[1], reverse=True)[1]\n",
        "    if best[1] - second[1] < 2:\n",
        "        return \"other\"\n",
        "    return best[0]\n",
        "\n",
        "def detect_facilities(dom: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Return dict of boolean indicators for facilities present on the page.\n",
        "    This is the same implementation you asked to preserve, with minimal addition to reliably detect login forms.\n",
        "    \"\"\"\n",
        "    facilities = {\n",
        "        \"has_login_form\": False,\n",
        "        \"has_password_field\": False,\n",
        "        \"has_payment_checkout\": False,\n",
        "        \"has_cart_keywords\": False,\n",
        "        \"has_file_upload\": False,\n",
        "        \"has_contact_form\": False,\n",
        "        \"has_otp_field\": False,\n",
        "        \"has_social_login\": False,\n",
        "        \"has_download_links\": False,\n",
        "        \"has_admin_panel_links\": False,\n",
        "    }\n",
        "\n",
        "    text = (dom.get(\"title\") or \"\") + \" \" + (dom.get(\"meta_description\") or \"\") + \" \" + (dom.get(\"text\") or \"\")\n",
        "    t = text.lower()\n",
        "\n",
        "    # login / password detection (minimal edits from previous function)\n",
        "    for f in dom.get(\"forms\", []):\n",
        "        inputs = f.get(\"inputs\", [])\n",
        "        # if there is any explicit password input\n",
        "        if any((i.get(\"type\") or \"\").lower() == \"password\" for i in inputs):\n",
        "            facilities[\"has_login_form\"] = True\n",
        "            facilities[\"has_password_field\"] = True\n",
        "            if any(k in f.get(\"action\", \"\").lower() for k in [\"/login\", \"/signin\", \"/auth\", \"account\", \"session\"]):\n",
        "                facilities[\"has_login_form\"] = True\n",
        "\n",
        "        # MINIMAL ADDITION:\n",
        "        # detect common login forms that don't have type=\"password\"\n",
        "        # e.g., email or username field plus a submit/button input\n",
        "        names = [ (i.get(\"name\") or \"\").lower() for i in inputs ]\n",
        "        placeholders = [ (i.get(\"placeholder\") or \"\").lower() for i in inputs ]\n",
        "        types = [ (i.get(\"type\") or \"\").lower() for i in inputs ]\n",
        "        # if there's an email/username field and a submit/button, consider login form present\n",
        "        if (any(\"user\" in n or \"email\" in n or \"login\" in n or \"username\" in n for n in names) or\n",
        "            any(\"email\" in p or \"user\" in p or \"login\" in p or \"username\" in p for p in placeholders)):\n",
        "            if any(t in (\"submit\", \"button\") or t == \"\" for t in types):\n",
        "                facilities[\"has_login_form\"] = True\n",
        "\n",
        "        # file upload\n",
        "        if any((i.get(\"type\") or \"\").lower() in (\"file\", \"fileupload\") for i in inputs):\n",
        "            facilities[\"has_file_upload\"] = True\n",
        "\n",
        "        # contact form (presence of name/email/message fields)\n",
        "        if any(\"email\" in n or \"message\" in n or \"contact\" in n for n in names):\n",
        "            facilities[\"has_contact_form\"] = True\n",
        "\n",
        "        # OTP field detection\n",
        "        if any(\"otp\" in (i.get(\"name\") or \"\").lower() or \"one-time\" in (i.get(\"placeholder\") or \"\").lower() for i in inputs):\n",
        "            facilities[\"has_otp_field\"] = True\n",
        "\n",
        "        # textual indicators for social login (within page text)\n",
        "        if any(kw in t for kw in [\"login with google\", \"login with facebook\", \"sign in with google\", \"oauth\"]):\n",
        "            facilities[\"has_social_login\"] = True\n",
        "\n",
        "    # payment / cart detection via keywords or links\n",
        "    if any(kw in t for kw in [\"checkout\", \"add to cart\", \"shopping cart\", \"place order\", \"card number\", \"cvv\", \"billing\"]):\n",
        "        facilities[\"has_payment_checkout\"] = True\n",
        "    if any(kw in t for kw in [\"cart\", \"basket\", \"sku\", \"price\"]):\n",
        "        facilities[\"has_cart_keywords\"] = True\n",
        "\n",
        "    # social login from outside forms\n",
        "    if any(kw in t for kw in [\"login with google\", \"login with facebook\", \"sign in with google\", \"oauth\"]):\n",
        "        facilities[\"has_social_login\"] = True\n",
        "\n",
        "    # download links & admin links from anchors\n",
        "    for a in dom.get(\"links\", []):\n",
        "        href = (a.get(\"href\") or \"\").lower()\n",
        "        if href.endswith((\".exe\", \".apk\", \".msi\", \".zip\", \".scr\")):\n",
        "            facilities[\"has_download_links\"] = True\n",
        "        if any(x in href for x in [\"/admin\", \"/wp-admin\", \"panel\", \"/manage\"]):\n",
        "            facilities[\"has_admin_panel_links\"] = True\n",
        "\n",
        "    # final sanity: prominent 'login' text\n",
        "    if \"log in\" in t or \"sign in\" in t or \"sign-in\" in t:\n",
        "        facilities[\"has_login_form\"] = True\n",
        "\n",
        "    return facilities\n",
        "\n",
        "# -------------------------\n",
        "# LLM prompt builder & call\n",
        "# -------------------------\n",
        "def build_llm_prompt(url: str, site_category: str, facilities: Dict[str, Any], dom: Dict[str, Any]) -> str:\n",
        "    lines = []\n",
        "    lines.append(\"You are a cybersecurity analyst. Analyze this website and list realistic phishing or malicious scenarios.\")\n",
        "    lines.append(f\"URL: {url}\")\n",
        "    lines.append(f\"Estimated category: {site_category}\")\n",
        "    lines.append(\"Detected facilities (True/False):\")\n",
        "    for k, v in facilities.items():\n",
        "        lines.append(f\"  - {k}: {v}\")\n",
        "    lines.append(\"\\nPage title: \" + (dom.get(\"title\") or \"\"))\n",
        "    lines.append(\"Meta description: \" + (dom.get(\"meta_description\") or \"\"))\n",
        "    sample_text = (dom.get(\"text\") or \"\")[:1000]\n",
        "    lines.append(\"Top text snippet: \" + sample_text)\n",
        "    sample_links = dom.get(\"links\", [])[:10]\n",
        "    if sample_links:\n",
        "        lines.append(\"Sample links (href -> text):\")\n",
        "        for a in sample_links:\n",
        "            lines.append(f\"  - {a.get('href')} -> '{a.get('text')}'\")\n",
        "    sample_forms = dom.get(\"forms\", [])[:5]\n",
        "    if sample_forms:\n",
        "        lines.append(\"Sample forms (action, method, inputs):\")\n",
        "        for f in sample_forms:\n",
        "            lines.append(f\"  - action: {f.get('action')}, method: {f.get('method')}, inputs: {f.get('inputs')}\")\n",
        "    lines.append(\"\\nInstructions:\")\n",
        "    lines.append(\"  1) Provide 3-6 plausible phishing or malicious scenarios that attackers might attempt on this site.\")\n",
        "    lines.append(\"  2) For each scenario, give: attack vector, preconditions, user-visible indicators (what users might see).\")\n",
        "    lines.append(\"  3) Provide 2 prioritized mitigations or detection steps for defenders.\")\n",
        "    lines.append(\"Return the answer as JSON with keys: scenarios (list of objects) and mitigations (list).\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def call_openai_chat(api_key: str, prompt: str, model=\"gpt-3.5-turbo\", max_tokens=700) -> Tuple[Optional[Dict], Optional[str]]:\n",
        "    if not api_key:\n",
        "        return None, \"no-api-key-provided\"\n",
        "    try:\n",
        "        url = \"https://api.openai.com/v1/chat/completions\"\n",
        "        headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
        "        body = {\n",
        "            \"model\": model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"temperature\": 0.2,\n",
        "            \"max_tokens\": max_tokens,\n",
        "        }\n",
        "        resp = requests.post(url, headers=headers, json=body, timeout=60)\n",
        "        resp.raise_for_status()\n",
        "        return resp.json(), None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "# -------------------------\n",
        "# Orchestrator: scrape -> classify -> facilities -> LLM -> persist\n",
        "# -------------------------\n",
        "def analyze_and_persist(url: str, parquet_path: str = PARQUET_OUTPUT, selenium_fallback: bool = True, call_llm: bool = CALL_OPENAI) -> Dict[str, Any]:\n",
        "    report: Dict[str, Any] = {\n",
        "        \"url\": url,\n",
        "        \"scraped_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"fetch_method\": None,\n",
        "        \"fetch_error\": None,\n",
        "        \"final_url\": None,\n",
        "        \"dom\": None,\n",
        "        \"category\": None,\n",
        "        \"facilities\": None,\n",
        "        \"llm_prompt\": None,\n",
        "        \"llm_response\": None,\n",
        "        \"notes\": [],\n",
        "    }\n",
        "\n",
        "    # 1) Try requests\n",
        "    resp, err = fetch_with_requests(url)\n",
        "    html = None\n",
        "    if resp and not err:\n",
        "        html = resp.text\n",
        "        report[\"final_url\"] = resp.url\n",
        "        report[\"fetch_method\"] = \"requests\"\n",
        "    else:\n",
        "        report[\"fetch_method\"] = \"requests_failed\"\n",
        "        report[\"fetch_error\"] = err\n",
        "        report[\"notes\"].append(f\"requests failed: {err}\")\n",
        "\n",
        "    # 2) If content too small or missing, optionally use selenium fallback\n",
        "    if (not html or len(html.strip()) < 120) and selenium_fallback:\n",
        "        report[\"notes\"].append(\"Attempting Selenium fallback (requests insufficient)\")\n",
        "        selenium_result, s_err = fetch_with_selenium(url)\n",
        "        if selenium_result:\n",
        "            final, html = selenium_result\n",
        "            report[\"final_url\"] = final or report[\"final_url\"] or url\n",
        "            report[\"fetch_method\"] = \"selenium\"\n",
        "            report[\"notes\"].append(\"Selenium fetch succeeded\")\n",
        "        else:\n",
        "            report[\"notes\"].append(f\"Selenium fetch failed: {s_err}\")\n",
        "            report[\"fetch_error\"] = report[\"fetch_error\"] or s_err\n",
        "\n",
        "    # 3) If no html available -> fill NA for content-based/behavioral and persist minimal info\n",
        "    if not html:\n",
        "        report[\"dom\"] = {\n",
        "            \"title\": \"\",\n",
        "            \"meta_description\": \"\",\n",
        "            \"text\": \"\",\n",
        "            \"links\": [],\n",
        "            \"forms\": [],\n",
        "            \"images\": [],\n",
        "            \"logo\": \"\",\n",
        "            \"scripts_count\": \"NA\",\n",
        "            \"inline_event_handlers\": \"NA\",\n",
        "        }\n",
        "        report[\"category\"] = \"unknown\"\n",
        "        report[\"facilities\"] = {k: \"NA\" for k in [\"has_login_form\",\"has_password_field\",\"has_payment_checkout\",\n",
        "                                                   \"has_cart_keywords\",\"has_file_upload\",\"has_contact_form\",\n",
        "                                                   \"has_otp_field\",\"has_social_login\",\"has_download_links\",\n",
        "                                                   \"has_admin_panel_links\"]}\n",
        "        # Save and return\n",
        "        _append_record_to_parquet_and_json(report, parquet_path)\n",
        "        return report\n",
        "\n",
        "    # 4) Parse DOM & extract fields\n",
        "    dom = extract_basic_dom_fields(report.get(\"final_url\") or url, html)\n",
        "    report[\"dom\"] = dom\n",
        "\n",
        "    # 5) classify & detect facilities\n",
        "    category = classify_site_from_text(dom.get(\"title\"), dom.get(\"meta_description\"), dom.get(\"text\"))\n",
        "    report[\"category\"] = category\n",
        "    facilities = detect_facilities(dom)\n",
        "    report[\"facilities\"] = facilities\n",
        "\n",
        "    # 6) Build LLM prompt\n",
        "    prompt = build_llm_prompt(report[\"final_url\"], category, facilities, dom)\n",
        "    report[\"llm_prompt\"] = prompt\n",
        "\n",
        "    # 7) Call LLM if requested\n",
        "    if call_llm:\n",
        "        rjson, err = call_openai_chat(OPENAI_API_KEY, prompt, model=OPENAI_MODEL)\n",
        "        if err:\n",
        "            report[\"llm_error\"] = err\n",
        "            report[\"notes\"].append(f\"OpenAI call failed: {err}\")\n",
        "        else:\n",
        "            report[\"llm_response\"] = rjson\n",
        "\n",
        "    # 8) Persist record\n",
        "    _append_record_to_parquet_and_json(report, parquet_path)\n",
        "    return report\n",
        "\n",
        "# -------------------------\n",
        "# Persistence helpers\n",
        "# -------------------------\n",
        "def _append_record_to_parquet_and_json(record: Dict[str, Any], parquet_path: str):\n",
        "    # Flatten fields for parquet\n",
        "    row = {\n",
        "        \"url\": record.get(\"url\"),\n",
        "        \"scraped_at\": record.get(\"scraped_at\"),\n",
        "        \"final_url\": record.get(\"final_url\"),\n",
        "        \"fetch_method\": record.get(\"fetch_method\"),\n",
        "        \"fetch_error\": record.get(\"fetch_error\"),\n",
        "        \"category\": record.get(\"category\"),\n",
        "        \"title\": record.get(\"dom\", {}).get(\"title\"),\n",
        "        \"meta_description\": record.get(\"dom\", {}).get(\"meta_description\"),\n",
        "        \"text_snippet\": (record.get(\"dom\", {}).get(\"text\") or \"\")[:2000],\n",
        "        \"links_count\": len(record.get(\"dom\", {}).get(\"links\") or []),\n",
        "        \"forms_count\": len(record.get(\"dom\", {}).get(\"forms\") or []),\n",
        "        \"images_count\": len(record.get(\"dom\", {}).get(\"images\") or []),\n",
        "        \"logo\": record.get(\"dom\", {}).get(\"logo\"),\n",
        "        \"scripts_count\": record.get(\"dom\", {}).get(\"scripts_count\"),\n",
        "        \"inline_event_handlers\": record.get(\"dom\", {}).get(\"inline_event_handlers\"),\n",
        "        \"facilities\": json.dumps(record.get(\"facilities\") or {}, ensure_ascii=False),\n",
        "        \"llm_response\": json.dumps(record.get(\"llm_response\") or {}, ensure_ascii=False),\n",
        "        \"notes\": json.dumps(record.get(\"notes\") or [], ensure_ascii=False),\n",
        "    }\n",
        "    df_row = pd.DataFrame([row])\n",
        "\n",
        "    # append or create parquet\n",
        "    try:\n",
        "        if os.path.exists(parquet_path):\n",
        "            existing = pd.read_parquet(parquet_path)\n",
        "            combined = pd.concat([existing, df_row], ignore_index=True)\n",
        "            combined.to_parquet(parquet_path, index=False)\n",
        "        else:\n",
        "            df_row.to_parquet(parquet_path, index=False)\n",
        "        print(f\"[INFO] Appended record for {row['url']} to {parquet_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not write parquet: {e}\")\n",
        "        # fallback: write JSON file\n",
        "        ts = int(time.time())\n",
        "        fname = os.path.join(JSON_DIR, f\"site_report_fallback_{ts}.json\")\n",
        "        with open(fname, \"w\", encoding=\"utf-8\") as fh:\n",
        "            json.dump(record, fh, ensure_ascii=False, indent=2)\n",
        "        print(f\"[INFO] Wrote fallback JSON to {fname}\")\n",
        "\n",
        "    # also always write a full JSON snapshot for easy inspection\n",
        "    ts = int(time.time())\n",
        "    fname = os.path.join(JSON_DIR, f\"site_report_{ts}.json\")\n",
        "    try:\n",
        "        with open(fname, \"w\", encoding=\"utf-8\") as fh:\n",
        "            json.dump(record, fh, ensure_ascii=False, indent=2)\n",
        "        print(f\"[INFO] Wrote JSON snapshot to {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not write JSON snapshot: {e}\")\n",
        "\n",
        "# -------------------------\n",
        "# CLI demo\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    TEST_URLS = [\n",
        "        # Modify/add URLs for testing (be mindful of terms of service and legality)\n",
        "        #\"https://www.paypal.com/signin\",\n",
        "        #\"https://example.com\",\n",
        "        \"https://telstra-109995.weeblysite.com/\"\n",
        "        # Add known phishing templates if you have them locally or test pages you control\n",
        "    ]\n",
        "\n",
        "    for u in TEST_URLS:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"[RUN] Analyzing {u}\")\n",
        "        rpt = analyze_and_persist(u, parquet_path=PARQUET_OUTPUT, selenium_fallback=True, call_llm=CALL_OPENAI)\n",
        "        print(\"Final URL:\", rpt.get(\"final_url\"))\n",
        "        print(\"Category:\", rpt.get(\"category\"))\n",
        "        print(\"Logo:\", rpt.get(\"dom\", {}).get(\"logo\"))\n",
        "        print(\"Facilities:\", json.dumps(rpt.get(\"facilities\"), indent=2))\n",
        "        if rpt.get(\"llm_response\"):\n",
        "            # Try to print a short preview of the model's answer (if present)\n",
        "            try:\n",
        "                preview = rpt[\"llm_response\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "                print(\"LLM preview (truncated):\\n\", preview[:800])\n",
        "            except Exception:\n",
        "                print(\"LLM response present but could not parse preview.\")\n",
        "        if rpt.get(\"fetch_error\"):\n",
        "            print(\"Fetch error:\", rpt.get(\"fetch_error\"))\n",
        "    print(\"\\nDone. Parquet file:\", PARQUET_OUTPUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU0GrAuyPP6I"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nczJhIXTdcC3"
      },
      "source": [
        "#### **Logo Matching**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0d9B-t2dgl0"
      },
      "outputs": [],
      "source": [
        "# ===== Install dependencies (Colab) =====\n",
        "#!pip install torch torchvision pillow faiss-cpu --quiet\n",
        "\n",
        "import torch, torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# ===== Config =====\n",
        "FOLDER = Path(\"/content/sample_data/images\")   # folder with dataset images\n",
        "IMG_SIZE = 224\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ===== Model & Preprocessing =====\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = torch.nn.Identity()   # remove classification head → feature extractor\n",
        "model.eval().to(DEVICE)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def get_features(img_path: Path):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    x = transform(img).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        feat = model(x).cpu().numpy()\n",
        "    return feat / np.linalg.norm(feat)  # L2 normalize\n",
        "\n",
        "# ===== Index dataset images =====\n",
        "image_paths = list(FOLDER.glob(\"*.jpg\")) + list(FOLDER.glob(\"*.png\"))\n",
        "features = np.vstack([get_features(p) for p in image_paths])\n",
        "\n",
        "index = faiss.IndexFlatL2(features.shape[1])\n",
        "index.add(features)\n",
        "\n",
        "print(f\"Indexed {len(image_paths)} images.\")\n",
        "\n",
        "# ===== Function to query top-5 similar images =====\n",
        "def find_similar(input_img: str, topk=5):\n",
        "    q_feat = get_features(Path(input_img))\n",
        "    D, I = index.search(q_feat, topk)  # D=distances, I=indices\n",
        "    results = [image_paths[i] for i in I[0]]\n",
        "    return results, D[0]\n",
        "\n",
        "# ===== Example usage =====\n",
        "query_img = \"/content/sample_data/images/sample.jpg\"   # replace with your input image\n",
        "matches, scores = find_similar(query_img)\n",
        "\n",
        "print(\"Top matches:\")\n",
        "for path, score in zip(matches, scores):\n",
        "    print(f\"{path} (distance={score:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GxWB3q_PPpc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atwa2vh_RIZB",
        "outputId": "8c4bff27-09d9-4d3a-d277-b677546552cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Scraping: https://telstra-109995.weeblysite.com/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1211829305.py:196: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"scraped_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Appended record for https://telstra-109995.weeblysite.com/ to scraped_pages.parquet\n",
            "[INFO] Done. Records saved to scraped_pages.parquet\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "site_scrape_bs_selenium.py\n",
        "\n",
        "Scrape webpages using Requests + BeautifulSoup. If requests fails to fetch the page\n",
        "(or returns an empty/minimal response), use Selenium (headless Chrome) to load the page\n",
        "and then parse the page source with BeautifulSoup.\n",
        "\n",
        "Extracted data (title, meta, text, links, forms, images, scripts, inline handlers, logo, fetch info)\n",
        "is saved to a Parquet file (one row per URL).\n",
        "\n",
        "Usage:\n",
        "  - Edit TEST_URLS and run: python site_scrape_bs_selenium.py\n",
        "  - Requires chromedriver; webdriver-manager will auto-install it.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "from typing import Dict, Any, List, Tuple, Optional\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tldextract\n",
        "import pandas as pd\n",
        "\n",
        "# Selenium imports\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "REQUEST_TIMEOUT = 12\n",
        "USER_AGENT = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "              \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\")\n",
        "HEADERS = {\"User-Agent\": USER_AGENT}\n",
        "\n",
        "PARQUET_OUTPUT = \"scraped_pages.parquet\"\n",
        "MAX_TEXT_CHARS = 200000  # limit text stored\n",
        "\n",
        "# -------------------------\n",
        "# Scraping helpers\n",
        "# -------------------------\n",
        "def fetch_with_requests(url: str, timeout: int = REQUEST_TIMEOUT) -> Tuple[Optional[requests.Response], Optional[str]]:\n",
        "    \"\"\"Try a GET via requests. Return (response, error_string)\"\"\"\n",
        "    try:\n",
        "        resp = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)\n",
        "        resp.raise_for_status()\n",
        "        # treat very small responses as insufficient\n",
        "        if not resp.text or len(resp.text.strip()) < 100:\n",
        "            return resp, \"fetched_but_content_small\"\n",
        "        return resp, None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def fetch_with_selenium(url: str, timeout: int = 20) -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Use Selenium + headless Chrome to load the page and return page_source (HTML) or error.\n",
        "    webdriver-manager used to automatically provide chromedriver.\n",
        "    \"\"\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.headless = True\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(f\"user-agent={USER_AGENT}\")\n",
        "    chrome_options.add_argument(\"--disable-extensions\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "\n",
        "    driver = None\n",
        "    try:\n",
        "        driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
        "        driver.set_page_load_timeout(timeout)\n",
        "        driver.get(url)\n",
        "        # give some time for JS to run (small, non-blocking)\n",
        "        time.sleep(1.0)\n",
        "        html = driver.page_source\n",
        "        final_url = driver.current_url\n",
        "        return (final_url, html), None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "    finally:\n",
        "        if driver:\n",
        "            try:\n",
        "                driver.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Extraction helpers\n",
        "# -------------------------\n",
        "def sanitize_text(s: Optional[str]) -> str:\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def extract_basic_dom_fields(base_url: str, html: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parse HTML with BeautifulSoup and extract:\n",
        "      - title, meta description\n",
        "      - text (trimmed)\n",
        "      - links: list of dicts {href, text}\n",
        "      - forms: list of dicts {action, method, inputs: [{type,name,placeholder}]}\n",
        "      - images: list of dicts {src, alt}\n",
        "      - scripts_count, inline_event_handlers_count\n",
        "      - logo (favicon or og:image)\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html or \"\", \"html.parser\")\n",
        "    out: Dict[str, Any] = {}\n",
        "    out[\"title\"] = sanitize_text(soup.title.string) if soup.title and soup.title.string else \"\"\n",
        "    meta_desc_tag = soup.find(\"meta\", attrs={\"name\": \"description\"}) or soup.find(\"meta\", attrs={\"property\": \"og:description\"})\n",
        "    out[\"meta_description\"] = sanitize_text(meta_desc_tag.get(\"content\")) if meta_desc_tag and meta_desc_tag.get(\"content\") else \"\"\n",
        "    # page text (limited)\n",
        "    out[\"text\"] = sanitize_text(soup.get_text(separator=\" \", strip=True))[:MAX_TEXT_CHARS]\n",
        "\n",
        "    # anchors / links\n",
        "    anchors = []\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        try:\n",
        "            href = a.get(\"href\").strip()\n",
        "            abs_href = urljoin(base_url, href)\n",
        "            text = (a.get_text(\" \", strip=True) or \"\").strip()\n",
        "            anchors.append({\"href\": abs_href, \"text\": text})\n",
        "        except Exception:\n",
        "            continue\n",
        "    out[\"links\"] = anchors\n",
        "\n",
        "    # forms\n",
        "    forms = []\n",
        "    for f in soup.find_all(\"form\"):\n",
        "        action = f.get(\"action\") or \"\"\n",
        "        method = (f.get(\"method\") or \"GET\").upper()\n",
        "        inputs = []\n",
        "        for inp in f.find_all([\"input\", \"select\", \"textarea\", \"button\"]):\n",
        "            itype = (inp.get(\"type\") or inp.name or \"\").lower()\n",
        "            name = (inp.get(\"name\") or \"\").strip()\n",
        "            placeholder = (inp.get(\"placeholder\") or \"\").strip()\n",
        "            inputs.append({\"type\": itype, \"name\": name, \"placeholder\": placeholder})\n",
        "        forms.append({\"action\": urljoin(base_url, action), \"method\": method, \"inputs\": inputs})\n",
        "    out[\"forms\"] = forms\n",
        "\n",
        "    # images\n",
        "    images = []\n",
        "    for img in soup.find_all(\"img\", src=True):\n",
        "        try:\n",
        "            src = img.get(\"src\")\n",
        "            images.append({\"src\": urljoin(base_url, src), \"alt\": (img.get(\"alt\") or \"\").strip()})\n",
        "        except Exception:\n",
        "            continue\n",
        "    out[\"images\"] = images\n",
        "\n",
        "    # favicon / og:image as logo\n",
        "    ico = None\n",
        "    link_icon = soup.find(\"link\", rel=lambda v: v and \"icon\" in v.lower())\n",
        "    og_image = soup.find(\"meta\", property=\"og:image\")\n",
        "    if link_icon and link_icon.get(\"href\"):\n",
        "        ico = urljoin(base_url, link_icon[\"href\"])\n",
        "    elif og_image and og_image.get(\"content\"):\n",
        "        ico = urljoin(base_url, og_image[\"content\"])\n",
        "    out[\"logo\"] = ico or \"\"\n",
        "\n",
        "    # scripts and inline handlers\n",
        "    out[\"scripts_count\"] = len(soup.find_all(\"script\"))\n",
        "    # inline handlers: count tags having attributes starting with \"on\" (onclick, onmouseover...)\n",
        "    inline_handlers = 0\n",
        "    for tag in soup.find_all(True):\n",
        "        for attr in tag.attrs.keys():\n",
        "            if isinstance(attr, str) and attr.lower().startswith(\"on\"):\n",
        "                inline_handlers += 1\n",
        "    out[\"inline_event_handlers\"] = inline_handlers\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Orchestrator: fetch -> extract -> save\n",
        "# -------------------------\n",
        "def scrape_page_and_save(url: str, parquet_path: str = PARQUET_OUTPUT, selenium_fallback: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Scrape a single page:\n",
        "     - Try requests first\n",
        "     - If requests fails or returns tiny content, try Selenium fallback to fetch rendered HTML\n",
        "     - Extract DOM fields via BeautifulSoup\n",
        "     - Save extracted record to Parquet (append if file exists)\n",
        "    Returns the record dict (also written to parquet).\n",
        "    \"\"\"\n",
        "    record: Dict[str, Any] = {\n",
        "        \"url\": url,\n",
        "        \"scraped_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"final_url\": None,\n",
        "        \"fetch_method\": None,\n",
        "        \"fetch_error\": None,\n",
        "        \"html_length\": 0,\n",
        "        \"dom\": None,\n",
        "    }\n",
        "\n",
        "    # 1) Try requests\n",
        "    resp, err = fetch_with_requests(url)\n",
        "    html = None\n",
        "    if resp and not err:\n",
        "        html = resp.text\n",
        "        record[\"final_url\"] = resp.url\n",
        "        record[\"fetch_method\"] = \"requests\"\n",
        "    else:\n",
        "        # record the error\n",
        "        record[\"fetch_error\"] = err\n",
        "        record[\"fetch_method\"] = \"requests_failed\"\n",
        "\n",
        "    # 2) If requests returned but content small/insufficient, or requests failed -> attempt Selenium fallback\n",
        "    need_selenium = False\n",
        "    if not html or len(html.strip()) < 120:\n",
        "        need_selenium = True\n",
        "\n",
        "    if need_selenium and selenium_fallback:\n",
        "        # try selenium to render JS & get page_source\n",
        "        selenium_result, s_err = fetch_with_selenium(url)\n",
        "        if selenium_result:\n",
        "            final_url, html = selenium_result\n",
        "            record[\"final_url\"] = final_url or record.get(\"final_url\") or url\n",
        "            record[\"fetch_method\"] = \"selenium\"\n",
        "            record[\"fetch_error\"] = record.get(\"fetch_error\") or None\n",
        "        else:\n",
        "            # selenium failed too\n",
        "            record[\"fetch_error\"] = (record.get(\"fetch_error\") or \"\") + (\"; selenium_failed: \" + str(s_err) if s_err else \"\")\n",
        "            # leave html None and proceed to set dom=NA later\n",
        "\n",
        "    # 3) If no html, set dom to NA and write minimal record\n",
        "    if not html:\n",
        "        record[\"html_length\"] = 0\n",
        "        record[\"dom\"] = {\n",
        "            \"title\": \"\",\n",
        "            \"meta_description\": \"\",\n",
        "            \"text\": \"\",\n",
        "            \"links\": [],\n",
        "            \"forms\": [],\n",
        "            \"images\": [],\n",
        "            \"logo\": \"\",\n",
        "            \"scripts_count\": \"NA\",\n",
        "            \"inline_event_handlers\": \"NA\",\n",
        "        }\n",
        "        # persist and return\n",
        "        _append_record_to_parquet(record, parquet_path)\n",
        "        return record\n",
        "\n",
        "    # 4) Extract DOM via BeautifulSoup\n",
        "    dom = extract_basic_dom_fields(record.get(\"final_url\") or url, html)\n",
        "    record[\"html_length\"] = len(html)\n",
        "    record[\"dom\"] = dom\n",
        "\n",
        "    # 5) Save record to parquet (append)\n",
        "    _append_record_to_parquet(record, parquet_path)\n",
        "    return record\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Parquet helper\n",
        "# -------------------------\n",
        "def _append_record_to_parquet(record: Dict[str, Any], parquet_path: str):\n",
        "    \"\"\"\n",
        "    Append a record to a parquet file. If file exists, read it and append; else create new.\n",
        "    For nested dicts (dom), convert to JSON string for compact storage, but keep separate columns for key fields.\n",
        "    \"\"\"\n",
        "    # Flatten / prepare row\n",
        "    row = {\n",
        "        \"url\": record.get(\"url\"),\n",
        "        \"scraped_at\": record.get(\"scraped_at\"),\n",
        "        \"final_url\": record.get(\"final_url\"),\n",
        "        \"fetch_method\": record.get(\"fetch_method\"),\n",
        "        \"fetch_error\": record.get(\"fetch_error\"),\n",
        "        \"html_length\": record.get(\"html_length\"),\n",
        "        # Dom top-level items for convenience\n",
        "        \"title\": record.get(\"dom\", {}).get(\"title\") if record.get(\"dom\") else None,\n",
        "        \"meta_description\": record.get(\"dom\", {}).get(\"meta_description\") if record.get(\"dom\") else None,\n",
        "        \"text\": record.get(\"dom\", {}).get(\"text\") if record.get(\"dom\") else None,\n",
        "        \"links_count\": len(record.get(\"dom\", {}).get(\"links\") or []) if record.get(\"dom\") else 0,\n",
        "        \"forms_count\": len(record.get(\"dom\", {}).get(\"forms\") or []) if record.get(\"dom\") else 0,\n",
        "        \"images_count\": len(record.get(\"dom\", {}).get(\"images\") or []) if record.get(\"dom\") else 0,\n",
        "        \"logo\": record.get(\"dom\", {}).get(\"logo\") if record.get(\"dom\") else None,\n",
        "        \"scripts_count\": record.get(\"dom\", {}).get(\"scripts_count\") if record.get(\"dom\") else None,\n",
        "        \"inline_event_handlers\": record.get(\"dom\", {}).get(\"inline_event_handlers\") if record.get(\"dom\") else None,\n",
        "        # Full DOM JSON for reproducibility\n",
        "        \"dom_json\": json.dumps(record.get(\"dom\") or {}, ensure_ascii=False),\n",
        "    }\n",
        "    df_row = pd.DataFrame([row])\n",
        "\n",
        "    # Append or create\n",
        "    try:\n",
        "        # If parquet exists, read and concat (keeps schema consistent)\n",
        "        import os\n",
        "        if os.path.exists(parquet_path):\n",
        "            old = pd.read_parquet(parquet_path)\n",
        "            new = pd.concat([old, df_row], ignore_index=True)\n",
        "            new.to_parquet(parquet_path, index=False)\n",
        "        else:\n",
        "            df_row.to_parquet(parquet_path, index=False)\n",
        "        print(f\"[INFO] Appended record for {row['url']} to {parquet_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not write parquet: {e}\")\n",
        "        # fallback: write a JSON file with timestamp\n",
        "        ts = int(time.time())\n",
        "        fallback = f\"scrape_fallback_{ts}.json\"\n",
        "        with open(fallback, \"w\", encoding=\"utf-8\") as fh:\n",
        "            json.dump(record, fh, ensure_ascii=False, indent=2)\n",
        "        print(f\"[INFO] Wrote fallback JSON to {fallback}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Example usage & tests\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    TEST_URLS = [\n",
        "        #\"https://www.example.com\",\n",
        "        \"https://telstra-109995.weeblysite.com/\"\n",
        "        # add more URLs for bulk scraping\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    for u in TEST_URLS:\n",
        "        print(f\"[INFO] Scraping: {u}\")\n",
        "        rec = scrape_page_and_save(u, parquet_path=PARQUET_OUTPUT, selenium_fallback=True)\n",
        "        results.append(rec)\n",
        "\n",
        "    print(\"[INFO] Done. Records saved to\", PARQUET_OUTPUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMwRoljBSUqX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIwTr-ReSUmz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsePrJnSWzmV",
        "outputId": "b438a4f2-25cf-4669-bd52-4355822feafd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'src': 'https://cdn3.editmysite.com/app/checkout/assets/checkout/system.c3b89b0b94f4ef0671b1.js', 'type': 'external', 'status': 'unknown'}\n",
            "{'src': 'https://cdn3.editmysite.com/app/checkout/assets/checkout/imports.en.74ef33aa0ad930b9.js', 'type': 'external', 'status': 'unknown'}\n",
            "{'src': 'https://cdn3.editmysite.com/app/checkout/assets/checkout/locale-imports-map.680f7c3236b652da.json', 'type': 'external', 'status': 'unknown'}\n",
            "{'src': 'https://cdn3.editmysite.com/app/website/js/runtime.7ee5d63b96d4f5d36052.js', 'type': 'external', 'status': 'unknown'}\n",
            "{'src': 'https://cdn3.editmysite.com/app/website/js/vue-modules.4a41b3ba298bf4563d97.js', 'type': 'external', 'status': 'benign'}\n",
            "{'src': 'https://cdn3.editmysite.com/app/website/js/languages/en.77c86c931176c693e0d9.js', 'type': 'external', 'status': 'unknown'}\n",
            "{'src': 'https://cdn3.editmysite.com/app/website/js/site.2f7222717d1bb29e7c3e.js', 'type': 'external', 'status': 'unknown'}\n",
            "{'src': None, 'type': 'inline', 'status': 'benign', 'malicious_hits': [], 'benign_hits': ['gtag'], 'preview': \"\\n        window.dataLayer = window.dataLayer || [];\\n        function gtag(){dataLayer.push(arguments);}\\n        gtag('js', new Date());\\n    \"}\n",
            "{'src': None, 'type': 'inline', 'status': 'benign', 'malicious_hits': [], 'benign_hits': ['bootstrap'], 'preview': '\\n    window.__DYNAMIC_BOOTSTRAP__ ??= {};\\n\\n    Object.assign(window.__DYNAMIC_BOOTSTRAP__, {\\n        featureFlags: {\"ecom.website.deprecated-layouts\":false,\"marketing-enable-drive-repeat-purchase-camp'}\n",
            "{'src': None, 'type': 'inline', 'status': 'benign', 'malicious_hits': [], 'benign_hits': ['bootstrap'], 'preview': '\\n        window.__BOOTSTRAP_STATE__ = {\"siteData\":{\"site\":{\"id\":\"99a52d60-64bc-11ee-b492-bdf79390cff5\",\"properties\":{\"classicSiteID\":\"440522637632940554\",\"catalogSiteId\":\"440522637632940554\",\"selected'}\n",
            "{'src': None, 'type': 'inline', 'status': 'unknown', 'malicious_hits': [], 'benign_hits': [], 'preview': \"\\n        window.stopSiteLoadingAnimation = () => {\\n            const loadingElement = document.querySelector('.loading-view');\\n            loadingElement?.remove();\\n        };\\n    \"}\n",
            "{'src': None, 'type': 'inline', 'status': 'unknown', 'malicious_hits': [], 'benign_hits': [], 'preview': '\\n    /**\\n     * PCI Compliance Flag\\n     *\\n     * This flag indicates if scripts have been filtered for PCI compliance (OpenTabs SSO Checkout route).\\n     * This value is set only for buyer dashboard/'}\n",
            "{'src': None, 'type': 'inline', 'status': 'benign', 'malicious_hits': [], 'benign_hits': ['bootstrap'], 'preview': \"\\n    // used for editor-frame and pub site bootstrap\\n    window.APP_ENV = 'production';\\n    window.APP_ORIGIN = 'https://square.online';\\n    window.APP_URL = 'https://www.weebly.com';\\n    window.ASSET\"}\n",
            "{'src': None, 'type': 'inline', 'status': 'unknown', 'malicious_hits': [], 'benign_hits': [], 'preview': \"\\n    (function (p, l, o, w, i, n, g) {\\n    var id = i + '-script';\\n    if (!l.getElementById(i)) {\\n        n = l.createElement(o);\\n        g = l.getElementsByTagName(o)[0];\\n        n.id = id;\\n        \"}\n",
            "{'src': None, 'type': 'inline', 'status': 'malicious', 'malicious_hits': ['fromCharCode'], 'benign_hits': [], 'preview': '\\n\\n(function (p, l, o, w, i, n, g) {\\n    if (!p[i]) {\\n        p.GlobalSnowplowNamespace = p.GlobalSnowplowNamespace || [];\\n        p.GlobalSnowplowNamespace.push(i);\\n        p[i] = function () {\\n      '}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "SAFE_KEYWORDS = [\n",
        "    \"google-analytics\", \"gtag\", \"fbq\", \"hotjar\", \"adsbygoogle\", \"googletag\",\n",
        "    \"jquery\", \"bootstrap\", \"react\", \"vue\", \"angular\"\n",
        "]\n",
        "\n",
        "MALICIOUS_PATTERNS = [\n",
        "    r\"eval\\(\", r\"atob\\(\", r\"unescape\\(\", r\"fromCharCode\",\n",
        "    r\"window\\.location\\s*=\", r\"document\\.onkeypress\",\n",
        "    r\"\\.exe\", r\"\\.apk\", r\"\\.scr\", r\"cryptominer\", r\"WebAssembly\"\n",
        "]\n",
        "\n",
        "def analyze_scripts(url: str):\n",
        "    try:\n",
        "        res = requests.get(url, timeout=10)\n",
        "        res.raise_for_status()\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        script_results = []\n",
        "        for script in soup.find_all(\"script\"):\n",
        "            code = script.string or \"\"\n",
        "            src = script.get(\"src\", \"\")\n",
        "\n",
        "            # External script (by src URL)\n",
        "            if src:\n",
        "                benign = any(safe in src.lower() for safe in SAFE_KEYWORDS)\n",
        "                script_results.append({\n",
        "                    \"src\": src,\n",
        "                    \"type\": \"external\",\n",
        "                    \"status\": \"benign\" if benign else \"unknown\"\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Inline script\n",
        "            if not code.strip():\n",
        "                continue\n",
        "\n",
        "            malicious_hits = [p for p in MALICIOUS_PATTERNS if re.search(p, code)]\n",
        "            benign_hits = [k for k in SAFE_KEYWORDS if k in code.lower()]\n",
        "\n",
        "            if malicious_hits and not benign_hits:\n",
        "                status = \"malicious\"\n",
        "            elif benign_hits:\n",
        "                status = \"benign\"\n",
        "            else:\n",
        "                status = \"unknown\"\n",
        "\n",
        "            script_results.append({\n",
        "                \"src\": None,\n",
        "                \"type\": \"inline\",\n",
        "                \"status\": status,\n",
        "                \"malicious_hits\": malicious_hits,\n",
        "                \"benign_hits\": benign_hits,\n",
        "                \"preview\": code[:200]  # first 200 chars\n",
        "            })\n",
        "\n",
        "        return script_results\n",
        "\n",
        "    except Exception as e:\n",
        "        return [{\"error\": str(e)}]\n",
        "\n",
        "\n",
        "# Example\n",
        "scripts = analyze_scripts(\"https://telstra-109995.weeblysite.com/\")\n",
        "for s in scripts:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0kgrekjO5HD"
      },
      "source": [
        "#### END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIRiqn6oO6ds"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkJv3XqdVKMc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryUuSUYhWgVh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
